#!/usr/bin/env python3
"""
Design Checker BWMS Rules Test
==============================
BWMS P&ID ë„ë©´ì—ì„œ ì œí’ˆë³„(ECS/HYCHLOR/AUTO) ê·œì¹™ ì ìš© ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.

í…ŒìŠ¤íŠ¸ ì„¤ì •:
- Baseline: product_type=AUTO (í˜„ì¬)
- Test A: product_type=ECS
- Test B: product_type=HYCHLOR

í‰ê°€ ì§€í‘œ:
- ì ìš©ëœ ê·œì¹™ ìˆ˜
- ì¹´í…Œê³ ë¦¬ë³„ ìœ„ë°˜ ì‚¬í•­
- ì‹¬ê°ë„ë³„ ìœ„ë°˜ ì‚¬í•­
- ê·œì • ì¤€ìˆ˜ìœ¨ (compliance_score)
"""

import requests
import json
import sys
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# Configuration
DESIGN_CHECKER_URL = "http://localhost:5019/api/v1/pipeline/validate"
SAMPLE_IMAGE = "/home/uproot/ax/poc/web-ui/public/samples/bwms_pid_sample.png"
OUTPUT_DIR = Path("/home/uproot/ax/poc/rnd/experiments/bwms_pipeline_improvement/results")

# Test configurations
TEST_CONFIGS = [
    {"name": "AUTO", "product_type": "AUTO", "description": "ìë™ ê°ì§€ (í˜„ì¬)"},
    {"name": "ECS", "product_type": "ECS", "description": "ECS ì „ìš© ê·œì¹™"},
    {"name": "HYCHLOR", "product_type": "HYCHLOR", "description": "HYCHLOR ì „ìš© ê·œì¹™"},
]

# Common parameters
COMMON_PARAMS = {
    "model_type": "pid_class_aware",
    "confidence": "0.25",
    "use_ocr": "true",
    "ocr_source": "edocr2",
}


def run_validation(image_path: str, product_type: str) -> dict:
    """Run Design Checker validation with specified product type."""
    with open(image_path, "rb") as f:
        files = {"file": (Path(image_path).name, f, "image/png")}
        data = {
            **COMMON_PARAMS,
            "product_type": product_type,
        }

        response = requests.post(DESIGN_CHECKER_URL, files=files, data=data, timeout=180)
        response.raise_for_status()
        return response.json()


def analyze_results(response: dict) -> dict:
    """Analyze validation results and compute metrics."""
    data = response.get("data", response)
    validation = data.get("validation", {})
    summary = data.get("summary", validation.get("summary", {}))
    violations = validation.get("violations", [])
    rules_applied = validation.get("rules_applied", [])
    processing_time = response.get("processing_time", data.get("processing_time", 0))

    # YOLO stats
    yolo = data.get("yolo", {})
    yolo_detections = yolo.get("total_detections", 0)
    yolo_class_counts = yolo.get("class_counts", {})

    # Violation analysis
    by_severity = defaultdict(int)
    by_category = defaultdict(int)
    by_rule = defaultdict(int)

    for v in violations:
        severity = v.get("severity", "unknown")
        category = v.get("category", "unknown")
        rule_id = v.get("rule_id", "unknown")
        by_severity[severity] += 1
        by_category[category] += 1
        by_rule[rule_id] += 1

    return {
        "yolo_detections": yolo_detections,
        "yolo_class_counts": yolo_class_counts,
        "rules_applied": len(rules_applied) if isinstance(rules_applied, list) else rules_applied,
        "total_violations": len(violations),
        "violations_by_severity": dict(by_severity),
        "violations_by_category": dict(by_category),
        "violations_by_rule": dict(by_rule),
        "compliance_score": summary.get("compliance_score", 0),
        "total_rules": summary.get("total_rules", 0),
        "passed_rules": summary.get("passed", 0),
        "failed_rules": summary.get("failed", 0),
        "processing_time": processing_time,
        "violations_detail": violations[:10],  # Top 10 violations for details
    }


def print_results(config: dict, analysis: dict):
    """Print formatted results."""
    print(f"\n{'='*60}")
    print(f"ğŸ”¬ {config['name']}: product_type={config['product_type']} ({config['description']})")
    print(f"{'='*60}")

    print(f"\nğŸ“Š YOLO ê²€ì¶œ: {analysis['yolo_detections']}ê°œ")
    if analysis['yolo_class_counts']:
        top_classes = sorted(analysis['yolo_class_counts'].items(), key=lambda x: -x[1])[:5]
        for cls, count in top_classes:
            print(f"   - {cls}: {count}ê°œ")

    print(f"\nğŸ“‹ ê·œì¹™ ê²€ì‚¬:")
    print(f"   ì ìš©ëœ ê·œì¹™: {analysis['rules_applied']}ê°œ")
    print(f"   ì´ ê·œì¹™: {analysis['total_rules']}ê°œ")
    print(f"   í†µê³¼: {analysis['passed_rules']}ê°œ")
    print(f"   ì‹¤íŒ¨: {analysis['failed_rules']}ê°œ")
    print(f"   ì¤€ìˆ˜ìœ¨: {analysis['compliance_score']:.1f}%")

    print(f"\nâš ï¸  ìœ„ë°˜ ì‚¬í•­: {analysis['total_violations']}ê°œ")

    if analysis['violations_by_severity']:
        print(f"\n   ì‹¬ê°ë„ë³„:")
        for severity, count in sorted(analysis['violations_by_severity'].items()):
            icon = "âŒ" if severity == "error" else "âš ï¸" if severity == "warning" else "â„¹ï¸"
            print(f"   {icon} {severity}: {count}ê°œ")

    if analysis['violations_by_category']:
        print(f"\n   ì¹´í…Œê³ ë¦¬ë³„:")
        for category, count in sorted(analysis['violations_by_category'].items(), key=lambda x: -x[1])[:5]:
            print(f"      {category}: {count}ê°œ")

    if analysis['violations_detail']:
        print(f"\n   ì£¼ìš” ìœ„ë°˜ ì‚¬í•­ (ìƒìœ„ 5ê°œ):")
        for i, v in enumerate(analysis['violations_detail'][:5], 1):
            severity = v.get("severity", "unknown")
            rule_id = v.get("rule_id", "unknown")
            message = v.get("message", v.get("description", ""))[:50]
            icon = "âŒ" if severity == "error" else "âš ï¸" if severity == "warning" else "â„¹ï¸"
            print(f"   {i}. {icon} [{rule_id}] {message}...")

    print(f"\nâ±ï¸  ì²˜ë¦¬ ì‹œê°„: {analysis['processing_time']:.2f}ì´ˆ")


def main():
    print("=" * 60)
    print("ğŸ”¬ Design Checker BWMS Rules Test")
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“ Image: {SAMPLE_IMAGE}")
    print("=" * 60)

    # Check image exists
    print("\nğŸ“· Checking image...")
    if not Path(SAMPLE_IMAGE).exists():
        print(f"   âŒ Image not found: {SAMPLE_IMAGE}")
        sys.exit(1)
    file_size = Path(SAMPLE_IMAGE).stat().st_size // 1024
    print(f"   âœ… Image found ({file_size}KB)")

    # Run tests
    results = []
    for config in TEST_CONFIGS:
        print(f"\nğŸš€ Running {config['name']} (product_type={config['product_type']})...")
        try:
            response = run_validation(SAMPLE_IMAGE, config['product_type'])
            analysis = analyze_results(response)

            results.append({
                "config": config,
                "analysis": analysis,
            })

            print_results(config, analysis)

        except requests.exceptions.RequestException as e:
            print(f"   âŒ API Error: {e}")
            continue
        except Exception as e:
            print(f"   âŒ Error: {e}")
            import traceback
            traceback.print_exc()
            continue

    if not results:
        print("\nâŒ All tests failed!")
        sys.exit(1)

    # Summary comparison
    print("\n" + "=" * 60)
    print("ğŸ“Š SUMMARY COMPARISON")
    print("=" * 60)
    print(f"\n{'Config':<10} {'Rules':>6} {'Passed':>7} {'Failed':>7} {'Score':>7} {'Time':>6}")
    print("-" * 50)

    for r in results:
        c = r['config']
        a = r['analysis']
        print(f"{c['name']:<10} {a['total_rules']:>6} {a['passed_rules']:>7} {a['failed_rules']:>7} {a['compliance_score']:>6.1f}% {a['processing_time']:>5.1f}s")

    # Rules comparison
    print("\n" + "=" * 60)
    print("ğŸ“‹ RULES COMPARISON")
    print("=" * 60)

    if len(results) >= 2:
        auto_result = next((r for r in results if r['config']['product_type'] == 'AUTO'), None)
        ecs_result = next((r for r in results if r['config']['product_type'] == 'ECS'), None)
        hychlor_result = next((r for r in results if r['config']['product_type'] == 'HYCHLOR'), None)

        if auto_result and ecs_result:
            auto = auto_result['analysis']
            ecs = ecs_result['analysis']
            print(f"\nğŸ”„ AUTO vs ECS:")
            print(f"   ê·œì¹™ ìˆ˜: {auto['total_rules']} vs {ecs['total_rules']} ({ecs['total_rules'] - auto['total_rules']:+d})")
            print(f"   ìœ„ë°˜ ìˆ˜: {auto['total_violations']} vs {ecs['total_violations']} ({ecs['total_violations'] - auto['total_violations']:+d})")
            print(f"   ì¤€ìˆ˜ìœ¨: {auto['compliance_score']:.1f}% vs {ecs['compliance_score']:.1f}%")

        if auto_result and hychlor_result:
            auto = auto_result['analysis']
            hychlor = hychlor_result['analysis']
            print(f"\nğŸ”„ AUTO vs HYCHLOR:")
            print(f"   ê·œì¹™ ìˆ˜: {auto['total_rules']} vs {hychlor['total_rules']} ({hychlor['total_rules'] - auto['total_rules']:+d})")
            print(f"   ìœ„ë°˜ ìˆ˜: {auto['total_violations']} vs {hychlor['total_violations']} ({hychlor['total_violations'] - auto['total_violations']:+d})")
            print(f"   ì¤€ìˆ˜ìœ¨: {auto['compliance_score']:.1f}% vs {hychlor['compliance_score']:.1f}%")

    # Recommendation
    print("\n" + "=" * 60)
    print("ğŸ’¡ RECOMMENDATION")
    print("=" * 60)

    if results:
        # Find best product type based on rule coverage
        best = max(results, key=lambda r: r['analysis']['total_rules'])
        print(f"\nâœ… ê¶Œì¥ ì„¤ì •: product_type={best['config']['product_type']}")
        print(f"   - ì ìš© ê·œì¹™: {best['analysis']['total_rules']}ê°œ")
        print(f"   - ìœ„ë°˜ ì‚¬í•­: {best['analysis']['total_violations']}ê°œ")
        print(f"   - ê·œì • ì¤€ìˆ˜ìœ¨: {best['analysis']['compliance_score']:.1f}%")

    # Save results
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    output_file = OUTPUT_DIR / f"design_checker_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        # Clean up for JSON serialization
        save_results = []
        for r in results:
            save_results.append({
                "config": r['config'],
                "analysis": {k: v for k, v in r['analysis'].items() if k != 'violations_detail'}
            })
        json.dump(save_results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“ Results saved: {output_file}")
    print("\nâœ… Test completed!")


if __name__ == "__main__":
    main()
