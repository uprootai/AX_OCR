# YOLOv11 êµ¬í˜„ ê°€ì´ë“œ (ìƒì„¸)

**ì‘ì„±ì¼**: 2025-10-31
**ëª©ì **: ê³µí•™ ë„ë©´ ì¹˜ìˆ˜/GD&T ì¶”ì¶œì„ ìœ„í•œ YOLOv11 End-to-End êµ¬í˜„

---

## ğŸ“‘ ëª©ì°¨

1. [ë°ì´í„°ì…‹ ì¡°í•© ë°©ë²•](#1-ë°ì´í„°ì…‹-ì¡°í•©-ë°©ë²•)
2. [ëª¨ë¸ í•™ìŠµ ë°©ë²•](#2-ëª¨ë¸-í•™ìŠµ-ë°©ë²•)
3. [ì¶”ë¡  ë°©ë²•](#3-ì¶”ë¡ -ë°©ë²•)
4. [API ì„œë²„ êµ¬ì¶•](#4-api-ì„œë²„-êµ¬ì¶•)
5. [Gateway í†µí•©](#5-gateway-í†µí•©)
6. [ì„±ëŠ¥ í‰ê°€](#6-ì„±ëŠ¥-í‰ê°€)

---

## 1. ë°ì´í„°ì…‹ ì¡°í•© ë°©ë²•

### 1.1 ë°ì´í„° êµ¬ì¡° ì„¤ê³„

YOLOv11ì€ ë‹¤ìŒê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ìš”êµ¬í•©ë‹ˆë‹¤:

```
datasets/
â””â”€â”€ engineering_drawings/
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ train/
    â”‚   â”‚   â”œâ”€â”€ drawing_001.jpg
    â”‚   â”‚   â”œâ”€â”€ drawing_002.jpg
    â”‚   â”‚   â””â”€â”€ ...
    â”‚   â”œâ”€â”€ val/
    â”‚   â”‚   â”œâ”€â”€ drawing_101.jpg
    â”‚   â”‚   â””â”€â”€ ...
    â”‚   â””â”€â”€ test/
    â”‚       â”œâ”€â”€ drawing_201.jpg
    â”‚       â””â”€â”€ ...
    â””â”€â”€ labels/
        â”œâ”€â”€ train/
        â”‚   â”œâ”€â”€ drawing_001.txt
        â”‚   â”œâ”€â”€ drawing_002.txt
        â”‚   â””â”€â”€ ...
        â”œâ”€â”€ val/
        â”‚   â”œâ”€â”€ drawing_101.txt
        â”‚   â””â”€â”€ ...
        â””â”€â”€ test/
            â”œâ”€â”€ drawing_201.txt
            â””â”€â”€ ...
```

### 1.2 í´ë˜ìŠ¤ ì •ì˜

**íŒŒì¼**: `datasets/engineering_drawings/classes.yaml`

```yaml
# YOLOv11 Dataset Configuration
path: /home/uproot/ax/poc/datasets/engineering_drawings
train: images/train
val: images/val
test: images/test

# Classes (14ê°œ)
names:
  0: diameter_dim        # Ï†476, Ï†370
  1: linear_dim          # 120, 245
  2: radius_dim          # R50, R25
  3: angular_dim         # 45Â°, 90Â°
  4: chamfer_dim         # 2x45Â°, C3
  5: tolerance_dim       # Â±0.1, +0.2/-0.1
  6: reference_dim       # (177), (245)
  7: flatness            # âŒ¹
  8: cylindricity        # â—‹
  9: position            # âŒ–
  10: perpendicularity   # âŠ¥
  11: parallelism        # âˆ¥
  12: surface_roughness  # Ra3.2, Ra6.3
  13: text_block         # ì¼ë°˜ í…ìŠ¤íŠ¸

# Number of classes
nc: 14
```

### 1.3 ë¼ë²¨ í¬ë§· (YOLO Format)

ê° ì´ë¯¸ì§€ì— ëŒ€ì‘í•˜ëŠ” `.txt` íŒŒì¼:

```
<class_id> <x_center> <y_center> <width> <height>
```

- **class_id**: 0-13 (í´ë˜ìŠ¤ ì¸ë±ìŠ¤)
- **x_center, y_center**: ì¤‘ì‹¬ì  ì¢Œí‘œ (0-1 ì •ê·œí™”)
- **width, height**: ë°•ìŠ¤ í¬ê¸° (0-1 ì •ê·œí™”)

**ì˜ˆì‹œ**: `drawing_001.txt`
```
0 0.234 0.456 0.05 0.03   # diameter_dim at (23.4%, 45.6%)
1 0.678 0.234 0.04 0.02   # linear_dim
7 0.456 0.789 0.06 0.04   # flatness symbol
```

### 1.4 ì¢Œí‘œ ë³€í™˜ í•¨ìˆ˜

eDOCrì˜ bboxë¥¼ YOLO í¬ë§·ìœ¼ë¡œ ë³€í™˜:

```python
def edocr_to_yolo_format(bbox, image_width, image_height):
    """
    eDOCr bboxë¥¼ YOLO í¬ë§·ìœ¼ë¡œ ë³€í™˜

    Args:
        bbox: dict with keys x, y, width, height (í”½ì…€ ì¢Œí‘œ)
        image_width: ì´ë¯¸ì§€ ë„ˆë¹„
        image_height: ì´ë¯¸ì§€ ë†’ì´

    Returns:
        tuple: (x_center, y_center, width, height) ì •ê·œí™”ëœ ì¢Œí‘œ
    """
    x = bbox['x']
    y = bbox['y']
    w = bbox['width']
    h = bbox['height']

    # ì¤‘ì‹¬ì  ê³„ì‚°
    x_center = (x + w / 2) / image_width
    y_center = (y + h / 2) / image_height

    # í¬ê¸° ì •ê·œí™”
    norm_width = w / image_width
    norm_height = h / image_height

    # 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
    x_center = max(0, min(1, x_center))
    y_center = max(0, min(1, y_center))
    norm_width = max(0, min(1, norm_width))
    norm_height = max(0, min(1, norm_height))

    return x_center, y_center, norm_width, norm_height
```

### 1.5 ë°ì´í„°ì…‹ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼**: `scripts/prepare_dataset.py`

```python
#!/usr/bin/env python3
"""
eDOCr ê²°ê³¼ë¥¼ YOLO ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜
"""
import os
import json
import shutil
from pathlib import Path
from PIL import Image
import random

def create_dataset_structure(output_dir):
    """ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ìƒì„±"""
    dirs = [
        'images/train', 'images/val', 'images/test',
        'labels/train', 'labels/val', 'labels/test'
    ]
    for d in dirs:
        Path(output_dir / d).mkdir(parents=True, exist_ok=True)

def classify_dimension(text):
    """ì¹˜ìˆ˜ í…ìŠ¤íŠ¸ë¥¼ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜"""
    text = text.strip()

    # Diameter
    if text.startswith('Ï†') or text.startswith('Ã˜'):
        return 0, 'diameter_dim'

    # Radius
    if text.startswith('R'):
        return 2, 'radius_dim'

    # Angular
    if 'Â°' in text:
        return 3, 'angular_dim'

    # Chamfer
    if 'x' in text and 'Â°' in text:
        return 4, 'chamfer_dim'
    if text.startswith('C'):
        return 4, 'chamfer_dim'

    # Tolerance
    if 'Â±' in text or ('+' in text and '-' in text):
        return 5, 'tolerance_dim'

    # Reference (in parentheses)
    if text.startswith('(') and text.endswith(')'):
        return 6, 'reference_dim'

    # Default: linear dimension
    return 1, 'linear_dim'

def classify_gdt_symbol(symbol_type):
    """GD&T ê¸°í˜¸ ë¶„ë¥˜"""
    gdt_map = {
        'flatness': 7,
        'cylindricity': 8,
        'position': 9,
        'perpendicularity': 10,
        'parallelism': 11
    }
    return gdt_map.get(symbol_type.lower(), 7)

def convert_annotation(annotation, image_path, output_label_path):
    """
    ë‹¨ì¼ ì–´ë…¸í…Œì´ì…˜ì„ YOLO í¬ë§·ìœ¼ë¡œ ë³€í™˜

    Args:
        annotation: dict with OCR results
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        output_label_path: ì¶œë ¥ ë¼ë²¨ íŒŒì¼ ê²½ë¡œ
    """
    # ì´ë¯¸ì§€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
    with Image.open(image_path) as img:
        img_width, img_height = img.size

    yolo_lines = []

    # Dimensions ë³€í™˜
    if 'dimensions' in annotation and annotation['dimensions']:
        for dim in annotation['dimensions']:
            if 'bbox' not in dim or 'value' not in dim:
                continue

            bbox = dim['bbox']
            value = dim['value']

            # í´ë˜ìŠ¤ ë¶„ë¥˜
            class_id, class_name = classify_dimension(value)

            # YOLO í¬ë§· ë³€í™˜
            x_center, y_center, width, height = edocr_to_yolo_format(
                bbox, img_width, img_height
            )

            yolo_lines.append(
                f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}"
            )

    # GD&T ê¸°í˜¸ ë³€í™˜
    if 'gdt' in annotation and annotation['gdt']:
        for gdt in annotation['gdt']:
            if 'bbox' not in gdt or 'type' not in gdt:
                continue

            bbox = gdt['bbox']
            symbol_type = gdt['type']

            # í´ë˜ìŠ¤ ë¶„ë¥˜
            class_id = classify_gdt_symbol(symbol_type)

            # YOLO í¬ë§· ë³€í™˜
            x_center, y_center, width, height = edocr_to_yolo_format(
                bbox, img_width, img_height
            )

            yolo_lines.append(
                f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}"
            )

    # Surface Roughness (í‘œë©´ì¡°ë„)
    if 'surface_roughness' in annotation and annotation['surface_roughness']:
        for sr in annotation['surface_roughness']:
            if 'bbox' not in sr:
                continue

            bbox = sr['bbox']
            class_id = 12  # surface_roughness

            x_center, y_center, width, height = edocr_to_yolo_format(
                bbox, img_width, img_height
            )

            yolo_lines.append(
                f"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}"
            )

    # ë¼ë²¨ íŒŒì¼ ì €ì¥
    with open(output_label_path, 'w') as f:
        f.write('\n'.join(yolo_lines))

    return len(yolo_lines)

def split_dataset(image_files, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """ë°ì´í„°ì…‹ì„ train/val/testë¡œ ë¶„í• """
    random.shuffle(image_files)

    total = len(image_files)
    train_end = int(total * train_ratio)
    val_end = train_end + int(total * val_ratio)

    return {
        'train': image_files[:train_end],
        'val': image_files[train_end:val_end],
        'test': image_files[val_end:]
    }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì„¤ì •
    source_images_dir = Path('/home/uproot/ax/poc/edocr2-api/uploads')
    source_annotations_dir = Path('/home/uproot/ax/poc/edocr2-api/results')
    output_dir = Path('/home/uproot/ax/poc/datasets/engineering_drawings')

    print("ğŸ“ Creating dataset structure...")
    create_dataset_structure(output_dir)

    # ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
    image_files = []

    for ext in image_extensions:
        image_files.extend(source_images_dir.glob(f'*{ext}'))

    print(f"ğŸ“Š Found {len(image_files)} images")

    if len(image_files) == 0:
        print("âŒ No images found. Please add images to:", source_images_dir)
        return

    # Train/Val/Test ë¶„í• 
    splits = split_dataset(image_files)

    print(f"âœ‚ï¸  Split: Train={len(splits['train'])}, Val={len(splits['val'])}, Test={len(splits['test'])}")

    # ê° ë¶„í• ë³„ë¡œ ì²˜ë¦¬
    total_annotations = 0

    for split_name, split_files in splits.items():
        print(f"\nğŸ”„ Processing {split_name} split...")

        for img_path in split_files:
            # ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ì°¾ê¸°
            annotation_path = source_annotations_dir / f"{img_path.stem}_result.json"

            if not annotation_path.exists():
                print(f"âš ï¸  No annotation for {img_path.name}, skipping")
                continue

            # ì´ë¯¸ì§€ ë³µì‚¬
            dst_image = output_dir / 'images' / split_name / img_path.name
            shutil.copy(img_path, dst_image)

            # ì–´ë…¸í…Œì´ì…˜ ë¡œë“œ
            with open(annotation_path, 'r') as f:
                annotation = json.load(f)

            # YOLO ë¼ë²¨ ìƒì„±
            dst_label = output_dir / 'labels' / split_name / f"{img_path.stem}.txt"
            count = convert_annotation(annotation, img_path, dst_label)

            total_annotations += count

            if count > 0:
                print(f"âœ… {img_path.name}: {count} objects")
            else:
                print(f"âš ï¸  {img_path.name}: No objects detected")

    print(f"\nğŸ‰ Dataset creation complete!")
    print(f"   Total images: {len(image_files)}")
    print(f"   Total annotations: {total_annotations}")
    print(f"   Output directory: {output_dir}")

    # data.yaml ìƒì„±
    yaml_path = output_dir / 'data.yaml'
    with open(yaml_path, 'w') as f:
        f.write(f"""# Engineering Drawings Dataset
path: {output_dir}
train: images/train
val: images/val
test: images/test

# Classes
names:
  0: diameter_dim
  1: linear_dim
  2: radius_dim
  3: angular_dim
  4: chamfer_dim
  5: tolerance_dim
  6: reference_dim
  7: flatness
  8: cylindricity
  9: position
  10: perpendicularity
  11: parallelism
  12: surface_roughness
  13: text_block

nc: 14
""")

    print(f"ğŸ“ Created data.yaml at: {yaml_path}")

def edocr_to_yolo_format(bbox, image_width, image_height):
    """eDOCr bboxë¥¼ YOLO í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    x = bbox.get('x', 0)
    y = bbox.get('y', 0)
    w = bbox.get('width', 50)
    h = bbox.get('height', 30)

    # ì¤‘ì‹¬ì  ê³„ì‚°
    x_center = (x + w / 2) / image_width
    y_center = (y + h / 2) / image_height

    # í¬ê¸° ì •ê·œí™”
    norm_width = w / image_width
    norm_height = h / image_height

    # 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
    x_center = max(0, min(1, x_center))
    y_center = max(0, min(1, y_center))
    norm_width = max(0, min(1, norm_width))
    norm_height = max(0, min(1, norm_height))

    return x_center, y_center, norm_width, norm_height

if __name__ == '__main__':
    main()
```

---

## 2. ëª¨ë¸ í•™ìŠµ ë°©ë²•

### 2.1 í•™ìŠµ í™˜ê²½ ì„¤ì •

**í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬**:
```bash
pip install ultralytics>=8.0.0
pip install torch>=2.0.0
pip install torchvision>=0.15.0
pip install opencv-python>=4.8.0
pip install pillow>=10.0.0
pip install pyyaml>=6.0
pip install tqdm
```

### 2.2 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼**: `scripts/train_yolo.py`

```python
#!/usr/bin/env python3
"""
YOLOv11 ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
"""
import argparse
from pathlib import Path
from ultralytics import YOLO
import torch

def train_model(
    model_size='n',
    data_yaml='datasets/engineering_drawings/data.yaml',
    epochs=100,
    imgsz=1280,
    batch=16,
    device='0',
    project='runs/train',
    name='engineering_drawings',
    resume=False,
    pretrained=True
):
    """
    YOLOv11 ëª¨ë¸ í•™ìŠµ

    Args:
        model_size: ëª¨ë¸ í¬ê¸° (n, s, m, l, x)
        data_yaml: ë°ì´í„°ì…‹ YAML ê²½ë¡œ
        epochs: ì—í­ ìˆ˜
        imgsz: ì´ë¯¸ì§€ í¬ê¸°
        batch: ë°°ì¹˜ í¬ê¸°
        device: GPU ë””ë°”ì´ìŠ¤
        project: í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬
        name: ì‹¤í—˜ ì´ë¦„
        resume: ì¤‘ë‹¨ëœ í•™ìŠµ ì¬ê°œ
        pretrained: ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ ì‚¬ìš©
    """

    print("=" * 70)
    print("ğŸš€ YOLOv11 Training Configuration")
    print("=" * 70)
    print(f"Model Size: yolo11{model_size}")
    print(f"Dataset: {data_yaml}")
    print(f"Epochs: {epochs}")
    print(f"Image Size: {imgsz}")
    print(f"Batch Size: {batch}")
    print(f"Device: {device}")
    print(f"Pretrained: {pretrained}")
    print("=" * 70)

    # GPU í™•ì¸
    if device != 'cpu':
        if not torch.cuda.is_available():
            print("âš ï¸  CUDA not available, using CPU")
            device = 'cpu'
        else:
            gpu_name = torch.cuda.get_device_name(0)
            print(f"âœ… Using GPU: {gpu_name}")

    # ëª¨ë¸ ë¡œë“œ
    if resume:
        print("ğŸ“‚ Resuming from last checkpoint...")
        model_path = Path(project) / name / 'weights' / 'last.pt'
        if not model_path.exists():
            print(f"âŒ Checkpoint not found: {model_path}")
            return
        model = YOLO(str(model_path))
    else:
        if pretrained:
            model_name = f'yolo11{model_size}.pt'
            print(f"ğŸ“¥ Loading pretrained model: {model_name}")
        else:
            model_name = f'yolo11{model_size}.yaml'
            print(f"ğŸ”¨ Training from scratch: {model_name}")

        model = YOLO(model_name)

    # í•™ìŠµ ì‹œì‘
    print("\nğŸ¯ Starting training...")
    print("=" * 70)

    results = model.train(
        data=data_yaml,
        epochs=epochs,
        imgsz=imgsz,
        batch=batch,
        device=device,
        project=project,
        name=name,

        # Optimization
        optimizer='AdamW',
        lr0=0.001,           # ì´ˆê¸° í•™ìŠµë¥ 
        lrf=0.01,            # ìµœì¢… í•™ìŠµë¥  (lr0 * lrf)
        momentum=0.937,
        weight_decay=0.0005,
        warmup_epochs=3.0,
        warmup_momentum=0.8,
        warmup_bias_lr=0.1,

        # Augmentation
        hsv_h=0.015,         # ìƒ‰ì¡° ì¦ê°•
        hsv_s=0.7,           # ì±„ë„ ì¦ê°•
        hsv_v=0.4,           # ëª…ë„ ì¦ê°•
        degrees=10.0,        # íšŒì „ (Â±10ë„)
        translate=0.1,       # ì´ë™
        scale=0.5,           # ìŠ¤ì¼€ì¼
        shear=0.0,           # ì „ë‹¨
        perspective=0.0,     # ì›ê·¼
        flipud=0.0,          # ìƒí•˜ ë°˜ì „ (ë„ë©´ì€ ë°©í–¥ ì¤‘ìš”)
        fliplr=0.5,          # ì¢Œìš° ë°˜ì „
        mosaic=1.0,          # ëª¨ìì´í¬ ì¦ê°•
        mixup=0.0,           # MixUp ì¦ê°•
        copy_paste=0.0,      # Copy-Paste ì¦ê°•

        # Settings
        save=True,
        save_period=10,      # 10 ì—í­ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        plots=True,
        verbose=True,
        patience=50,         # Early stopping

        # Validation
        val=True,

        # Multi-GPU (optional)
        # workers=8,
    )

    print("\n" + "=" * 70)
    print("âœ… Training complete!")
    print("=" * 70)
    print(f"ğŸ“Š Results saved to: {results.save_dir}")
    print(f"ğŸ† Best model: {results.save_dir / 'weights' / 'best.pt'}")
    print(f"ğŸ“ˆ Metrics:")
    print(f"   - mAP50: {results.results_dict.get('metrics/mAP50(B)', 'N/A')}")
    print(f"   - mAP50-95: {results.results_dict.get('metrics/mAP50-95(B)', 'N/A')}")

    return results

def main():
    parser = argparse.ArgumentParser(description='Train YOLOv11 on engineering drawings')

    parser.add_argument('--model-size', type=str, default='n',
                        choices=['n', 's', 'm', 'l', 'x'],
                        help='Model size (n=nano, s=small, m=medium, l=large, x=xlarge)')
    parser.add_argument('--data', type=str,
                        default='datasets/engineering_drawings/data.yaml',
                        help='Path to data.yaml')
    parser.add_argument('--epochs', type=int, default=100,
                        help='Number of epochs')
    parser.add_argument('--imgsz', type=int, default=1280,
                        help='Image size (high resolution for drawings)')
    parser.add_argument('--batch', type=int, default=16,
                        help='Batch size')
    parser.add_argument('--device', type=str, default='0',
                        help='CUDA device (0, 1, 2, ...) or cpu')
    parser.add_argument('--project', type=str, default='runs/train',
                        help='Project directory')
    parser.add_argument('--name', type=str, default='engineering_drawings',
                        help='Experiment name')
    parser.add_argument('--resume', action='store_true',
                        help='Resume training from last checkpoint')
    parser.add_argument('--no-pretrained', action='store_true',
                        help='Train from scratch (no pretrained weights)')

    args = parser.parse_args()

    train_model(
        model_size=args.model_size,
        data_yaml=args.data,
        epochs=args.epochs,
        imgsz=args.imgsz,
        batch=args.batch,
        device=args.device,
        project=args.project,
        name=args.name,
        resume=args.resume,
        pretrained=not args.no_pretrained
    )

if __name__ == '__main__':
    main()
```

### 2.3 í•™ìŠµ ì‹¤í–‰ ëª…ë ¹ì–´

#### ê¸°ë³¸ í•™ìŠµ (Nano ëª¨ë¸, ê¶Œì¥)
```bash
cd /home/uproot/ax/poc
python scripts/train_yolo.py \
    --model-size n \
    --epochs 100 \
    --imgsz 1280 \
    --batch 16 \
    --device 0
```

#### ê³ í•´ìƒë„ + í° ëª¨ë¸ (GPU ë©”ëª¨ë¦¬ 16GB ì´ìƒ)
```bash
python scripts/train_yolo.py \
    --model-size m \
    --epochs 150 \
    --imgsz 1920 \
    --batch 8 \
    --device 0
```

#### CPU í•™ìŠµ (ëŠë¦¼, í…ŒìŠ¤íŠ¸ìš©)
```bash
python scripts/train_yolo.py \
    --model-size n \
    --epochs 50 \
    --batch 4 \
    --device cpu
```

#### ì¤‘ë‹¨ëœ í•™ìŠµ ì¬ê°œ
```bash
python scripts/train_yolo.py --resume
```

### 2.4 í•™ìŠµ ëª¨ë‹ˆí„°ë§

í•™ìŠµ ì¤‘ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§:

```bash
# TensorBoard (ê¶Œì¥)
tensorboard --logdir runs/train

# ë˜ëŠ” ë¡œê·¸ íŒŒì¼ í™•ì¸
tail -f runs/train/engineering_drawings/results.txt
```

---

## 3. ì¶”ë¡  ë°©ë²•

### 3.1 ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼**: `scripts/inference_yolo.py`

```python
#!/usr/bin/env python3
"""
YOLOv11 ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
"""
import argparse
from pathlib import Path
from ultralytics import YOLO
import cv2
import json
import time

# í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘
CLASS_NAMES = {
    0: 'diameter_dim',
    1: 'linear_dim',
    2: 'radius_dim',
    3: 'angular_dim',
    4: 'chamfer_dim',
    5: 'tolerance_dim',
    6: 'reference_dim',
    7: 'flatness',
    8: 'cylindricity',
    9: 'position',
    10: 'perpendicularity',
    11: 'parallelism',
    12: 'surface_roughness',
    13: 'text_block'
}

def yolo_to_edocr_format(result, image_shape):
    """
    YOLO ê²°ê³¼ë¥¼ eDOCr í˜¸í™˜ í¬ë§·ìœ¼ë¡œ ë³€í™˜

    Args:
        result: YOLO detection result
        image_shape: (height, width) tuple

    Returns:
        dict: eDOCr í˜•ì‹ì˜ ê²°ê³¼
    """
    img_height, img_width = image_shape[:2]

    dimensions = []
    gdt = []
    surface_roughness = []
    text_blocks = []

    boxes = result.boxes

    for i, box in enumerate(boxes):
        # í´ë˜ìŠ¤ IDì™€ ì‹ ë¢°ë„
        cls_id = int(box.cls[0])
        confidence = float(box.conf[0])
        class_name = CLASS_NAMES.get(cls_id, 'unknown')

        # ë°”ìš´ë”© ë°•ìŠ¤ (xyxy í¬ë§·)
        x1, y1, x2, y2 = box.xyxy[0].tolist()

        # í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
        x = int(x1)
        y = int(y1)
        width = int(x2 - x1)
        height = int(y2 - y1)

        bbox = {
            'x': x,
            'y': y,
            'width': width,
            'height': height
        }

        # í´ë˜ìŠ¤ë³„ë¡œ ë¶„ë¥˜
        if cls_id <= 6:  # Dimensions (0-6)
            dimensions.append({
                'type': class_name,
                'value': '',  # OCR refinement í•„ìš”
                'unit': 'mm',
                'bbox': bbox,
                'confidence': confidence
            })

        elif cls_id <= 11:  # GD&T symbols (7-11)
            gdt.append({
                'type': class_name,
                'value': '',  # OCR refinement í•„ìš”
                'bbox': bbox,
                'confidence': confidence
            })

        elif cls_id == 12:  # Surface roughness
            surface_roughness.append({
                'value': '',  # OCR refinement í•„ìš”
                'bbox': bbox,
                'confidence': confidence
            })

        elif cls_id == 13:  # Text block
            text_blocks.append({
                'text': '',  # OCR refinement í•„ìš”
                'bbox': bbox,
                'confidence': confidence
            })

    return {
        'dimensions': dimensions,
        'gdt': gdt,
        'surface_roughness': surface_roughness,
        'text_blocks': text_blocks,
        'total_detections': len(boxes)
    }

def draw_detections(image, result):
    """
    ì´ë¯¸ì§€ì— ê²€ì¶œ ê²°ê³¼ ê·¸ë¦¬ê¸°

    Args:
        image: numpy array (BGR)
        result: YOLO detection result

    Returns:
        numpy array: ì–´ë…¸í…Œì´ì…˜ëœ ì´ë¯¸ì§€
    """
    annotated_img = image.copy()
    boxes = result.boxes

    # ìƒ‰ìƒ ì •ì˜ (BGR)
    colors = {
        'dimension': (255, 100, 0),     # Blue for dimensions
        'gdt': (0, 255, 100),           # Green for GD&T
        'surface': (0, 165, 255),       # Orange for surface
        'text': (255, 255, 0)           # Cyan for text
    }

    for box in boxes:
        cls_id = int(box.cls[0])
        confidence = float(box.conf[0])
        class_name = CLASS_NAMES.get(cls_id, 'unknown')

        # ë°”ìš´ë”© ë°•ìŠ¤
        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())

        # ìƒ‰ìƒ ì„ íƒ
        if cls_id <= 6:
            color = colors['dimension']
        elif cls_id <= 11:
            color = colors['gdt']
        elif cls_id == 12:
            color = colors['surface']
        else:
            color = colors['text']

        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        cv2.rectangle(annotated_img, (x1, y1), (x2, y2), color, 2)

        # ë¼ë²¨ ê·¸ë¦¬ê¸°
        label = f"{class_name} {confidence:.2f}"
        (label_w, label_h), _ = cv2.getTextSize(
            label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1
        )

        cv2.rectangle(
            annotated_img,
            (x1, y1 - label_h - 10),
            (x1 + label_w, y1),
            color,
            -1
        )

        cv2.putText(
            annotated_img,
            label,
            (x1, y1 - 5),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.5,
            (255, 255, 255),
            1
        )

    return annotated_img

def run_inference(
    model_path,
    source,
    output_dir='runs/inference',
    conf_threshold=0.25,
    iou_threshold=0.7,
    imgsz=1280,
    save_images=True,
    save_json=True,
    device='0'
):
    """
    YOLO ì¶”ë¡  ì‹¤í–‰

    Args:
        model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
        source: ì´ë¯¸ì§€ ë˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        conf_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
        iou_threshold: NMS IoU ì„ê³„ê°’
        imgsz: ì´ë¯¸ì§€ í¬ê¸°
        save_images: ì–´ë…¸í…Œì´ì…˜ëœ ì´ë¯¸ì§€ ì €ì¥
        save_json: JSON ê²°ê³¼ ì €ì¥
        device: GPU ë””ë°”ì´ìŠ¤
    """

    print("=" * 70)
    print("ğŸ” YOLOv11 Inference")
    print("=" * 70)
    print(f"Model: {model_path}")
    print(f"Source: {source}")
    print(f"Confidence threshold: {conf_threshold}")
    print(f"Image size: {imgsz}")
    print("=" * 70)

    # ëª¨ë¸ ë¡œë“œ
    model = YOLO(model_path)

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # ì¶”ë¡  ì‹¤í–‰
    start_time = time.time()

    results = model.predict(
        source=source,
        conf=conf_threshold,
        iou=iou_threshold,
        imgsz=imgsz,
        device=device,
        save=False,  # ìš°ë¦¬ê°€ ì§ì ‘ ì €ì¥
        verbose=True
    )

    elapsed_time = time.time() - start_time

    # ê²°ê³¼ ì²˜ë¦¬
    print(f"\nğŸ“Š Processing {len(results)} images...")

    all_results = []

    for i, result in enumerate(results):
        image_path = Path(result.path)
        image_name = image_path.stem

        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(str(image_path))

        # eDOCr í¬ë§·ìœ¼ë¡œ ë³€í™˜
        detection_result = yolo_to_edocr_format(result, image.shape)

        detection_result['image_name'] = image_name
        detection_result['image_path'] = str(image_path)
        detection_result['model'] = str(model_path)
        detection_result['inference_time'] = elapsed_time / len(results)

        all_results.append(detection_result)

        # í†µê³„ ì¶œë ¥
        print(f"âœ… {image_name}: {detection_result['total_detections']} detections")

        # ì–´ë…¸í…Œì´ì…˜ëœ ì´ë¯¸ì§€ ì €ì¥
        if save_images:
            annotated_img = draw_detections(image, result)
            save_path = output_path / f"{image_name}_annotated.jpg"
            cv2.imwrite(str(save_path), annotated_img)

        # JSON ì €ì¥
        if save_json:
            json_path = output_path / f"{image_name}_result.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(detection_result, f, indent=2, ensure_ascii=False)

    # ì „ì²´ í†µê³„
    total_detections = sum(r['total_detections'] for r in all_results)
    avg_time = elapsed_time / len(results)

    print("\n" + "=" * 70)
    print("âœ… Inference complete!")
    print("=" * 70)
    print(f"ğŸ“Š Statistics:")
    print(f"   - Total images: {len(results)}")
    print(f"   - Total detections: {total_detections}")
    print(f"   - Average detections/image: {total_detections / len(results):.1f}")
    print(f"   - Total time: {elapsed_time:.2f}s")
    print(f"   - Average time/image: {avg_time:.2f}s")
    print(f"   - FPS: {1/avg_time:.2f}")
    print(f"ğŸ“ Results saved to: {output_path}")

    # ì „ì²´ ìš”ì•½ ì €ì¥
    summary = {
        'total_images': len(results),
        'total_detections': total_detections,
        'average_detections_per_image': total_detections / len(results),
        'total_time': elapsed_time,
        'average_time_per_image': avg_time,
        'fps': 1 / avg_time,
        'results': all_results
    }

    summary_path = output_path / 'summary.json'
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    return summary

def main():
    parser = argparse.ArgumentParser(description='YOLOv11 Inference on engineering drawings')

    parser.add_argument('--model', type=str, required=True,
                        help='Path to trained model (best.pt)')
    parser.add_argument('--source', type=str, required=True,
                        help='Image file or directory')
    parser.add_argument('--output', type=str, default='runs/inference',
                        help='Output directory')
    parser.add_argument('--conf', type=float, default=0.25,
                        help='Confidence threshold')
    parser.add_argument('--iou', type=float, default=0.7,
                        help='NMS IoU threshold')
    parser.add_argument('--imgsz', type=int, default=1280,
                        help='Image size')
    parser.add_argument('--device', type=str, default='0',
                        help='CUDA device or cpu')
    parser.add_argument('--no-save-images', action='store_true',
                        help='Do not save annotated images')
    parser.add_argument('--no-save-json', action='store_true',
                        help='Do not save JSON results')

    args = parser.parse_args()

    run_inference(
        model_path=args.model,
        source=args.source,
        output_dir=args.output,
        conf_threshold=args.conf,
        iou_threshold=args.iou,
        imgsz=args.imgsz,
        save_images=not args.no_save_images,
        save_json=not args.no_save_json,
        device=args.device
    )

if __name__ == '__main__':
    main()
```

### 3.2 ì¶”ë¡  ì‹¤í–‰ ëª…ë ¹ì–´

#### ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡ 
```bash
python scripts/inference_yolo.py \
    --model runs/train/engineering_drawings/weights/best.pt \
    --source test_images/drawing_001.jpg \
    --output runs/inference/test
```

#### ë””ë ‰í† ë¦¬ ë°°ì¹˜ ì¶”ë¡ 
```bash
python scripts/inference_yolo.py \
    --model runs/train/engineering_drawings/weights/best.pt \
    --source test_images/ \
    --conf 0.25 \
    --imgsz 1280
```

#### ê³ ì‹ ë¢°ë„ë§Œ ê²€ì¶œ
```bash
python scripts/inference_yolo.py \
    --model runs/train/engineering_drawings/weights/best.pt \
    --source test_image.jpg \
    --conf 0.5  # 50% ì´ìƒë§Œ
```

---

## 4. API ì„œë²„ êµ¬ì¶•

### 4.1 FastAPI ì„œë²„

**íŒŒì¼**: `yolo-api/api_server.py` (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ êµ¬í˜„)

### 4.2 Docker ì„¤ì •

**íŒŒì¼**: `yolo-api/Dockerfile` (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ êµ¬í˜„)

---

## 5. Gateway í†µí•©

Gateway APIì— YOLO ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ (ë‹¤ìŒ ë‹¨ê³„)

---

## 6. ì„±ëŠ¥ í‰ê°€

### 6.1 í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼**: `scripts/evaluate_yolo.py`

```python
#!/usr/bin/env python3
"""
YOLOv11 ëª¨ë¸ í‰ê°€
"""
import argparse
from ultralytics import YOLO

def evaluate_model(
    model_path,
    data_yaml='datasets/engineering_drawings/data.yaml',
    split='test',
    imgsz=1280,
    device='0'
):
    """
    ëª¨ë¸ í‰ê°€ ì‹¤í–‰
    """
    print("=" * 70)
    print("ğŸ“Š YOLOv11 Model Evaluation")
    print("=" * 70)
    print(f"Model: {model_path}")
    print(f"Dataset: {data_yaml}")
    print(f"Split: {split}")
    print("=" * 70)

    # ëª¨ë¸ ë¡œë“œ
    model = YOLO(model_path)

    # í‰ê°€ ì‹¤í–‰
    metrics = model.val(
        data=data_yaml,
        split=split,
        imgsz=imgsz,
        device=device,
        save_json=True,
        save_hybrid=True,
        plots=True
    )

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ğŸ“ˆ Evaluation Results")
    print("=" * 70)
    print(f"Precision: {metrics.box.p:.4f}")
    print(f"Recall: {metrics.box.r:.4f}")
    print(f"mAP50: {metrics.box.map50:.4f}")
    print(f"mAP50-95: {metrics.box.map:.4f}")
    print(f"F1 Score: {2 * (metrics.box.p * metrics.box.r) / (metrics.box.p + metrics.box.r):.4f}")

    return metrics

def main():
    parser = argparse.ArgumentParser(description='Evaluate YOLOv11 model')

    parser.add_argument('--model', type=str, required=True,
                        help='Path to model weights')
    parser.add_argument('--data', type=str,
                        default='datasets/engineering_drawings/data.yaml',
                        help='Path to data.yaml')
    parser.add_argument('--split', type=str, default='test',
                        choices=['train', 'val', 'test'],
                        help='Dataset split to evaluate')
    parser.add_argument('--imgsz', type=int, default=1280,
                        help='Image size')
    parser.add_argument('--device', type=str, default='0',
                        help='CUDA device or cpu')

    args = parser.parse_args()

    evaluate_model(
        model_path=args.model,
        data_yaml=args.data,
        split=args.split,
        imgsz=args.imgsz,
        device=args.device
    )

if __name__ == '__main__':
    main()
```

---

## ìš”ì•½: ì „ì²´ ì›Œí¬í”Œë¡œìš°

```
1. ë°ì´í„°ì…‹ ì¤€ë¹„
   â””â”€> python scripts/prepare_dataset.py

2. ëª¨ë¸ í•™ìŠµ
   â””â”€> python scripts/train_yolo.py --model-size n --epochs 100

3. ëª¨ë¸ í‰ê°€
   â””â”€> python scripts/evaluate_yolo.py --model runs/train/engineering_drawings/weights/best.pt

4. ì¶”ë¡  í…ŒìŠ¤íŠ¸
   â””â”€> python scripts/inference_yolo.py --model best.pt --source test_images/

5. API ì„œë²„ êµ¬ì¶•
   â””â”€> (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ êµ¬í˜„)

6. Gateway í†µí•©
   â””â”€> (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ êµ¬í˜„)
```

---

**ë‹¤ìŒ ë¬¸ì„œ**: API ì„œë²„ êµ¬ì¶• ë° Docker ë°°í¬
**ì‘ì„±ì**: Claude 3.7 Sonnet
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-31
