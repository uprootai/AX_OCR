"""
EDGNet API Server
ê·¸ë˜í”„ ì‹ ê²½ë§ ê¸°ë°˜ ë„ë©´ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤

í¬íŠ¸: 5002
ê¸°ëŠ¥: ë„ë©´ì„ Contour/Text/Dimension ì»´í¬ë„ŒíŠ¸ë¡œ ë¶„ë¥˜
"""

import os
import sys
import json
import time
import shutil
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List
from datetime import datetime

from fastapi import FastAPI, File, UploadFile, Form, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
import numpy as np

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add edgnet to path
# In Docker container, EDGNet is mounted at /app/edgnet
EDGNET_PATH = Path("/app/edgnet")
if not EDGNET_PATH.exists():
    # Fallback to relative path for local development
    EDGNET_PATH = Path(__file__).parent.parent.parent / "dev" / "edgnet"
sys.path.insert(0, str(EDGNET_PATH))
logger.info(f"EDGNet path: {EDGNET_PATH}")

# Import EDGNet pipeline
try:
    from pipeline import EDGNetPipeline
    EDGNET_AVAILABLE = True
    logger.info("âœ… EDGNet pipeline imported successfully")
except ImportError as e:
    EDGNET_AVAILABLE = False
    logger.warning(f"âš ï¸ EDGNet pipeline import failed: {e}")
    logger.warning("Will use mock data for segmentation")

# Initialize FastAPI
app = FastAPI(
    title="EDGNet API",
    description="Engineering Drawing Graph Neural Network Segmentation Service",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration
UPLOAD_DIR = Path("/tmp/edgnet/uploads")
RESULTS_DIR = Path("/tmp/edgnet/results")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'tiff', 'bmp'}


# =====================
# Pydantic Models
# =====================

class HealthResponse(BaseModel):
    status: str
    service: str
    version: str
    timestamp: str


class SegmentRequest(BaseModel):
    visualize: bool = Field(True, description="ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± ì—¬ë¶€")
    num_classes: int = Field(3, description="ë¶„ë¥˜ í´ë˜ìŠ¤ ìˆ˜ (2 or 3)")
    save_graph: bool = Field(False, description="ê·¸ë˜í”„ ë°ì´í„° ì €ì¥ ì—¬ë¶€")


class ClassificationStats(BaseModel):
    contour: int = Field(0, description="ìœ¤ê³½ì„  ì»´í¬ë„ŒíŠ¸ ìˆ˜")
    text: int = Field(0, description="í…ìŠ¤íŠ¸ ì»´í¬ë„ŒíŠ¸ ìˆ˜")
    dimension: int = Field(0, description="ì¹˜ìˆ˜ ì»´í¬ë„ŒíŠ¸ ìˆ˜")


class GraphStats(BaseModel):
    nodes: int
    edges: int
    avg_degree: float


class SegmentResponse(BaseModel):
    status: str
    data: Dict[str, Any]
    processing_time: float
    file_id: str


class VectorizeRequest(BaseModel):
    save_bezier: bool = Field(True, description="Bezier ê³¡ì„  ì €ì¥ ì—¬ë¶€")


class VectorizeResponse(BaseModel):
    status: str
    data: Dict[str, Any]
    processing_time: float
    file_id: str


# =====================
# Helper Functions
# =====================

def allowed_file(filename: str) -> bool:
    """íŒŒì¼ í™•ì¥ì ê²€ì¦"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


def bezier_to_bbox(bezier_curve, n_samples=50):
    """
    Convert Bezier curve to bounding box

    Args:
        bezier_curve: Bezier curve object with evaluate() method
        n_samples: Number of points to sample for bbox calculation

    Returns:
        dict: {'x': int, 'y': int, 'width': int, 'height': int}
    """
    try:
        t_vals = np.linspace(0, 1, n_samples)
        points = bezier_curve.evaluate(t_vals)

        x_min = int(np.min(points[:, 0]))
        y_min = int(np.min(points[:, 1]))
        x_max = int(np.max(points[:, 0]))
        y_max = int(np.max(points[:, 1]))

        return {
            'x': x_min,
            'y': y_min,
            'width': x_max - x_min,
            'height': y_max - y_min
        }
    except Exception as e:
        logger.error(f"Failed to compute bbox: {e}")
        return {'x': 0, 'y': 0, 'width': 0, 'height': 0}


def process_segmentation(
    file_path: Path,
    visualize: bool = True,
    num_classes: int = 3,
    save_graph: bool = False
) -> Dict[str, Any]:
    """
    ë„ë©´ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬ (ì‹¤ì œ EDGNet íŒŒì´í”„ë¼ì¸ ì‚¬ìš©)
    """
    try:
        logger.info(f"Processing file: {file_path}")
        logger.info(f"Options: visualize={visualize}, num_classes={num_classes}")

        if not EDGNET_AVAILABLE:
            # EDGNet íŒŒì´í”„ë¼ì¸ì´ ì—†ìœ¼ë©´ ëª…ì‹œì  ì—ëŸ¬ ë°˜í™˜
            logger.error("âŒ EDGNet pipeline not available")
            raise HTTPException(
                status_code=503,
                detail="EDGNet pipeline not available. Please install EDGNet dependencies."
            )

        # Initialize EDGNet pipeline with model
        # Model is mounted at /models/ inside the container
        model_path = Path("/models/graphsage_dimension_classifier.pth")

        if not model_path.exists():
            # ëª¨ë¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ëª…ì‹œì  ì—ëŸ¬ ë°˜í™˜
            logger.error(f"âŒ Model not found: {model_path}")
            raise HTTPException(
                status_code=503,
                detail=f"EDGNet model not found at {model_path}. Please download the model file."
            )

        logger.info(f"Loading model from: {model_path}")

        # Auto-detect GPU availability / GPU ìë™ ê°ì§€
        try:
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.info(f"Using device: {device}")
            if device == 'cuda':
                logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
                logger.info(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        except ImportError:
            device = 'cpu'
            logger.warning("PyTorch not available, using CPU")

        pipeline = EDGNetPipeline(model_path=str(model_path), device=device)

        # Process drawing
        logger.info("Running EDGNet pipeline...")
        output_dir = RESULTS_DIR / file_path.stem if save_graph else None
        pipeline_result = pipeline.process_drawing(
            str(file_path),
            output_dir=output_dir,
            visualize=visualize
        )

        # Extract results
        bezier_curves = pipeline_result['bezier_curves']
        predictions = pipeline_result['predictions']
        G = pipeline_result['graph']

        logger.info(f"âœ… Pipeline complete: {len(bezier_curves)} components")

        # Count classifications
        # Class mapping from training: 0=Dimension, 1=Text, 2=Contour, 3=Other
        class_counts = {"dimension": 0, "text": 0, "contour": 0, "other": 0}
        class_map = {0: "dimension", 1: "text", 2: "contour", 3: "other"}

        if predictions is not None:
            unique, counts = np.unique(predictions, return_counts=True)
            for cls, cnt in zip(unique, counts):
                class_name = class_map.get(int(cls), "unknown")
                if class_name in class_counts:
                    class_counts[class_name] = int(cnt)

        # Build components list with bboxes
        components = []
        if predictions is not None:
            for i, (bezier, pred) in enumerate(zip(bezier_curves, predictions)):
                bbox = bezier_to_bbox(bezier)
                classification = class_map.get(int(pred), "unknown")

                components.append({
                    "id": i,
                    "classification": classification,
                    "bbox": bbox,
                    "confidence": 0.9  # EDGNet doesn't provide confidence scores
                })

        # Calculate graph stats
        avg_degree = (2 * G.number_of_edges() / G.number_of_nodes()) if G.number_of_nodes() > 0 else 0

        result = {
            "num_components": len(bezier_curves),
            "classifications": class_counts,
            "graph": {
                "nodes": G.number_of_nodes(),
                "edges": G.number_of_edges(),
                "avg_degree": round(avg_degree, 2)
            },
            "vectorization": {
                "num_bezier_curves": len(bezier_curves),
                "total_length": 0  # Would need to calculate from bezier curves
            },
            "components": components  # NEW: Actual component data with bboxes
        }

        if visualize and output_dir:
            result["visualization_url"] = f"/api/v1/result/{file_path.stem}/predictions.png"

        if save_graph and output_dir:
            result["graph_url"] = f"/api/v1/result/{file_path.stem}/graph.pkl"

        logger.info(f"âœ… Segmentation complete: {class_counts}")
        return result

    except Exception as e:
        logger.error(f"âŒ Segmentation failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Segmentation failed: {str(e)}")


def process_vectorization(
    file_path: Path,
    save_bezier: bool = True
) -> Dict[str, Any]:
    """
    ë„ë©´ ë²¡í„°í™” ì²˜ë¦¬

    TODO: ì‹¤ì œ ë²¡í„°í™” íŒŒì´í”„ë¼ì¸ ì—°ë™
    """
    try:
        logger.info(f"Vectorizing file: {file_path}")

        # Simulate processing
        time.sleep(2)

        result = {
            "num_curves": 150,
            "curve_types": {
                "line": 85,
                "arc": 45,
                "bezier": 20
            },
            "total_length": 12450.5,
            "processing_steps": {
                "skeletonization": "completed",
                "tracing": "completed",
                "bezier_fitting": "completed"
            }
        }

        if save_bezier:
            result["bezier_file"] = f"/api/v1/result/{file_path.stem}_curves.json"

        return result

    except Exception as e:
        logger.error(f"Vectorization failed: {e}")
        raise HTTPException(status_code=500, detail=f"Vectorization failed: {str(e)}")


def cleanup_old_files(directory: Path, max_age_hours: int = 24):
    """ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ"""
    try:
        current_time = time.time()
        for file_path in directory.glob("*"):
            if file_path.is_file():
                file_age = current_time - file_path.stat().st_mtime
                if file_age > max_age_hours * 3600:
                    file_path.unlink()
                    logger.info(f"Deleted old file: {file_path}")
    except Exception as e:
        logger.error(f"Cleanup failed: {e}")


# =====================
# Startup Event
# =====================

@app.on_event("startup")
async def startup_event():
    """Validate EDGNet pipeline and model on startup"""
    logger.info("ğŸš€ Starting EDGNet API...")
    logger.info("ğŸ” Validating EDGNet pipeline...")

    if not EDGNET_AVAILABLE:
        logger.error("âŒ EDGNet pipeline NOT available")
        logger.error("   Install EDGNet from: https://github.com/[repository_url]")
        logger.error("   EDGNet API will return 503 errors until pipeline is installed")
    else:
        logger.info("âœ… EDGNet pipeline available")

        # Check model file
        model_path = Path("/models/graphsage_dimension_classifier.pth")
        if not model_path.exists():
            logger.error(f"âŒ Model file NOT found: {model_path}")
            logger.error("   Download model from: [model_url]")
            logger.error("   EDGNet API will return 503 errors until model is available")
        else:
            logger.info(f"âœ… Model file found: {model_path}")
            logger.info("âœ… EDGNet API ready for segmentation")

    logger.info("âœ… EDGNet API startup complete")


# =====================
# API Endpoints
# =====================

@app.get("/", response_model=HealthResponse)
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "online",
        "service": "EDGNet API",
        "version": "1.0.0",
        "timestamp": datetime.now().isoformat()
    }


@app.get("/health", response_model=HealthResponse)
@app.get("/api/v1/health", response_model=HealthResponse)
async def health_check():
    """
    Health check endpoint / í—¬ìŠ¤ì²´í¬

    Returns the current health status of the EDGNet API service.
    """
    # Check if pipeline and model are available
    model_path = Path("/models/graphsage_dimension_classifier.pth")
    is_ready = EDGNET_AVAILABLE and model_path.exists()

    status = "healthy" if is_ready else "degraded"

    return {
        "status": status,
        "service": "EDGNet API",
        "version": "1.0.0",
        "timestamp": datetime.now().isoformat()
    }


@app.post("/api/v1/segment", response_model=SegmentResponse)
async def segment_drawing(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(..., description="ë„ë©´ ì´ë¯¸ì§€ (PNG, JPG)"),
    visualize: bool = Form(True, description="ì‹œê°í™” ìƒì„±"),
    num_classes: int = Form(3, description="ë¶„ë¥˜ í´ë˜ìŠ¤ ìˆ˜ (2 or 3)"),
    save_graph: bool = Form(False, description="ê·¸ë˜í”„ ì €ì¥")
):
    """
    ë„ë©´ ì„¸ê·¸ë©˜í…Œì´ì…˜ - ì»´í¬ë„ŒíŠ¸ ë¶„ë¥˜

    - **file**: ë„ë©´ ì´ë¯¸ì§€ (PNG, JPG, TIFF)
    - **visualize**: ë¶„ë¥˜ ê²°ê³¼ ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„± ì—¬ë¶€
    - **num_classes**: 2 (Text/Non-text) ë˜ëŠ” 3 (Contour/Text/Dimension)
    - **save_graph**: ê·¸ë˜í”„ êµ¬ì¡° JSON ì €ì¥ ì—¬ë¶€
    """
    start_time = time.time()

    # íŒŒì¼ ê²€ì¦
    if not allowed_file(file.filename):
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file type. Allowed: {', '.join(ALLOWED_EXTENSIONS)}"
        )

    # íŒŒì¼ í¬ê¸° ì²´í¬
    file.file.seek(0, 2)
    file_size = file.file.tell()
    file.file.seek(0)

    if file_size > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"File too large. Max size: {MAX_FILE_SIZE / 1024 / 1024}MB"
        )

    # íŒŒì¼ ì €ì¥
    file_id = f"{int(time.time())}_{file.filename}"
    file_path = UPLOAD_DIR / file_id

    try:
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        logger.info(f"File uploaded: {file_id} ({file_size / 1024:.2f} KB)")

        # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬
        segment_result = process_segmentation(
            file_path,
            visualize=visualize,
            num_classes=num_classes,
            save_graph=save_graph
        )

        processing_time = time.time() - start_time

        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…: ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ
        background_tasks.add_task(cleanup_old_files, UPLOAD_DIR)

        return {
            "status": "success",
            "data": segment_result,
            "processing_time": round(processing_time, 2),
            "file_id": file_id
        }

    except Exception as e:
        logger.error(f"Error processing file: {e}")
        if file_path.exists():
            file_path.unlink()
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/vectorize", response_model=VectorizeResponse)
async def vectorize_drawing(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(..., description="ë„ë©´ ì´ë¯¸ì§€"),
    save_bezier: bool = Form(True, description="Bezier ê³¡ì„  ì €ì¥")
):
    """
    ë„ë©´ ë²¡í„°í™”

    - **file**: ë„ë©´ ì´ë¯¸ì§€ (PNG, JPG)
    - **save_bezier**: Bezier ê³¡ì„  ë°ì´í„° JSON ì €ì¥ ì—¬ë¶€
    """
    start_time = time.time()

    # íŒŒì¼ ê²€ì¦
    if not allowed_file(file.filename):
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file type. Allowed: {', '.join(ALLOWED_EXTENSIONS)}"
        )

    # íŒŒì¼ ì €ì¥
    file_id = f"{int(time.time())}_{file.filename}"
    file_path = UPLOAD_DIR / file_id

    try:
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        logger.info(f"File uploaded for vectorization: {file_id}")

        # ë²¡í„°í™” ì²˜ë¦¬
        vectorize_result = process_vectorization(
            file_path,
            save_bezier=save_bezier
        )

        processing_time = time.time() - start_time

        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
        background_tasks.add_task(cleanup_old_files, UPLOAD_DIR)

        return {
            "status": "success",
            "data": vectorize_result,
            "processing_time": round(processing_time, 2),
            "file_id": file_id
        }

    except Exception as e:
        logger.error(f"Error vectorizing file: {e}")
        if file_path.exists():
            file_path.unlink()
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/result/{filename}")
async def get_result_file(filename: str):
    """ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    file_path = RESULTS_DIR / filename

    if not file_path.exists():
        raise HTTPException(status_code=404, detail="File not found")

    return FileResponse(
        path=file_path,
        media_type='application/octet-stream',
        filename=filename
    )


@app.delete("/api/v1/cleanup")
async def cleanup_files(max_age_hours: int = 24):
    """ìˆ˜ë™ íŒŒì¼ ì •ë¦¬"""
    try:
        cleanup_old_files(UPLOAD_DIR, max_age_hours)
        cleanup_old_files(RESULTS_DIR, max_age_hours)
        return {"status": "success", "message": f"Cleaned up files older than {max_age_hours} hours"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# =====================
# Main
# =====================

if __name__ == "__main__":
    port = int(os.getenv("EDGNET_PORT", 5002))
    workers = int(os.getenv("EDGNET_WORKERS", 2))

    logger.info(f"Starting EDGNet API on port {port} with {workers} workers")

    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=port,
        workers=workers,
        log_level="info",
        reload=False
    )
