"""
Pipeline Engine
ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì—”ì§„ - DAG ê¸°ë°˜ ë™ì  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
"""
import asyncio
import uuid
import time
import logging
import json
import os
import orjson  # ê³ ì„±ëŠ¥ JSON ì§ë ¬í™”
from typing import Dict, Any, Optional, AsyncGenerator

from ..schemas.workflow import (
    WorkflowDefinition,
    WorkflowExecutionResponse,
)
from ..validators.dag_validator import DAGValidator
from ..executors.executor_registry import ExecutorRegistry
from .execution_context import ExecutionContext
from .input_collector import collect_node_inputs

# ê²°ê³¼ ì €ì¥ ê¸°ëŠ¥
try:
    from utils.result_manager import get_result_manager
    RESULT_SAVING_ENABLED = os.getenv("ENABLE_RESULT_SAVING", "true").lower() == "true"
except ImportError:
    RESULT_SAVING_ENABLED = False
    get_result_manager = None

logger = logging.getLogger(__name__)


class PipelineEngine:
    """ì›Œí¬í”Œë¡œìš° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì—”ì§„"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.result_manager = get_result_manager() if RESULT_SAVING_ENABLED and get_result_manager else None
        self.node_execution_order = 0  # ë…¸ë“œ ì‹¤í–‰ ìˆœì„œ ì¶”ì 
        # ì‹¤í–‰ ì¤‘ì¸ ì›Œí¬í”Œë¡œìš° ì¶”ì  (ì·¨ì†Œìš©)
        self._running_executions: Dict[str, asyncio.Event] = {}

    def cancel_execution(self, execution_id: str) -> bool:
        """
        ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì·¨ì†Œ

        Args:
            execution_id: ì·¨ì†Œí•  ì‹¤í–‰ ID

        Returns:
            ì·¨ì†Œ ì„±ê³µ ì—¬ë¶€
        """
        if execution_id in self._running_executions:
            self.logger.info(f"ğŸ›‘ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì·¨ì†Œ ìš”ì²­: {execution_id}")
            self._running_executions[execution_id].set()
            return True
        else:
            self.logger.warning(f"ì·¨ì†Œí•  ì‹¤í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {execution_id}")
            return False

    def get_running_executions(self) -> list:
        """í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì›Œí¬í”Œë¡œìš° ëª©ë¡"""
        return list(self._running_executions.keys())

    def _is_cancelled(self, execution_id: str) -> bool:
        """ì‹¤í–‰ì´ ì·¨ì†Œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        if execution_id in self._running_executions:
            return self._running_executions[execution_id].is_set()
        return False

    async def execute_workflow(
        self,
        workflow: WorkflowDefinition,
        inputs: Dict[str, Any],
        config: Optional[Dict[str, Any]] = None,
    ) -> WorkflowExecutionResponse:
        """
        ì›Œí¬í”Œë¡œìš° ì‹¤í–‰

        Args:
            workflow: ì›Œí¬í”Œë¡œìš° ì •ì˜
            inputs: ì´ˆê¸° ì…ë ¥ ë°ì´í„°
            config: ì‹¤í–‰ ì„¤ì • (execution_mode: 'sequential' | 'parallel')

        Returns:
            ì‹¤í–‰ ê²°ê³¼
        """
        execution_id = str(uuid.uuid4())

        # ì‹¤í–‰ ëª¨ë“œ ê²°ì • (ê¸°ë³¸ê°’: sequential)
        execution_mode = (config or {}).get("execution_mode", "sequential")
        self.logger.info(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘: {workflow.name} (ID: {execution_id}, ëª¨ë“œ: {execution_mode})")

        try:
            # 1. ê²€ì¦ ë‹¨ê³„
            self.logger.info("1ë‹¨ê³„: DAG ê²€ì¦")
            nodes_dict = [node.model_dump() for node in workflow.nodes]
            edges_dict = [edge.model_dump() for edge in workflow.edges]

            validator = DAGValidator(nodes_dict, edges_dict)
            is_valid, errors = validator.validate_all()

            if not is_valid:
                error_msg = "; ".join(errors)
                self.logger.error(f"DAG ê²€ì¦ ì‹¤íŒ¨: {error_msg}")
                return WorkflowExecutionResponse(
                    execution_id=execution_id,
                    status="failed",
                    workflow_name=workflow.name,
                    node_statuses=[],
                    error=f"ì›Œí¬í”Œë¡œìš° ê²€ì¦ ì‹¤íŒ¨: {error_msg}",
                )

            # 2. ì‹¤í–‰ ì¤€ë¹„
            self.logger.info("2ë‹¨ê³„: ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”")
            context = ExecutionContext(execution_id, workflow, inputs)

            # ëª¨ë“  ë…¸ë“œë¥¼ pending ìƒíƒœë¡œ ì´ˆê¸°í™”
            for node in workflow.nodes:
                context.set_node_status(node.id, "pending")

            # 3. ìœ„ìƒ ì •ë ¬
            self.logger.info("3ë‹¨ê³„: ìœ„ìƒ ì •ë ¬ ë° ì‹¤í–‰ ìˆœì„œ ê²°ì •")
            sorted_nodes = validator.topological_sort()
            self.logger.info(f"ì‹¤í–‰ ìˆœì„œ: {' -> '.join(sorted_nodes)}")

            # 4. ë³‘ë ¬ ê·¸ë£¹ ì‹ë³„
            parallel_groups = validator.find_parallel_groups()
            self.logger.info(f"ë³‘ë ¬ ì‹¤í–‰ ê·¸ë£¹: {parallel_groups}")

            # 5. ë…¸ë“œ ì‹¤í–‰
            self.logger.info("4ë‹¨ê³„: ë…¸ë“œ ì‹¤í–‰")

            if execution_mode == "sequential":
                # ìˆœì°¨ ì‹¤í–‰: ìœ„ìƒ ì •ë ¬ ìˆœì„œëŒ€ë¡œ í•˜ë‚˜ì”© ì‹¤í–‰
                self.logger.info("ğŸ”„ ìˆœì°¨ ì‹¤í–‰ ëª¨ë“œ")
                for node_id in sorted_nodes:
                    try:
                        self.logger.info(f"ë…¸ë“œ ì‹¤í–‰: {node_id}")
                        await self._execute_node(node_id, workflow, context)
                    except Exception as e:
                        self.logger.error(f"ë…¸ë“œ {node_id} ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
                        context.set_node_status(node_id, "failed", error=str(e))
            else:
                # ë³‘ë ¬ ì‹¤í–‰: ë³‘ë ¬ ê·¸ë£¹ ë‹¨ìœ„ë¡œ ì‹¤í–‰
                self.logger.info("âš¡ ë³‘ë ¬ ì‹¤í–‰ ëª¨ë“œ")
                for group_idx, node_group in enumerate(parallel_groups):
                    self.logger.info(f"ê·¸ë£¹ {group_idx + 1}/{len(parallel_groups)} ì‹¤í–‰: {node_group}")

                    if len(node_group) > 1:
                        # ë³‘ë ¬ ì‹¤í–‰
                        tasks = [
                            self._execute_node(node_id, workflow, context)
                            for node_id in node_group
                        ]
                        results = await asyncio.gather(*tasks, return_exceptions=True)

                        # ì—ëŸ¬ ì²´í¬
                        for node_id, result in zip(node_group, results):
                            if isinstance(result, Exception):
                                self.logger.error(f"ë…¸ë“œ {node_id} ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {result}")
                                context.set_node_status(node_id, "failed", error=str(result))
                    else:
                        # ë‹¨ì¼ ì‹¤í–‰
                        node_id = node_group[0]
                        try:
                            await self._execute_node(node_id, workflow, context)
                        except Exception as e:
                            self.logger.error(f"ë…¸ë“œ {node_id} ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
                            context.set_node_status(node_id, "failed", error=str(e))

            # 6. ê²°ê³¼ ì§‘ê³„
            self.logger.info("5ë‹¨ê³„: ê²°ê³¼ ì§‘ê³„")
            execution_time = (time.time() - context.start_time) * 1000

            # ìµœì¢… ì¶œë ¥ ê²°ì • (ë¦¬í”„ ë…¸ë“œë“¤ì˜ ì¶œë ¥)
            leaf_nodes = validator.find_leaf_nodes()
            final_output = {}
            for leaf_id in leaf_nodes:
                leaf_output = context.get_node_output(leaf_id)
                if leaf_output:
                    final_output[leaf_id] = leaf_output

            # ì „ì²´ ì‹¤í–‰ ìƒíƒœ ê²°ì •
            has_failed = any(s.status == "failed" for s in context.node_statuses.values())
            overall_status = "failed" if has_failed else "completed"

            self.logger.info(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ: {overall_status} (ì†Œìš” ì‹œê°„: {execution_time:.2f}ms)")

            return WorkflowExecutionResponse(
                execution_id=execution_id,
                status=overall_status,
                workflow_name=workflow.name,
                node_statuses=list(context.node_statuses.values()),
                final_output=final_output if final_output else None,
                execution_time_ms=execution_time,
            )

        except Exception as e:
            self.logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
            return WorkflowExecutionResponse(
                execution_id=execution_id,
                status="failed",
                workflow_name=workflow.name,
                node_statuses=[],
                error=f"ì‹¤í–‰ ì—”ì§„ ì—ëŸ¬: {str(e)}",
            )

    async def _execute_node(
        self,
        node_id: str,
        workflow: WorkflowDefinition,
        context: ExecutionContext,
    ):
        """
        ë‹¨ì¼ ë…¸ë“œ ì‹¤í–‰

        Args:
            node_id: ë…¸ë“œ ID
            workflow: ì›Œí¬í”Œë¡œìš° ì •ì˜
            context: ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸
        """
        # ë…¸ë“œ ì •ì˜ ì°¾ê¸°
        node = next((n for n in workflow.nodes if n.id == node_id), None)
        if not node:
            raise ValueError(f"ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {node_id}")

        self.logger.info(f"ë…¸ë“œ ì‹¤í–‰ ì‹œì‘: {node.id} ({node.type})")
        self.logger.info(f"ë…¸ë“œ íŒŒë¼ë¯¸í„°: {node.parameters}")  # ë””ë²„ê¹…ìš©
        context.set_node_status(node.id, "running", progress=0.0)

        try:
            # ì…ë ¥ ë°ì´í„° ìˆ˜ì§‘
            inputs = await collect_node_inputs(node_id, workflow, context)

            # Executor ìƒì„±
            executor = ExecutorRegistry.create(
                node_id=node.id,
                node_type=node.type,
                parameters=node.parameters,
            )

            # ì‹¤í–‰
            result = await executor.run(inputs, context.global_vars)

            if result["success"]:
                # ì„±ê³µ
                context.set_node_output(node.id, result["data"])
                context.set_node_status(
                    node.id,
                    "completed",
                    progress=1.0,
                    output=result["data"],
                )
                self.logger.info(f"ë…¸ë“œ ì‹¤í–‰ ì™„ë£Œ: {node.id}")
            else:
                # ì‹¤íŒ¨
                context.set_node_status(
                    node.id,
                    "failed",
                    progress=0.0,
                    error=result.get("error", "Unknown error"),
                )
                self.logger.error(f"ë…¸ë“œ ì‹¤í–‰ ì‹¤íŒ¨: {node.id} - {result.get('error')}")

        except Exception as e:
            context.set_node_status(node.id, "failed", error=str(e))
            self.logger.error(f"ë…¸ë“œ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {node.id} - {e}", exc_info=True)
            raise

    async def execute_workflow_stream(
        self,
        workflow: WorkflowDefinition,
        inputs: Dict[str, Any],
        config: Optional[Dict[str, Any]] = None,
    ) -> AsyncGenerator[str, None]:
        """
        ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (SSE ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)

        ì‹¤í–‰ ì¤‘ ê° ë…¸ë“œì˜ ìƒíƒœ ë³€í™”ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ yieldí•©ë‹ˆë‹¤.

        Args:
            workflow: ì›Œí¬í”Œë¡œìš° ì •ì˜
            inputs: ì´ˆê¸° ì…ë ¥ ë°ì´í„°
            config: ì‹¤í–‰ ì„¤ì • (execution_mode: 'sequential' | 'parallel')

        Yields:
            SSE í¬ë§· ì´ë²¤íŠ¸ ë¬¸ìì—´
        """
        execution_id = str(uuid.uuid4())

        # ì‹¤í–‰ ëª¨ë“œ ê²°ì • (ê¸°ë³¸ê°’: sequential)
        execution_mode = (config or {}).get("execution_mode", "sequential")
        self.logger.info(f"[SSE] ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘: {workflow.name} (ID: {execution_id}, ëª¨ë“œ: {execution_mode})")

        # ì·¨ì†Œ í”Œë˜ê·¸ ë“±ë¡
        cancel_event = asyncio.Event()
        self._running_executions[execution_id] = cancel_event

        # ê²°ê³¼ ì €ì¥ ì„¸ì…˜ ì´ˆê¸°í™”
        self.node_execution_order = 0
        session_dir = None
        if self.result_manager:
            try:
                session_dir = self.result_manager.create_session(workflow.name)
                self.logger.info(f"[ê²°ê³¼ì €ì¥] ì„¸ì…˜ ìƒì„±: {session_dir}")
            except Exception as e:
                self.logger.warning(f"[ê²°ê³¼ì €ì¥] ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")

        try:
            # ì´ˆê¸° ìƒíƒœ ì „ì†¡
            yield self._format_sse_event({
                "type": "workflow_start",
                "execution_id": execution_id,
                "workflow_name": workflow.name,
                "timestamp": time.time()
            })

            # 1. DAG ê²€ì¦
            nodes_dict = [node.model_dump() for node in workflow.nodes]
            edges_dict = [edge.model_dump() for edge in workflow.edges]

            validator = DAGValidator(nodes_dict, edges_dict)
            is_valid, errors = validator.validate_all()

            if not is_valid:
                error_msg = "; ".join(errors)
                yield self._format_sse_event({
                    "type": "error",
                    "message": f"ì›Œí¬í”Œë¡œìš° ê²€ì¦ ì‹¤íŒ¨: {error_msg}"
                })
                return

            # 2. ì‹¤í–‰ ì¤€ë¹„
            context = ExecutionContext(execution_id, workflow, inputs)

            # ëª¨ë“  ë…¸ë“œë¥¼ pending ìƒíƒœë¡œ ì´ˆê¸°í™”
            for node in workflow.nodes:
                context.set_node_status(node.id, "pending")

            # 3. ìœ„ìƒ ì •ë ¬
            sorted_nodes = validator.topological_sort()
            parallel_groups = validator.find_parallel_groups()

            yield self._format_sse_event({
                "type": "execution_plan",
                "sorted_nodes": sorted_nodes,
                "parallel_groups": parallel_groups,
                "execution_mode": execution_mode,
                "total_nodes": len(workflow.nodes)
            })

            # 4. ë…¸ë“œ ì‹¤í–‰
            if execution_mode == "sequential":
                # ìˆœì°¨ ì‹¤í–‰: ìœ„ìƒ ì •ë ¬ ìˆœì„œëŒ€ë¡œ í•˜ë‚˜ì”© ì‹¤í–‰
                self.logger.info("ğŸ”„ [SSE] ìˆœì°¨ ì‹¤í–‰ ëª¨ë“œ")
                for node_id in sorted_nodes:
                    # ì·¨ì†Œ í™•ì¸
                    if self._is_cancelled(execution_id):
                        self.logger.info(f"ğŸ›‘ [SSE] ì›Œí¬í”Œë¡œìš° ì·¨ì†Œë¨ (ë…¸ë“œ {node_id} ì‹¤í–‰ ì „)")
                        yield self._format_sse_event({
                            "type": "workflow_cancelled",
                            "execution_id": execution_id,
                            "cancelled_at_node": node_id,
                            "message": "Workflow cancelled by user"
                        })
                        return

                    # ë…¸ë“œ ì‹¤í–‰ ì‹œì‘ ì´ë²¤íŠ¸
                    yield self._format_sse_event({
                        "type": "node_start",
                        "node_id": node_id,
                        "status": "running"
                    })

                    async for event in self._execute_single_node_stream(
                        node_id, workflow, context, session_dir
                    ):
                        yield event
            else:
                # ë³‘ë ¬ ì‹¤í–‰: ë³‘ë ¬ ê·¸ë£¹ ë‹¨ìœ„ë¡œ ì‹¤í–‰
                self.logger.info("âš¡ [SSE] ë³‘ë ¬ ì‹¤í–‰ ëª¨ë“œ")
                for group_idx, node_group in enumerate(parallel_groups):
                    # ì·¨ì†Œ í™•ì¸
                    if self._is_cancelled(execution_id):
                        self.logger.info(f"ğŸ›‘ [SSE] ì›Œí¬í”Œë¡œìš° ì·¨ì†Œë¨ (ê·¸ë£¹ {group_idx + 1} ì‹¤í–‰ ì „)")
                        yield self._format_sse_event({
                            "type": "workflow_cancelled",
                            "execution_id": execution_id,
                            "cancelled_at_group": group_idx + 1,
                            "message": "Workflow cancelled by user"
                        })
                        return

                    if len(node_group) > 1:
                        # ë³‘ë ¬ ì‹¤í–‰
                        tasks = []
                        for node_id in node_group:
                            tasks.append(self._execute_node_with_events(
                                node_id, workflow, context, self._event_callback
                            ))

                        # ë³‘ë ¬ ì‹¤í–‰ ì¤‘ ì´ë²¤íŠ¸ë¥¼ ë°›ê¸° ìœ„í•´ gather ì‚¬ìš©
                        results = await asyncio.gather(*tasks, return_exceptions=True)

                        # ê° ë…¸ë“œ ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡ ë° ê²°ê³¼ ì €ì¥
                        for node_id, result in zip(node_group, results):
                            node_status = context.get_node_status(node_id)
                            if node_status:
                                # ê²°ê³¼ ì €ì¥
                                if self.result_manager and session_dir and node_status.output:
                                    try:
                                        node = next((n for n in workflow.nodes if n.id == node_id), None)
                                        self.node_execution_order += 1
                                        self.result_manager.save_node_result(
                                            node_id=node_id,
                                            node_type=node.type if node else "unknown",
                                            result=node_status.output,
                                            execution_order=self.node_execution_order,
                                            session_dir=session_dir
                                        )
                                    except Exception as save_err:
                                        self.logger.warning(f"[ê²°ê³¼ì €ì¥] ë³‘ë ¬ ë…¸ë“œ ì €ì¥ ì‹¤íŒ¨: {save_err}")

                                yield self._format_sse_event({
                                    "type": "node_update",
                                    "node_id": node_id,
                                    "status": node_status.status,
                                    "progress": node_status.progress,
                                    "error": node_status.error,
                                    "output": node_status.output
                                })
                    else:
                        # ë‹¨ì¼ ì‹¤í–‰
                        node_id = node_group[0]

                        # ë…¸ë“œ ì‹¤í–‰ ì‹œì‘ ì´ë²¤íŠ¸
                        yield self._format_sse_event({
                            "type": "node_start",
                            "node_id": node_id,
                            "status": "running"
                        })

                        async for event in self._execute_single_node_stream(
                            node_id, workflow, context, session_dir
                        ):
                            yield event

            # 5. ì™„ë£Œ
            execution_time = (time.time() - context.start_time) * 1000

            # ìµœì¢… ì¶œë ¥ ê²°ì •
            leaf_nodes = validator.find_leaf_nodes()
            final_output = {}
            for leaf_id in leaf_nodes:
                leaf_output = context.get_node_output(leaf_id)
                if leaf_output:
                    final_output[leaf_id] = leaf_output

            # ì „ì²´ ì‹¤í–‰ ìƒíƒœ ê²°ì •
            has_failed = any(s.status == "failed" for s in context.node_statuses.values())
            overall_status = "failed" if has_failed else "completed"

            # ì›Œí¬í”Œë¡œìš° ë©”íƒ€ë°ì´í„° ì €ì¥
            if self.result_manager and session_dir:
                try:
                    self.result_manager.save_workflow_metadata(
                        workflow_name=workflow.name,
                        nodes=[n.model_dump() for n in workflow.nodes],
                        execution_time=execution_time / 1000,  # ms -> seconds
                        status=overall_status,
                        session_dir=session_dir
                    )
                    self.logger.info(f"[ê²°ê³¼ì €ì¥] ì›Œí¬í”Œë¡œìš° ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {session_dir}")
                except Exception as meta_err:
                    self.logger.warning(f"[ê²°ê³¼ì €ì¥] ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {meta_err}")

            # ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡
            yield self._format_sse_event({
                "type": "workflow_complete",
                "status": overall_status,
                "execution_time_ms": execution_time,
                "result_save_path": str(session_dir) if session_dir else None,  # ì €ì¥ ê²½ë¡œ ì¶”ê°€
                "node_statuses": [
                    {
                        "node_id": ns.node_id,
                        "status": ns.status,
                        "progress": ns.progress,
                        "error": ns.error,
                        "output": ns.output  # âœ… output ì¶”ê°€ (ì´ë¯¸ì§€ í¬í•¨)
                    }
                    for ns in context.node_statuses.values()
                ],
                "final_output": final_output if final_output else None
            })

        except Exception as e:
            self.logger.error(f"[SSE] ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜ˆì™¸: {e}", exc_info=True)
            yield self._format_sse_event({
                "type": "error",
                "message": f"ì‹¤í–‰ ì—”ì§„ ì—ëŸ¬: {str(e)}"
            })
        finally:
            # ì‹¤í–‰ ì •ë³´ ì •ë¦¬
            if execution_id in self._running_executions:
                del self._running_executions[execution_id]
                self.logger.info(f"[SSE] ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì •ë³´ ì •ë¦¬: {execution_id}")

    async def _execute_node_with_events(
        self,
        node_id: str,
        workflow: WorkflowDefinition,
        context: ExecutionContext,
        event_callback: Optional[callable] = None
    ):
        """ì´ë²¤íŠ¸ ì½œë°±ì„ ì§€ì›í•˜ëŠ” ë…¸ë“œ ì‹¤í–‰"""
        await self._execute_node(node_id, workflow, context)

    async def _execute_single_node_stream(
        self,
        node_id: str,
        workflow: WorkflowDefinition,
        context: ExecutionContext,
        session_dir: Optional[Any] = None
    ) -> AsyncGenerator[str, None]:
        """ë‹¨ì¼ ë…¸ë“œ ì‹¤í–‰ ë° SSE ì´ë²¤íŠ¸ ìƒì„± (ìˆœì°¨/ë³‘ë ¬ ëª¨ë“œ ê³µìš©)"""
        try:
            # í•˜íŠ¸ë¹„íŠ¸ì™€ í•¨ê»˜ ë…¸ë“œ ì‹¤í–‰
            start_time = time.time()
            heartbeat_interval = 5  # 5ì´ˆë§ˆë‹¤ í•˜íŠ¸ë¹„íŠ¸

            # ë…¸ë“œ ì‹¤í–‰ì„ Taskë¡œ ì‹œì‘
            execute_task = asyncio.create_task(
                self._execute_node(node_id, workflow, context)
            )

            # ì‹¤í–‰ ì™„ë£Œê¹Œì§€ í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡
            while not execute_task.done():
                try:
                    # 5ì´ˆ ëŒ€ê¸° ë˜ëŠ” ì‹¤í–‰ ì™„ë£Œ
                    await asyncio.wait_for(
                        asyncio.shield(execute_task),
                        timeout=heartbeat_interval
                    )
                except asyncio.TimeoutError:
                    # íƒ€ì„ì•„ì›ƒ = ì•„ì§ ì‹¤í–‰ ì¤‘ â†’ í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡
                    elapsed = int(time.time() - start_time)
                    yield self._format_sse_event({
                        "type": "node_heartbeat",
                        "node_id": node_id,
                        "status": "running",
                        "elapsed_seconds": elapsed,
                        "message": f"ì²˜ë¦¬ ì¤‘... ({elapsed}ì´ˆ ê²½ê³¼)"
                    })

            # Task ì™„ë£Œ í›„ ì˜ˆì™¸ í™•ì¸
            await execute_task

            node_status = context.get_node_status(node_id)

            # ë””ë²„ê·¸: output í™•ì¸
            output_data = node_status.output if node_status else None
            if output_data:
                output_keys = list(output_data.keys()) if isinstance(output_data, dict) else []
                has_image = 'image' in output_keys or 'visualized_image' in output_keys
                image_size = len(output_data.get('image', '') or output_data.get('visualized_image', '') or '') if has_image else 0
                self.logger.info(f"[SSE] Node {node_id} output keys: {output_keys}, has_image: {has_image}, image_size: {image_size}")
            else:
                self.logger.warning(f"[SSE] Node {node_id} has NO output!")

            # ê²°ê³¼ ì €ì¥
            if self.result_manager and session_dir and output_data:
                try:
                    node = next((n for n in workflow.nodes if n.id == node_id), None)
                    self.node_execution_order += 1
                    saved = self.result_manager.save_node_result(
                        node_id=node_id,
                        node_type=node.type if node else "unknown",
                        result=output_data,
                        execution_order=self.node_execution_order,
                        session_dir=session_dir
                    )
                    if saved:
                        self.logger.info(f"[ê²°ê³¼ì €ì¥] {node_id}: {list(saved.keys())}")
                except Exception as save_err:
                    self.logger.warning(f"[ê²°ê³¼ì €ì¥] ë…¸ë“œ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {save_err}")

            # ë…¸ë“œ ì™„ë£Œ ì´ë²¤íŠ¸
            yield self._format_sse_event({
                "type": "node_complete",
                "node_id": node_id,
                "status": node_status.status if node_status else "completed",
                "progress": node_status.progress if node_status else 1.0,
                "output": output_data
            })
        except Exception as e:
            # ë…¸ë“œ ì‹¤íŒ¨ ì´ë²¤íŠ¸
            yield self._format_sse_event({
                "type": "node_error",
                "node_id": node_id,
                "status": "failed",
                "error": str(e)
            })

    def _event_callback(self, event: Dict[str, Any]):
        """ë…¸ë“œ ì‹¤í–‰ ì¤‘ ì´ë²¤íŠ¸ ì½œë°± (ì¶”í›„ í™•ì¥ ê°€ëŠ¥)"""
        pass

    def _format_sse_event(self, data: Dict[str, Any]) -> str:
        """SSE ì´ë²¤íŠ¸ í¬ë§· ìƒì„± (orjsonìœ¼ë¡œ ìµœì í™”)"""
        # orjsonì€ bytes ë°˜í™˜, decode í•„ìš”
        json_str = orjson.dumps(data, option=orjson.OPT_NON_STR_KEYS).decode('utf-8')
        return f"data: {json_str}\n\n"
