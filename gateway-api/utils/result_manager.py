"""
Result Manager
íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ì €ì¥ ë° ì •ë¦¬ ê´€ë¦¬

ì €ì¥ êµ¬ì¡°:
/tmp/gateway/results/
  â””â”€â”€ 2025-12-08/
      â””â”€â”€ 14-30-25_workflow-name/
          â”œâ”€â”€ metadata.json          # ì›Œí¬í”Œë¡œìš° ë©”íƒ€ì •ë³´
          â”œâ”€â”€ node_1_imageinput.json  # ë…¸ë“œë³„ JSON ê²°ê³¼
          â”œâ”€â”€ node_1_imageinput.jpg   # ë…¸ë“œë³„ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€
          â”œâ”€â”€ node_2_edocr2.json
          â”œâ”€â”€ node_2_edocr2.jpg
          â””â”€â”€ ...

ìë™ ì •ë¦¬:
- Gateway API ì‹œì‘ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
- ë§¤ì¼ ìƒˆë²½ 2ì‹œì— ì˜¤ë˜ëœ ê²°ê³¼ ìë™ ì‚­ì œ
- ê¸°ë³¸ ë³´ê´€ ê¸°ê°„: 7ì¼
"""
import os
import json
import base64
import shutil
import logging
import asyncio
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List

logger = logging.getLogger(__name__)

# ê¸°ë³¸ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
DEFAULT_RESULT_DIR = os.getenv("RESULT_SAVE_DIR", "/tmp/gateway/results")

# ê¸°ë³¸ ë³´ê´€ ê¸°ê°„ (ì¼)
DEFAULT_RETENTION_DAYS = int(os.getenv("RESULT_RETENTION_DAYS", "7"))


class ResultManager:
    """íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì €ì¥ ë° ì •ë¦¬ ê´€ë¦¬ì"""

    def __init__(self, base_dir: str = DEFAULT_RESULT_DIR):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.current_session_dir: Optional[Path] = None

    def create_session(self, workflow_name: str = "workflow") -> Path:
        """
        ìƒˆ ì €ì¥ ì„¸ì…˜ ìƒì„±

        Args:
            workflow_name: ì›Œí¬í”Œë¡œìš° ì´ë¦„

        Returns:
            ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        now = datetime.now()
        date_str = now.strftime("%Y-%m-%d")
        time_str = now.strftime("%H-%M-%S")

        # ì›Œí¬í”Œë¡œìš° ì´ë¦„ ì •ë¦¬ (íŒŒì¼ëª…ì— ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡)
        safe_name = "".join(c if c.isalnum() or c in "-_" else "_" for c in workflow_name)
        safe_name = safe_name[:50]  # ê¸¸ì´ ì œí•œ

        session_name = f"{time_str}_{safe_name}"
        session_dir = self.base_dir / date_str / session_name
        session_dir.mkdir(parents=True, exist_ok=True)

        self.current_session_dir = session_dir
        logger.info(f"ê²°ê³¼ ì €ì¥ ì„¸ì…˜ ìƒì„±: {session_dir}")

        return session_dir

    def save_node_result(
        self,
        node_id: str,
        node_type: str,
        result: Dict[str, Any],
        execution_order: int = 0,
        session_dir: Optional[Path] = None
    ) -> Dict[str, str]:
        """
        ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ ì €ì¥

        Args:
            node_id: ë…¸ë“œ ID
            node_type: ë…¸ë“œ íƒ€ì…
            result: ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            execution_order: ì‹¤í–‰ ìˆœì„œ
            session_dir: ì €ì¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ current_session_dir ì‚¬ìš©)

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
        """
        save_dir = session_dir or self.current_session_dir
        if not save_dir:
            logger.warning("ì €ì¥ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. create_session()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
            return {}

        saved_files = {}
        prefix = f"node_{execution_order:02d}_{node_type}"

        # 1. JSON ê²°ê³¼ ì €ì¥ (ì´ë¯¸ì§€ ë°ì´í„° ì œì™¸)
        json_path = save_dir / f"{prefix}.json"
        json_result = self._prepare_json_result(result, node_id, node_type)
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_result, f, ensure_ascii=False, indent=2, default=str)
        saved_files['json'] = str(json_path)
        logger.debug(f"JSON ì €ì¥: {json_path}")

        # 2. ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ì €ì¥
        image_data = self._extract_image_data(result)
        if image_data:
            image_path = save_dir / f"{prefix}.jpg"
            self._save_base64_image(image_data, image_path)
            saved_files['image'] = str(image_path)
            logger.debug(f"ì´ë¯¸ì§€ ì €ì¥: {image_path}")

        # 3. ì›ë³¸ ì´ë¯¸ì§€ë„ ë³„ë„ ì €ì¥ (ìˆëŠ” ê²½ìš°)
        original_image = result.get('original_image')
        if original_image and original_image != image_data:
            original_path = save_dir / f"{prefix}_original.jpg"
            self._save_base64_image(original_image, original_path)
            saved_files['original'] = str(original_path)

        return saved_files

    def save_workflow_metadata(
        self,
        workflow_name: str,
        nodes: List[Dict[str, Any]],
        execution_time: float,
        status: str = "completed",
        session_dir: Optional[Path] = None
    ):
        """
        ì›Œí¬í”Œë¡œìš° ë©”íƒ€ë°ì´í„° ì €ì¥

        Args:
            workflow_name: ì›Œí¬í”Œë¡œìš° ì´ë¦„
            nodes: ë…¸ë“œ ëª©ë¡
            execution_time: ì´ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
            status: ì‹¤í–‰ ìƒíƒœ
            session_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        """
        save_dir = session_dir or self.current_session_dir
        if not save_dir:
            return

        metadata = {
            "workflow_name": workflow_name,
            "executed_at": datetime.now().isoformat(),
            "execution_time_seconds": execution_time,
            "status": status,
            "total_nodes": len(nodes),
            "nodes": [
                {
                    "id": n.get("id"),
                    "type": n.get("type"),
                    "label": n.get("label", n.get("type")),
                }
                for n in nodes
            ]
        }

        metadata_path = save_dir / "metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        logger.info(f"ì›Œí¬í”Œë¡œìš° ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")

    def _prepare_json_result(
        self,
        result: Dict[str, Any],
        node_id: str,
        node_type: str
    ) -> Dict[str, Any]:
        """JSON ì €ì¥ìš© ê²°ê³¼ ì¤€ë¹„ (ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ ë°ì´í„° ì œì™¸)"""
        json_result = {
            "node_id": node_id,
            "node_type": node_type,
            "saved_at": datetime.now().isoformat(),
        }

        # ì´ë¯¸ì§€ í•„ë“œ ì œì™¸í•˜ê³  ë³µì‚¬
        image_fields = {'image', 'visualized_image', 'original_image', 'overlay_image'}

        for key, value in result.items():
            if key in image_fields:
                # ì´ë¯¸ì§€ í•„ë“œëŠ” ì¡´ì¬ ì—¬ë¶€ë§Œ ê¸°ë¡
                json_result[f"{key}_saved"] = bool(value)
            elif key == 'raw_response':
                # raw_responseì—ì„œë„ ì´ë¯¸ì§€ ì œì™¸
                if isinstance(value, dict):
                    cleaned = {k: v for k, v in value.items() if k not in image_fields}
                    json_result[key] = cleaned
            else:
                json_result[key] = value

        return json_result

    def _extract_image_data(self, result: Dict[str, Any]) -> Optional[str]:
        """ê²°ê³¼ì—ì„œ ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ"""
        # ìš°ì„ ìˆœìœ„: visualized_image > image > overlay_image
        for field in ['visualized_image', 'image', 'overlay_image']:
            image_data = result.get(field)
            if image_data and isinstance(image_data, str) and len(image_data) > 100:
                return image_data
        return None

    def _save_base64_image(self, base64_data: str, file_path: Path):
        """Base64 ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # Data URL prefix ì œê±°
            if base64_data.startswith('data:'):
                if ',' in base64_data:
                    base64_data = base64_data.split(',', 1)[1]

            image_bytes = base64.b64decode(base64_data)
            with open(file_path, 'wb') as f:
                f.write(image_bytes)
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")

    def cleanup_old_results(
        self,
        max_age_days: int = DEFAULT_RETENTION_DAYS,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        ì˜¤ë˜ëœ ê²°ê³¼ ì •ë¦¬

        Args:
            max_age_days: ë³´ê´€ ê¸°ê°„ (ì¼)
            dry_run: Trueë©´ ì‹¤ì œ ì‚­ì œ ì—†ì´ ëª©ë¡ë§Œ ë°˜í™˜

        Returns:
            ì •ë¦¬ ê²°ê³¼ í†µê³„
        """
        cutoff_date = datetime.now() - timedelta(days=max_age_days)
        deleted_dirs = []
        total_size = 0

        for date_dir in self.base_dir.iterdir():
            if not date_dir.is_dir():
                continue

            try:
                dir_date = datetime.strptime(date_dir.name, "%Y-%m-%d")
                if dir_date < cutoff_date:
                    # ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°
                    dir_size = sum(f.stat().st_size for f in date_dir.rglob('*') if f.is_file())
                    total_size += dir_size

                    if not dry_run:
                        shutil.rmtree(date_dir)
                        logger.info(f"ì‚­ì œë¨: {date_dir} ({dir_size / 1024:.1f} KB)")

                    deleted_dirs.append({
                        "path": str(date_dir),
                        "date": date_dir.name,
                        "size_bytes": dir_size
                    })
            except ValueError:
                # ë‚ ì§œ í˜•ì‹ì´ ì•„ë‹Œ ë””ë ‰í† ë¦¬ëŠ” ë¬´ì‹œ
                continue

        result = {
            "deleted_count": len(deleted_dirs),
            "total_size_bytes": total_size,
            "total_size_mb": total_size / (1024 * 1024),
            "max_age_days": max_age_days,
            "dry_run": dry_run,
            "deleted_directories": deleted_dirs
        }

        logger.info(
            f"ê²°ê³¼ ì •ë¦¬ ì™„ë£Œ: {len(deleted_dirs)}ê°œ ë””ë ‰í† ë¦¬ "
            f"({total_size / (1024 * 1024):.2f} MB) "
            f"{'(dry run)' if dry_run else 'ì‚­ì œë¨'}"
        )

        return result

    def get_statistics(self) -> Dict[str, Any]:
        """ê²°ê³¼ ì €ì¥ì†Œ í†µê³„"""
        total_size = 0
        total_files = 0
        date_counts = {}

        for date_dir in self.base_dir.iterdir():
            if not date_dir.is_dir():
                continue

            try:
                datetime.strptime(date_dir.name, "%Y-%m-%d")
                session_count = sum(1 for d in date_dir.iterdir() if d.is_dir())
                file_count = sum(1 for f in date_dir.rglob('*') if f.is_file())
                dir_size = sum(f.stat().st_size for f in date_dir.rglob('*') if f.is_file())

                date_counts[date_dir.name] = {
                    "sessions": session_count,
                    "files": file_count,
                    "size_bytes": dir_size
                }
                total_size += dir_size
                total_files += file_count
            except ValueError:
                continue

        return {
            "base_dir": str(self.base_dir),
            "total_size_bytes": total_size,
            "total_size_mb": total_size / (1024 * 1024),
            "total_files": total_files,
            "dates": date_counts,
            "retention_days": DEFAULT_RETENTION_DAYS
        }

    def list_recent_sessions(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ìµœê·¼ ì„¸ì…˜ ëª©ë¡"""
        sessions = []

        for date_dir in sorted(self.base_dir.iterdir(), reverse=True):
            if not date_dir.is_dir():
                continue

            try:
                datetime.strptime(date_dir.name, "%Y-%m-%d")
                for session_dir in sorted(date_dir.iterdir(), reverse=True):
                    if not session_dir.is_dir():
                        continue

                    metadata_path = session_dir / "metadata.json"
                    metadata = {}
                    if metadata_path.exists():
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)

                    sessions.append({
                        "path": str(session_dir),
                        "date": date_dir.name,
                        "session": session_dir.name,
                        "workflow_name": metadata.get("workflow_name", "unknown"),
                        "executed_at": metadata.get("executed_at"),
                        "status": metadata.get("status", "unknown"),
                        "total_nodes": metadata.get("total_nodes", 0)
                    })

                    if len(sessions) >= limit:
                        return sessions
            except ValueError:
                continue

        return sessions


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_result_manager: Optional[ResultManager] = None


def get_result_manager() -> ResultManager:
    """ResultManager ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _result_manager
    if _result_manager is None:
        _result_manager = ResultManager()
    return _result_manager


def cleanup_results(max_age_days: int = DEFAULT_RETENTION_DAYS, dry_run: bool = False):
    """
    ê²°ê³¼ ì •ë¦¬ í—¬í¼ í•¨ìˆ˜

    ì‚¬ìš© ì˜ˆ:
        from utils.result_manager import cleanup_results
        cleanup_results(max_age_days=7)
    """
    manager = get_result_manager()
    return manager.cleanup_old_results(max_age_days=max_age_days, dry_run=dry_run)


# =============================================================================
# ìë™ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬
# =============================================================================

# ì •ë¦¬ ìŠ¤ì¼€ì¤„ ì„¤ì •
CLEANUP_INTERVAL_HOURS = int(os.getenv("RESULT_CLEANUP_INTERVAL_HOURS", "24"))  # ê¸°ë³¸ 24ì‹œê°„
CLEANUP_HOUR = int(os.getenv("RESULT_CLEANUP_HOUR", "2"))  # ê¸°ë³¸ ìƒˆë²½ 2ì‹œ

_cleanup_task: Optional[asyncio.Task] = None


async def _scheduled_cleanup_loop():
    """
    ìŠ¤ì¼€ì¤„ëœ ì •ë¦¬ ë£¨í”„

    ë§¤ì¼ ì§€ì •ëœ ì‹œê°„(ê¸°ë³¸ ìƒˆë²½ 2ì‹œ)ì— ì˜¤ë˜ëœ ê²°ê³¼ë¥¼ ìë™ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    logger.info(f"ğŸ—‘ï¸ ê²°ê³¼ ìë™ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ë§¤ì¼ {CLEANUP_HOUR}ì‹œ, ë³´ê´€ê¸°ê°„: {DEFAULT_RETENTION_DAYS}ì¼)")

    while True:
        try:
            # ë‹¤ìŒ ì •ë¦¬ ì‹œê°„ ê³„ì‚°
            now = datetime.now()
            next_cleanup = now.replace(hour=CLEANUP_HOUR, minute=0, second=0, microsecond=0)

            # ì´ë¯¸ ì§€ë‚œ ì‹œê°„ì´ë©´ ë‹¤ìŒ ë‚ ë¡œ
            if next_cleanup <= now:
                next_cleanup += timedelta(days=1)

            # ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
            wait_seconds = (next_cleanup - now).total_seconds()
            logger.info(f"ğŸ—‘ï¸ ë‹¤ìŒ ì •ë¦¬ ì˜ˆì •: {next_cleanup.strftime('%Y-%m-%d %H:%M')} ({wait_seconds/3600:.1f}ì‹œê°„ í›„)")

            # ëŒ€ê¸°
            await asyncio.sleep(wait_seconds)

            # ì •ë¦¬ ì‹¤í–‰
            logger.info("ğŸ—‘ï¸ ìë™ ì •ë¦¬ ì‹œì‘...")
            manager = get_result_manager()
            result = manager.cleanup_old_results(max_age_days=DEFAULT_RETENTION_DAYS)

            if result["deleted_count"] > 0:
                logger.info(
                    f"ğŸ—‘ï¸ ìë™ ì •ë¦¬ ì™„ë£Œ: {result['deleted_count']}ê°œ ë””ë ‰í† ë¦¬ ì‚­ì œ "
                    f"({result['total_size_mb']:.2f} MB í™•ë³´)"
                )
            else:
                logger.info("ğŸ—‘ï¸ ìë™ ì •ë¦¬ ì™„ë£Œ: ì‚­ì œí•  í•­ëª© ì—†ìŒ")

        except asyncio.CancelledError:
            logger.info("ğŸ—‘ï¸ ê²°ê³¼ ìë™ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ë¨")
            break
        except Exception as e:
            logger.error(f"ğŸ—‘ï¸ ìë™ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ 1ì‹œê°„ í›„ ì¬ì‹œë„
            await asyncio.sleep(3600)


def start_cleanup_scheduler():
    """
    ìë™ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘

    Gateway API startupì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    global _cleanup_task

    if _cleanup_task is not None and not _cleanup_task.done():
        logger.warning("ğŸ—‘ï¸ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
        return

    _cleanup_task = asyncio.create_task(_scheduled_cleanup_loop())
    logger.info("ğŸ—‘ï¸ ê²°ê³¼ ìë™ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡ë¨")


def stop_cleanup_scheduler():
    """
    ìë™ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€

    Gateway API shutdownì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    global _cleanup_task

    if _cleanup_task is not None and not _cleanup_task.done():
        _cleanup_task.cancel()
        logger.info("ğŸ—‘ï¸ ê²°ê³¼ ìë™ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€ ìš”ì²­ë¨")
