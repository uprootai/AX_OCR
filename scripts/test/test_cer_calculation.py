#!/usr/bin/env python3
"""
CER (Character Error Rate) ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸

Multimodal LLMìœ¼ë¡œì„œ ë„ë©´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³ ,
OCR ê²°ê³¼ì™€ ë¹„êµí•˜ì—¬ ì‹¤ì œ ì •í™•ë„ë¥¼ ì¸¡ì •
"""

import json
from typing import List, Dict, Tuple
from pathlib import Path
import re

# Ground Truth: ë„ë©´ì—ì„œ ì‹¤ì œë¡œ ë³´ì´ëŠ” ì¹˜ìˆ˜ ë° í…ìŠ¤íŠ¸
GROUND_TRUTH_SAMPLE1 = {
    'filename': 'S60ME-C INTERM-SHAFT_ëŒ€ ì£¼ì¡°ì „.jpg',
    'dimensions': [
        {'text': 'Ï†476', 'type': 'diameter', 'value': 476.0},
        {'text': 'Ï†370', 'type': 'diameter', 'value': 370.0},
        {'text': 'Ï†9.204 +0.1 -0.2', 'type': 'diameter', 'value': 9.204, 'tolerance': '+0.1/-0.2'},
        {'text': 'Ï†1313Â±2', 'type': 'diameter', 'value': 1313.0, 'tolerance': 'Â±2'},
        {'text': '(177)', 'type': 'reference', 'value': 177.0},
        {'text': '7Â±0.5', 'type': 'linear', 'value': 7.0, 'tolerance': 'Â±0.5'},
        {'text': '5mm', 'type': 'linear', 'value': 5.0},
        {'text': '1.5', 'type': 'linear', 'value': 1.5},
        {'text': '5', 'type': 'linear', 'value': 5.0},
        # ì •ë©´ë„ (í•˜ë‹¨ ì›í˜• ë·°)ì— ë” ë§ì€ ì¹˜ìˆ˜ê°€ ìˆì§€ë§Œ ì‘ì•„ì„œ ì½ê¸° ì–´ë ¤ì›€
    ],
    'gdt': [
        {'text': 'Ra 2', 'type': 'surface_roughness', 'value': 'Ra 2'},
        {'text': 'Ra 3', 'type': 'surface_roughness', 'value': 'Ra 3'},
        {'text': 'âŒ–', 'type': 'position_tolerance', 'symbol': 'âŒ–'},
        {'text': 'â–³', 'type': 'triangular_note', 'symbol': 'â–³'},
    ],
    'text': [
        'DWG-',
        'Ref.',
        'Rev.1',
        'Rev.2',
        'Rev.3',
        'Rev.9',
        'DSE BEARING Co., LTD.',
        # í…Œì´ë¸” ë‚´ìš©ë“¤
    ]
}


def normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™”: ê³µë°± ì œê±°, ëŒ€ì†Œë¬¸ì í†µì¼"""
    return re.sub(r'\s+', '', text.lower())


def calculate_cer(reference: str, hypothesis: str) -> float:
    """
    CER (Character Error Rate) ê³„ì‚°

    CER = (S + D + I) / N
    S = Substitutions (ëŒ€ì²´)
    D = Deletions (ì‚­ì œ)
    I = Insertions (ì‚½ì…)
    N = ì°¸ì¡° í…ìŠ¤íŠ¸ ê¸¸ì´
    """
    ref = normalize_text(reference)
    hyp = normalize_text(hypothesis)

    # Levenshtein Distance ê³„ì‚°
    n, m = len(ref), len(hyp)
    dp = [[0] * (m + 1) for _ in range(n + 1)]

    for i in range(n + 1):
        dp[i][0] = i
    for j in range(m + 1):
        dp[0][j] = j

    for i in range(1, n + 1):
        for j in range(1, m + 1):
            if ref[i-1] == hyp[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = min(
                    dp[i-1][j] + 1,    # Deletion
                    dp[i][j-1] + 1,    # Insertion
                    dp[i-1][j-1] + 1   # Substitution
                )

    edit_distance = dp[n][m]
    cer = edit_distance / n if n > 0 else 0.0

    return cer, edit_distance, n


def match_dimensions(ground_truth: List[Dict], ocr_results: List[Dict]) -> Dict:
    """
    Ground Truthì™€ OCR ê²°ê³¼ë¥¼ ë§¤ì¹­í•˜ì—¬ ì •í™•ë„ ê³„ì‚°
    """
    matched = []
    unmatched_gt = []
    false_positives = []

    gt_used = [False] * len(ground_truth)
    ocr_used = [False] * len(ocr_results)

    # ê°’ ê¸°ë°˜ ë§¤ì¹­
    for i, gt in enumerate(ground_truth):
        best_match = None
        best_score = float('inf')

        for j, ocr in enumerate(ocr_results):
            if ocr_used[j]:
                continue

            # ê°’ ì°¨ì´ ê³„ì‚°
            value_diff = abs(gt['value'] - ocr['value'])

            if value_diff < best_score and value_diff < 10:  # 10 ì´ë‚´ ì˜¤ì°¨ í—ˆìš©
                best_match = j
                best_score = value_diff

        if best_match is not None:
            gt_used[i] = True
            ocr_used[best_match] = True

            gt_text = gt.get('text', str(gt['value']))
            ocr_value = ocr_results[best_match]['value']
            ocr_text = f"Ï†{ocr_value}" if ocr_results[best_match]['type'] == 'diameter' else str(ocr_value)

            cer, dist, length = calculate_cer(gt_text, ocr_text)

            matched.append({
                'ground_truth': gt_text,
                'ocr_result': ocr_text,
                'value_diff': best_score,
                'cer': cer,
                'edit_distance': dist,
                'correct': cer < 0.2  # 20% ì´í•˜ ì˜¤ì°¨ëŠ” ì •í™•í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
            })

    # ë§¤ì¹­ ì•ˆëœ GT (ëˆ„ë½)
    for i, used in enumerate(gt_used):
        if not used:
            unmatched_gt.append(ground_truth[i])

    # ë§¤ì¹­ ì•ˆëœ OCR (ì˜¤ê²€ì¶œ)
    for j, used in enumerate(ocr_used):
        if not used:
            false_positives.append(ocr_results[j])

    return {
        'matched': matched,
        'unmatched_gt': unmatched_gt,
        'false_positives': false_positives,
        'recall': len([m for m in matched if m['correct']]) / len(ground_truth) if ground_truth else 0,
        'precision': len([m for m in matched if m['correct']]) / len(ocr_results) if ocr_results else 0,
        'avg_cer': sum(m['cer'] for m in matched) / len(matched) if matched else 1.0
    }


def evaluate_ocr_results(json_file: str):
    """OCR ê²°ê³¼ JSON íŒŒì¼ì„ ì½ê³  CER ê³„ì‚°"""

    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    print("="*80)
    print("OCR ì„±ëŠ¥ í‰ê°€: CER (Character Error Rate) ê¸°ë°˜")
    print("="*80)
    print()

    # ì²« ë²ˆì§¸ ìƒ˜í”Œ í‰ê°€
    sample = data[0]
    print(f"ìƒ˜í”Œ: {sample['sample']}")
    print(f"Ground Truth: {len(GROUND_TRUTH_SAMPLE1['dimensions'])}ê°œ ì¹˜ìˆ˜")
    print("-"*80)
    print()

    results_summary = []

    for test in sample['tests']:
        if not test['success']:
            continue

        method = test['method']
        ocr_dims = test.get('dimensions', [])

        if not ocr_dims:
            continue

        print(f"\n{'='*80}")
        print(f"ë°©ë²•: {method}")
        print(f"{'='*80}")

        # ì¹˜ìˆ˜ ë§¤ì¹­ ë° í‰ê°€
        match_result = match_dimensions(GROUND_TRUTH_SAMPLE1['dimensions'], ocr_dims)

        print(f"\nğŸ“Š í†µê³„:")
        print(f"  - Ground Truth: {len(GROUND_TRUTH_SAMPLE1['dimensions'])}ê°œ")
        print(f"  - OCR ì¸ì‹: {len(ocr_dims)}ê°œ")
        print(f"  - ì •í™• ë§¤ì¹­: {len([m for m in match_result['matched'] if m['correct']])}ê°œ")
        print(f"  - ì˜¤ì°¨ ìˆëŠ” ë§¤ì¹­: {len([m for m in match_result['matched'] if not m['correct']])}ê°œ")
        print(f"  - ëˆ„ë½ (False Negative): {len(match_result['unmatched_gt'])}ê°œ")
        print(f"  - ì˜¤ê²€ì¶œ (False Positive): {len(match_result['false_positives'])}ê°œ")

        print(f"\nğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:")
        print(f"  - Recall (ì¬í˜„ìœ¨): {match_result['recall']*100:.1f}%")
        print(f"  - Precision (ì •ë°€ë„): {match_result['precision']*100:.1f}%")
        print(f"  - í‰ê·  CER: {match_result['avg_cer']*100:.1f}%")
        print(f"  - F1 Score: {2*match_result['recall']*match_result['precision']/(match_result['recall']+match_result['precision'])*100:.1f}%" if (match_result['recall']+match_result['precision']) > 0 else "  - F1 Score: 0.0%")

        # ìƒì„¸ ë§¤ì¹­ ê²°ê³¼
        print(f"\nğŸ“ ìƒì„¸ ë§¤ì¹­ ê²°ê³¼:")
        for i, match in enumerate(match_result['matched'][:10], 1):  # ì²˜ìŒ 10ê°œë§Œ
            status = "âœ…" if match['correct'] else "âš ï¸"
            print(f"  {status} {i}. GT: '{match['ground_truth']}' â†’ OCR: '{match['ocr_result']}' "
                  f"(CER: {match['cer']*100:.1f}%, ê°’ ì°¨ì´: {match['value_diff']:.2f})")

        if len(match_result['matched']) > 10:
            print(f"  ... ì™¸ {len(match_result['matched'])-10}ê°œ")

        # ëˆ„ë½ëœ ê²ƒë“¤
        if match_result['unmatched_gt']:
            print(f"\nâŒ ëˆ„ë½ëœ ì¹˜ìˆ˜ ({len(match_result['unmatched_gt'])}ê°œ):")
            for miss in match_result['unmatched_gt'][:5]:
                print(f"  - {miss.get('text', miss['value'])}")

        # ìš”ì•½ ì €ì¥
        results_summary.append({
            'method': method,
            'recall': match_result['recall'],
            'precision': match_result['precision'],
            'avg_cer': match_result['avg_cer'],
            'f1': 2*match_result['recall']*match_result['precision']/(match_result['recall']+match_result['precision']) if (match_result['recall']+match_result['precision']) > 0 else 0
        })

    # ìµœì¢… ë¹„êµ
    print(f"\n\n{'='*80}")
    print("ìµœì¢… ë¹„êµ (CER ê¸°ë°˜)")
    print(f"{'='*80}\n")

    print(f"{'ë°©ë²•':<30} | {'Recall':<10} | {'Precision':<10} | {'Avg CER':<10} | {'F1 Score':<10}")
    print("-"*80)

    for result in sorted(results_summary, key=lambda x: x['f1'], reverse=True):
        print(f"{result['method']:<30} | "
              f"{result['recall']*100:>8.1f}% | "
              f"{result['precision']*100:>8.1f}% | "
              f"{result['avg_cer']*100:>8.1f}% | "
              f"{result['f1']*100:>8.1f}%")

    # ìµœì  ëª¨ë¸ ì„ ì •
    if results_summary:
        best = max(results_summary, key=lambda x: x['f1'])
        print(f"\nğŸ† ìµœì  ëª¨ë¸ (F1 ê¸°ì¤€): {best['method']}")
        print(f"   - F1 Score: {best['f1']*100:.1f}%")
        print(f"   - Recall: {best['recall']*100:.1f}%")
        print(f"   - Precision: {best['precision']*100:.1f}%")
        print(f"   - Avg CER: {best['avg_cer']*100:.1f}%")


if __name__ == '__main__':
    json_file = 'ocr_performance_comparison_20251031_195252.json'
    evaluate_ocr_results(json_file)
