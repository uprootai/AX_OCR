"""
Vision Language Model API Server
Multimodal LLM ê¸°ë°˜ ë„ë©´ ë¶„ì„ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤

í¬íŠ¸: 5004
ê¸°ëŠ¥: Information Block ì¶”ì¶œ, ì¹˜ìˆ˜ ì¶”ì¶œ, ì œì¡° ê³µì • ì¶”ë¡ , QC Checklist ìƒì„±
"""

import os
import sys
import json
import time
import base64
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List, Union
from datetime import datetime
import io

from fastapi import FastAPI, File, UploadFile, Form, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
from PIL import Image
import httpx
import torch
from transformers import AutoProcessor, AutoModelForCausalLM, Blip2Processor, Blip2ForConditionalGeneration

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize FastAPI
app = FastAPI(
    title="Vision Language Model API",
    description="Multimodal LLM Service for Engineering Drawing Analysis",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

UPLOAD_DIR = Path("/tmp/vl-api/uploads")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# API í‚¤ ìƒíƒœ (startup ì‹œ ê²€ì¦ë¨)
_api_keys_validated = False
_available_models = []

# Florence-2 ë¡œì»¬ ëª¨ë¸ (API í‚¤ ì—†ì„ ë•Œ í´ë°±)
_florence_model = None
_florence_processor = None
_florence_device = None
FLORENCE_MODEL_ID = "microsoft/Florence-2-base"


# =====================
# Pydantic Models
# =====================

class HealthResponse(BaseModel):
    status: str
    service: str
    version: str
    timestamp: str
    available_models: List[str]


class InfoBlockRequest(BaseModel):
    query_fields: List[str] = Field(
        default=["name", "part number", "material", "scale", "weight"],
        description="ì¶”ì¶œí•  ì •ë³´ í•„ë“œ ëª©ë¡"
    )
    model: str = Field(default="claude-3-5-sonnet-20241022", description="ì‚¬ìš©í•  VL ëª¨ë¸")


class InfoBlockResponse(BaseModel):
    status: str
    data: Dict[str, str]
    processing_time: float
    model_used: str


class DimensionExtractionRequest(BaseModel):
    model: str = Field(default="claude-3-5-sonnet-20241022", description="ì‚¬ìš©í•  VL ëª¨ë¸")


class DimensionExtractionResponse(BaseModel):
    status: str
    data: List[str]
    processing_time: float
    model_used: str


class ManufacturingProcessRequest(BaseModel):
    model: str = Field(default="gpt-4o", description="ì‚¬ìš©í•  VL ëª¨ë¸")


class ManufacturingProcessResponse(BaseModel):
    status: str
    data: Dict[str, str]
    processing_time: float
    model_used: str


class QCChecklistRequest(BaseModel):
    model: str = Field(default="gpt-4o", description="ì‚¬ìš©í•  VL ëª¨ë¸")


class QCChecklistResponse(BaseModel):
    status: str
    data: List[str]
    processing_time: float
    model_used: str


class AnalyzeRequest(BaseModel):
    """ë²”ìš© VQA (Visual Question Answering) ìš”ì²­"""
    model: str = Field(default="claude-3-5-sonnet-20241022", description="ì‚¬ìš©í•  VL ëª¨ë¸")
    temperature: float = Field(default=0.0, ge=0.0, le=1.0, description="ìƒì„± temperature")


class AnalyzeResponse(BaseModel):
    """ë²”ìš© VQA ì‘ë‹µ"""
    status: str
    mode: str = Field(description="ë¶„ì„ ëª¨ë“œ: 'vqa' (ì§ˆë¬¸-ë‹µë³€) ë˜ëŠ” 'captioning' (ì¼ë°˜ ì„¤ëª…)")
    answer: Optional[str] = Field(None, description="ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ (VQA ëª¨ë“œ)")
    caption: Optional[str] = Field(None, description="ì´ë¯¸ì§€ ì„¤ëª… (ìº¡ì…”ë‹ ëª¨ë“œ)")
    question: Optional[str] = Field(None, description="ì‚¬ìš©ì ì§ˆë¬¸ (VQA ëª¨ë“œ)")
    confidence: float = Field(default=1.0, description="ë‹µë³€ ì‹ ë¢°ë„")
    processing_time: float
    model_used: str


# =====================
# Helper Functions
# =====================

def encode_image_to_base64(image_bytes: bytes) -> str:
    """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©"""
    return base64.b64encode(image_bytes).decode('utf-8')


async def call_claude_api(
    image_bytes: bytes,
    prompt: str,
    model: str = "claude-3-5-sonnet-20241022",
    max_tokens: int = 4096,
    temperature: float = 0.0
) -> str:
    """
    Claude API í˜¸ì¶œ

    Args:
        image_bytes: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
        prompt: í”„ë¡¬í”„íŠ¸
        model: ëª¨ë¸ëª…
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        temperature: ìƒì„± ë‹¤ì–‘ì„± (0-1, 0=ê²°ì •ì , 1=ì°½ì˜ì )

    Returns:
        ëª¨ë¸ ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    if not ANTHROPIC_API_KEY:
        raise HTTPException(status_code=500, detail="ANTHROPIC_API_KEY not set")

    try:
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
        base64_image = encode_image_to_base64(image_bytes)

        # ì´ë¯¸ì§€ í˜•ì‹ ê°ì§€
        img = Image.open(io.BytesIO(image_bytes))
        image_format = img.format.lower() if img.format else "png"
        if image_format == "jpg":
            image_format = "jpeg"

        # Claude API ìš”ì²­
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                "https://api.anthropic.com/v1/messages",
                headers={
                    "x-api-key": ANTHROPIC_API_KEY,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json"
                },
                json={
                    "model": model,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "image",
                                    "source": {
                                        "type": "base64",
                                        "media_type": f"image/{image_format}",
                                        "data": base64_image
                                    }
                                },
                                {
                                    "type": "text",
                                    "text": prompt
                                }
                            ]
                        }
                    ]
                }
            )

            if response.status_code != 200:
                logger.error(f"Claude API error: {response.status_code} - {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"Claude API error: {response.text}"
                )

            result = response.json()
            text = result["content"][0]["text"]

            logger.info(f"Claude API response: {len(text)} characters")
            return text

    except Exception as e:
        logger.error(f"Claude API call failed: {e}")
        raise HTTPException(status_code=500, detail=f"Claude API error: {str(e)}")


async def call_openai_gpt4v_api(
    image_bytes: bytes,
    prompt: str,
    model: str = "gpt-4o",
    max_tokens: int = 4096,
    temperature: float = 0.0
) -> str:
    """
    OpenAI GPT-4V API í˜¸ì¶œ

    Args:
        image_bytes: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
        prompt: í”„ë¡¬í”„íŠ¸
        model: ëª¨ë¸ëª…
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        temperature: ìƒì„± ë‹¤ì–‘ì„± (0-1, 0=ê²°ì •ì , 1=ì°½ì˜ì )

    Returns:
        ëª¨ë¸ ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    if not OPENAI_API_KEY:
        raise HTTPException(status_code=500, detail="OPENAI_API_KEY not set")

    try:
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
        base64_image = encode_image_to_base64(image_bytes)

        # ì´ë¯¸ì§€ í˜•ì‹ ê°ì§€
        img = Image.open(io.BytesIO(image_bytes))
        image_format = img.format.lower() if img.format else "png"
        if image_format == "jpg":
            image_format = "jpeg"

        # OpenAI API ìš”ì²­
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "messages": [
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": prompt
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/{image_format};base64,{base64_image}"
                                    }
                                }
                            ]
                        }
                    ],
                    "max_tokens": max_tokens,
                    "temperature": temperature
                }
            )

            if response.status_code != 200:
                logger.error(f"OpenAI API error: {response.status_code} - {response.text}")
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"OpenAI API error: {response.text}"
                )

            result = response.json()
            text = result["choices"][0]["message"]["content"]

            logger.info(f"OpenAI API response: {len(text)} characters")
            return text

    except Exception as e:
        logger.error(f"OpenAI API call failed: {e}")
        raise HTTPException(status_code=500, detail=f"OpenAI API error: {str(e)}")


async def call_local_vl_api(
    image_bytes: bytes,
    prompt: str = "",
    mode: str = "caption"
) -> str:
    """
    ë¡œì»¬ VL ëª¨ë¸ í˜¸ì¶œ (BLIP)

    Args:
        image_bytes: ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë°ì´í„°
        prompt: í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­, BLIPëŠ” conditional generation ì§€ì›)
        mode: 'caption' (ê¸°ë³¸) ë˜ëŠ” 'vqa'

    Returns:
        ëª¨ë¸ ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    global _florence_model, _florence_processor, _florence_device

    if _florence_model is None:
        raise HTTPException(status_code=500, detail="Local VL model not loaded")

    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        img = Image.open(io.BytesIO(image_bytes)).convert("RGB")

        # BLIPì€ conditional captioning ì§€ì›
        if prompt and prompt.strip():
            # í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ conditional generation
            inputs = _florence_processor(
                images=img,
                text=prompt,
                return_tensors="pt"
            ).to(_florence_device)
        else:
            # í”„ë¡¬í”„íŠ¸ ì—†ìœ¼ë©´ unconditional captioning
            inputs = _florence_processor(
                images=img,
                return_tensors="pt"
            ).to(_florence_device)

        # ì¶”ë¡ 
        with torch.no_grad():
            generated_ids = _florence_model.generate(
                **inputs,
                max_new_tokens=100,
                num_beams=3
            )

        # ë””ì½”ë”©
        generated_text = _florence_processor.decode(
            generated_ids[0], skip_special_tokens=True
        )

        return generated_text.strip()

    except Exception as e:
        logger.error(f"Local VL inference failed: {e}")
        raise HTTPException(status_code=500, detail=f"Local VL error: {str(e)}")


def parse_json_from_text(text: str) -> Union[Dict, List]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ ë° íŒŒì‹±

    ëª¨ë¸ì´ ```json ... ``` í˜•íƒœë¡œ ê°ì‹¼ ê²½ìš° ì²˜ë¦¬
    """
    try:
        # ì½”ë“œ ë¸”ë¡ ì œê±°
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0].strip()
        elif "```" in text:
            text = text.split("```")[1].split("```")[0].strip()

        # JSON íŒŒì‹±
        return json.loads(text)
    except json.JSONDecodeError as e:
        logger.warning(f"JSON parsing failed, attempting to extract: {e}")
        # ë‹¨ìˆœíˆ ì¤‘ê´„í˜¸ ë˜ëŠ” ëŒ€ê´„í˜¸ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
        import re
        json_pattern = r'(\{.*\}|\[.*\])'
        match = re.search(json_pattern, text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                pass

        # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
        logger.error(f"Could not parse JSON from text: {text[:200]}")
        raise ValueError(f"Failed to parse JSON: {text[:200]}")


# =====================
# Startup/Shutdown Events
# =====================

@app.on_event("startup")
async def startup_event():
    """Validate API keys and load Florence-2 on startup"""
    global _api_keys_validated, _available_models, _florence_model, _florence_processor, _florence_device

    logger.info("ğŸš€ Starting VL API...")
    logger.info("ğŸ”‘ Validating API keys...")

    missing_keys = []
    available_models = []

    # Check Anthropic API key
    if ANTHROPIC_API_KEY:
        logger.info("  âœ… ANTHROPIC_API_KEY is set")
        available_models.extend([
            "claude-3-5-sonnet-20241022",
            "claude-3-opus-20240229",
            "claude-3-haiku-20240307"
        ])
    else:
        logger.warning("  âš ï¸  ANTHROPIC_API_KEY is NOT set")
        missing_keys.append("ANTHROPIC_API_KEY")

    # Check OpenAI API key
    if OPENAI_API_KEY:
        logger.info("  âœ… OPENAI_API_KEY is set")
        available_models.extend([
            "gpt-4o",
            "gpt-4-turbo",
            "gpt-4"
        ])
    else:
        logger.warning("  âš ï¸  OPENAI_API_KEY is NOT set")
        missing_keys.append("OPENAI_API_KEY")

    # Load local VL model as fallback (always available)
    logger.info("ğŸ”„ Loading local VL model...")
    try:
        _florence_device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"  Using device: {_florence_device}")

        # Try BLIP-2 (smaller model with better compatibility)
        BLIP_MODEL_ID = "Salesforce/blip-image-captioning-base"
        from transformers import BlipProcessor, BlipForConditionalGeneration

        _florence_processor = BlipProcessor.from_pretrained(BLIP_MODEL_ID)
        _florence_model = BlipForConditionalGeneration.from_pretrained(
            BLIP_MODEL_ID,
            torch_dtype=torch.float16 if _florence_device == "cuda" else torch.float32
        ).to(_florence_device)

        _florence_model.eval()
        available_models.append("blip-base")
        logger.info("  âœ… BLIP model loaded successfully")
    except Exception as e:
        logger.error(f"  âŒ Failed to load local model: {e}")
        logger.warning("  Local model will not be available")

    # Update global state
    _available_models = available_models
    _api_keys_validated = True

    # Log summary
    if missing_keys and not _florence_model:
        logger.error("âŒ No API keys and Florence-2 failed to load! VL API will not work.")
        logger.error("   Set ANTHROPIC_API_KEY or OPENAI_API_KEY environment variables")
    elif missing_keys:
        logger.warning(f"âš ï¸  Missing API keys: {', '.join(missing_keys)}")
        logger.info(f"âœ… Florence-2 available as fallback")
    else:
        logger.info(f"âœ… All API keys validated. Available models: {len(available_models)}")

    logger.info(f"âœ… VL API ready. Available models: {available_models}")


@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    logger.info("ğŸ‘‹ Shutting down VL API...")


# =====================
# API Endpoints
# =====================

@app.get("/api/v1/info")
async def get_api_info():
    """
    API ë©”íƒ€ë°ì´í„° - BlueprintFlow Auto Discoverìš©
    """
    return {
        "id": "vl",
        "name": "VL",
        "display_name": "Vision-Language Model",
        "version": "1.0.0",
        "description": "ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì´í•´í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ AI. ë„ë©´ ë¶„ì„, ì§ˆë¬¸-ë‹µë³€, ì„¤ëª… ìƒì„±",
        "endpoint": "/api/v1/analyze",
        "method": "POST",
        "requires_image": True,

        # ì…ë ¥ ì •ì˜
        "inputs": [
            {
                "name": "image",
                "type": "file",
                "description": "ë¶„ì„í•  ë„ë©´ ì´ë¯¸ì§€",
                "required": True
            },
            {
                "name": "prompt",
                "type": "string",
                "description": "ì§ˆë¬¸ ë˜ëŠ” ë¶„ì„ ìš”ì²­ (ì„ íƒì‚¬í•­). ì˜ˆ: 'ì´ ë„ë©´ì˜ ì¹˜ìˆ˜ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”'",
                "required": False
            }
        ],

        # ì¶œë ¥ ì •ì˜
        "outputs": [
            {
                "name": "mode",
                "type": "string",
                "description": "ë¶„ì„ ëª¨ë“œ ('vqa' ë˜ëŠ” 'captioning')"
            },
            {
                "name": "answer",
                "type": "string",
                "description": "ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ (VQA ëª¨ë“œ)"
            },
            {
                "name": "caption",
                "type": "string",
                "description": "ì´ë¯¸ì§€ ì„¤ëª… (ìº¡ì…”ë‹ ëª¨ë“œ)"
            },
            {
                "name": "confidence",
                "type": "number",
                "description": "ë‹µë³€ ì‹ ë¢°ë„ (0-1)"
            }
        ],

        # íŒŒë¼ë¯¸í„° ì •ì˜
        "parameters": [
            {
                "name": "model",
                "type": "select",
                "options": ["blip-base", "claude-3-5-sonnet-20241022", "gpt-4o", "gpt-4-turbo"],
                "default": "blip-base",
                "description": "ì‚¬ìš©í•  VL ëª¨ë¸ (blip-baseëŠ” ë¡œì»¬ ëª¨ë¸)"
            },
            {
                "name": "temperature",
                "type": "number",
                "default": 0.0,
                "min": 0.0,
                "max": 1.0,
                "step": 0.1,
                "description": "ìƒì„± temperature (0=ê²°ì •ì , 1=ì°½ì˜ì )"
            }
        ],

        # ì…ë ¥ í•„ë“œ ë§¤í•‘
        "input_mappings": {
            "prompt": "inputs.text"  # TextInputì˜ text â†’ VL APIì˜ prompt
        },

        # BlueprintFlow UI ì„¤ì •
        "blueprintflow": {
            "icon": "ğŸ‘ï¸",
            "color": "#ec4899",
            "category": "api"
        },

        # ì¶œë ¥ í•„ë“œ ë§¤í•‘
        "output_mappings": {
            "mode": "mode",
            "answer": "answer",
            "caption": "caption",
            "confidence": "confidence"
        }
    }


@app.get("/health", response_model=HealthResponse)
@app.get("/api/v1/health", response_model=HealthResponse)
async def health_check():
    """
    Health check endpoint / í—¬ìŠ¤ì²´í¬

    Returns the current health status and available VL models.
    """
    global _available_models

    # Use cached available models from startup
    available_models = _available_models if _api_keys_validated else []

    # Fallback: check if keys exist (for backward compatibility)
    if not available_models:
        if ANTHROPIC_API_KEY:
            available_models.extend([
                "claude-3-5-sonnet-20241022",
                "claude-3-opus-20240229",
                "claude-3-haiku-20240307"
            ])
        if OPENAI_API_KEY:
            available_models.extend([
                "gpt-4o",
                "gpt-4-turbo",
                "gpt-4-vision-preview"
            ])

    return HealthResponse(
        status="healthy",
        service="vl-api",
        version="1.0.0",
        timestamp=datetime.now().isoformat(),
        available_models=available_models
    )


@app.post("/api/v1/extract_info_block", response_model=InfoBlockResponse)
async def extract_info_block(
    file: UploadFile = File(...),
    query_fields: str = Form(default='["name", "part number", "material", "scale", "weight"]'),
    model: str = Form(default="claude-3-5-sonnet-20241022"),
    temperature: float = Form(default=0.0, description="Generation temperature (0-1, 0=deterministic, 1=creative)")
):
    """
    Information Blockì—ì„œ íŠ¹ì • ì •ë³´ ì¶”ì¶œ

    ë…¼ë¬¸ ì„¹ì…˜ 4.1 êµ¬í˜„
    """
    start_time = time.time()

    try:
        # íŒŒì¼ ì½ê¸°
        image_bytes = await file.read()

        # query_fields íŒŒì‹±
        fields = json.loads(query_fields)

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""Based on the image, return only a python dictionary extracting this information: {fields}.

The image contains an engineering drawing information block (title block). Extract the requested fields exactly as they appear.

Return ONLY a valid JSON dictionary with the field names as keys and extracted values as values. If a field is not found, use null as the value.

Example format:
{{
    "name": "Intermediate Shaft",
    "part number": "A12-311197-9",
    "material": "STS304",
    "scale": "1:2",
    "weight": "5.2kg"
}}"""

        # ëª¨ë¸ ì„ íƒ ë° í˜¸ì¶œ
        if model.startswith("claude"):
            response_text = await call_claude_api(image_bytes, prompt, model, temperature=temperature)
        elif model.startswith("gpt"):
            response_text = await call_openai_gpt4v_api(image_bytes, prompt, model, temperature=temperature)
        else:
            raise HTTPException(status_code=400, detail=f"Unsupported model: {model}")

        # JSON íŒŒì‹±
        extracted_data = parse_json_from_text(response_text)

        processing_time = time.time() - start_time

        logger.info(f"Extracted info block: {extracted_data}")

        return InfoBlockResponse(
            status="success",
            data=extracted_data,
            processing_time=processing_time,
            model_used=model
        )

    except Exception as e:
        logger.error(f"Info block extraction failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/extract_dimensions", response_model=DimensionExtractionResponse)
async def extract_dimensions(
    file: UploadFile = File(...),
    model: str = Form(default="claude-3-5-sonnet-20241022"),
    temperature: float = Form(default=0.0, description="Generation temperature (0-1)")
):
    """
    VL ëª¨ë¸ë¡œ ì¹˜ìˆ˜ ì¶”ì¶œ (eDOCr ëŒ€ì²´)

    ë…¼ë¬¸ ì„¹ì…˜ 4.4 êµ¬í˜„
    """
    start_time = time.time()

    try:
        # íŒŒì¼ ì½ê¸°
        image_bytes = await file.read()

        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ System Role + Query)
        prompt = """You are a specialized OCR system capable of reading mechanical drawings. You read:
- Measurements: usually scattered and oriented text in the image with arrows in the surroundings. If tolerances are present, read them as "nominal +upper -lower", e.g., "10 +0.1 -0.0"
- Angles: usually oriented text with arrows in the surroundings

Based on the image, return only a python list of strings extracting dimensions.

Examples:
["Ï†476", "Ï†370", "Ï†9.204 +0.1 -0.2", "Ï†1313Â±2", "(177)", "7Â±0.5", "5mm", "1.5", "5"]

Return ONLY a valid JSON list of dimension strings. Do not include any other text or explanation."""

        # ëª¨ë¸ ì„ íƒ ë° í˜¸ì¶œ
        if model.startswith("claude"):
            response_text = await call_claude_api(image_bytes, prompt, model, temperature=temperature)
        elif model.startswith("gpt"):
            response_text = await call_openai_gpt4v_api(image_bytes, prompt, model, temperature=temperature)
        else:
            raise HTTPException(status_code=400, detail=f"Unsupported model: {model}")

        # JSON íŒŒì‹±
        dimensions = parse_json_from_text(response_text)

        if not isinstance(dimensions, list):
            raise ValueError("Response is not a list")

        processing_time = time.time() - start_time

        logger.info(f"Extracted {len(dimensions)} dimensions")

        return DimensionExtractionResponse(
            status="success",
            data=dimensions,
            processing_time=processing_time,
            model_used=model
        )

    except Exception as e:
        logger.error(f"Dimension extraction failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/infer_manufacturing_process", response_model=ManufacturingProcessResponse)
async def infer_manufacturing_process(
    info_block: UploadFile = File(...),
    part_views: UploadFile = File(...),
    model: str = Form(default="gpt-4o"),
    temperature: float = Form(default=0.0, description="Generation temperature (0-1)")
):
    """
    ì œì¡° ê³µì • ì¶”ë¡ 

    ë…¼ë¬¸ ì„¹ì…˜ 4.2 êµ¬í˜„
    """
    start_time = time.time()

    try:
        # íŒŒì¼ ì½ê¸°
        info_block_bytes = await info_block.read()
        part_views_bytes = await part_views.read()

        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ Query)
        prompt = """You are getting the information block of the drawing in the first image and the views of the part in the second image.

I need you to return a python dictionary with the manufacturing processes (keys) and short description (values) that are best for this part.

Consider:
- Part geometry (cylindrical, flat surfaces, holes, etc.)
- Material specifications
- Tolerances and surface finish requirements
- GD&T specifications

Return ONLY a valid JSON dictionary. Example format:
{{
    "Turning": "Used for creating the cylindrical shape of the part, including the outer diameters and chamfers",
    "Drilling/Boring": "To achieve the internal diameter and the countersink specified",
    "Milling": "For creating the flat surfaces if needed",
    "Reaming": "To ensure the internal diameter precision",
    "Grinding": "To achieve the surface finish required on precise diameters",
    "Deburring": "To break all sharp edges and remove burrs as specified"
}}"""

        # ë‘ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸° (side-by-side)
        img1 = Image.open(io.BytesIO(info_block_bytes))
        img2 = Image.open(io.BytesIO(part_views_bytes))

        # ìƒˆ ì´ë¯¸ì§€ ìƒì„± (ê°€ë¡œë¡œ ë‚˜ë€íˆ)
        total_width = img1.width + img2.width
        max_height = max(img1.height, img2.height)
        combined_img = Image.new('RGB', (total_width, max_height), (255, 255, 255))
        combined_img.paste(img1, (0, 0))
        combined_img.paste(img2, (img1.width, 0))

        # bytesë¡œ ë³€í™˜
        img_byte_arr = io.BytesIO()
        combined_img.save(img_byte_arr, format='PNG')
        combined_bytes = img_byte_arr.getvalue()

        # ëª¨ë¸ í˜¸ì¶œ
        if model.startswith("claude"):
            response_text = await call_claude_api(combined_bytes, prompt, model)
        elif model.startswith("gpt"):
            response_text = await call_openai_gpt4v_api(combined_bytes, prompt, model)
        else:
            raise HTTPException(status_code=400, detail=f"Unsupported model: {model}")

        # JSON íŒŒì‹±
        processes = parse_json_from_text(response_text)

        if not isinstance(processes, dict):
            raise ValueError("Response is not a dictionary")

        processing_time = time.time() - start_time

        logger.info(f"Inferred {len(processes)} manufacturing processes")

        return ManufacturingProcessResponse(
            status="success",
            data=processes,
            processing_time=processing_time,
            model_used=model
        )

    except Exception as e:
        logger.error(f"Manufacturing process inference failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/generate_qc_checklist", response_model=QCChecklistResponse)
async def generate_qc_checklist(
    file: UploadFile = File(...),
    model: str = Form(default="gpt-4o"),
    temperature: float = Form(default=0.0, description="Generation temperature (0-1)")
):
    """
    í’ˆì§ˆ ê´€ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸ ìë™ ìƒì„±

    ë…¼ë¬¸ ì„¹ì…˜ 4.3 êµ¬í˜„
    """
    start_time = time.time()

    try:
        # íŒŒì¼ ì½ê¸°
        image_bytes = await file.read()

        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ Query)
        prompt = """I need you to provide a Python list containing only the measurementsâ€”numerical values and tolerancesâ€”that need to be checked in the quality control process.

Focus on:
- Critical dimensions that affect part fit and assembly
- Dimensions with tight tolerances
- Dimensions with GD&T specifications
- Surface finish requirements

Return ONLY a valid JSON list of measurement strings. Example:
["Ã˜21.5 Â± 0.1", "Ã˜38 H12", "Ra 1.6", "Flatness 0.05"]

Do not include reference dimensions or non-critical measurements."""

        # ëª¨ë¸ í˜¸ì¶œ
        if model.startswith("claude"):
            response_text = await call_claude_api(image_bytes, prompt, model)
        elif model.startswith("gpt"):
            response_text = await call_openai_gpt4v_api(image_bytes, prompt, model)
        else:
            raise HTTPException(status_code=400, detail=f"Unsupported model: {model}")

        # JSON íŒŒì‹±
        checklist = parse_json_from_text(response_text)

        if not isinstance(checklist, list):
            raise ValueError("Response is not a list")

        processing_time = time.time() - start_time

        logger.info(f"Generated QC checklist with {len(checklist)} items")

        return QCChecklistResponse(
            status="success",
            data=checklist,
            processing_time=processing_time,
            model_used=model
        )

    except Exception as e:
        logger.error(f"QC checklist generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v1/analyze", response_model=AnalyzeResponse)
async def analyze_image(
    file: UploadFile = File(...),
    prompt: Optional[str] = Form(None, description="ì§ˆë¬¸ ë˜ëŠ” ë¶„ì„ ìš”ì²­ (ì„ íƒì‚¬í•­)"),
    model: str = Form(default="claude-3-5-sonnet-20241022"),
    temperature: float = Form(default=0.0, ge=0.0, le=1.0)
):
    """
    ë²”ìš© VQA (Visual Question Answering) ì—”ë“œí¬ì¸íŠ¸

    - promptê°€ ìˆìœ¼ë©´: ì§ˆë¬¸-ë‹µë³€ ëª¨ë“œ (VQA)
    - promptê°€ ì—†ìœ¼ë©´: ì¼ë°˜ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ëª¨ë“œ

    Examples:
        - "ì´ ë„ë©´ì˜ ëª¨ë“  ì¹˜ìˆ˜ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”"
        - "ìš©ì ‘ ê¸°í˜¸ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”"
        - "ì´ ë¶€í’ˆì˜ ì¬ì§ˆì€ ë¬´ì—‡ì¸ê°€ìš”?"
    """
    start_time = time.time()

    try:
        # íŒŒì¼ ì½ê¸°
        image_bytes = await file.read()

        # í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ VQA ëª¨ë“œ, ì—†ìœ¼ë©´ ìº¡ì…”ë‹ ëª¨ë“œ
        if prompt and prompt.strip():
            # VQA (Visual Question Answering) ëª¨ë“œ
            system_prompt = f"""You are an expert in analyzing engineering drawings and mechanical parts.

User Question: {prompt}

Please answer the question based on the image. Be specific and accurate. If you cannot find the requested information, clearly state that."""

            # VL ëª¨ë¸ í˜¸ì¶œ
            if model.startswith("claude"):
                response_text = await call_claude_api(image_bytes, system_prompt, model, temperature=temperature)
            elif model.startswith("gpt"):
                response_text = await call_openai_gpt4v_api(image_bytes, system_prompt, model, temperature=temperature)
            elif model.startswith("blip") or model.startswith("florence"):
                response_text = await call_local_vl_api(image_bytes, prompt, "vqa")
            else:
                # ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ì¸ ê²½ìš° ë¡œì»¬ ëª¨ë¸ í´ë°±
                if _florence_model is not None:
                    logger.warning(f"Unsupported model {model}, falling back to BLIP")
                    response_text = await call_local_vl_api(image_bytes, prompt, "vqa")
                    model = "blip-base"
                else:
                    raise HTTPException(status_code=400, detail=f"Unsupported model: {model}")

            processing_time = time.time() - start_time

            logger.info(f"VQA completed: Q='{prompt}' A='{response_text[:100]}...'")

            return AnalyzeResponse(
                status="success",
                mode="vqa",
                answer=response_text,
                question=prompt,
                confidence=0.95,  # VL ëª¨ë¸ì˜ ê¸°ë³¸ ì‹ ë¢°ë„
                processing_time=processing_time,
                model_used=model
            )

        else:
            # ì¼ë°˜ ì´ë¯¸ì§€ ìº¡ì…”ë‹ ëª¨ë“œ
            caption_prompt = """Describe this engineering drawing or mechanical part in detail. Include:
- Type of drawing (assembly, detail, section view, etc.)
- Main components visible
- Key features (dimensions, symbols, annotations)
- Overall purpose or function

Provide a concise but informative description."""

            # VL ëª¨ë¸ í˜¸ì¶œ
            if model.startswith("claude"):
                caption_text = await call_claude_api(image_bytes, caption_prompt, model, temperature=temperature)
            elif model.startswith("gpt"):
                caption_text = await call_openai_gpt4v_api(image_bytes, caption_prompt, model, temperature=temperature)
            elif model.startswith("blip") or model.startswith("florence"):
                caption_text = await call_local_vl_api(image_bytes, "", "caption")
            else:
                # ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ì¸ ê²½ìš° ë¡œì»¬ ëª¨ë¸ í´ë°±
                if _florence_model is not None:
                    logger.warning(f"Unsupported model {model}, falling back to BLIP")
                    caption_text = await call_local_vl_api(image_bytes, "", "caption")
                    model = "blip-base"
                else:
                    raise HTTPException(status_code=400, detail=f"Unsupported model: {model}")

            processing_time = time.time() - start_time

            logger.info(f"Captioning completed: '{caption_text[:100]}...'")

            return AnalyzeResponse(
                status="success",
                mode="captioning",
                caption=caption_text,
                confidence=0.90,
                processing_time=processing_time,
                model_used=model
            )

    except Exception as e:
        logger.error(f"Image analysis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =====================
# Main
# =====================

if __name__ == "__main__":
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=5004,
        reload=True,
        log_level="info"
    )
