#!/usr/bin/env python3
"""
YOLOv11 ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
"""
import argparse
from pathlib import Path
from ultralytics import YOLO
import cv2
import json
import time

# í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘
CLASS_NAMES = {
    0: 'diameter_dim',
    1: 'linear_dim',
    2: 'radius_dim',
    3: 'angular_dim',
    4: 'chamfer_dim',
    5: 'tolerance_dim',
    6: 'reference_dim',
    7: 'flatness',
    8: 'cylindricity',
    9: 'position',
    10: 'perpendicularity',
    11: 'parallelism',
    12: 'surface_roughness',
    13: 'text_block'
}

def yolo_to_edocr_format(result, image_shape):
    """YOLO ê²°ê³¼ë¥¼ eDOCr í˜¸í™˜ í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    img_height, img_width = image_shape[:2]

    dimensions = []
    gdt = []
    surface_roughness = []
    text_blocks = []

    boxes = result.boxes

    for i, box in enumerate(boxes):
        # í´ë˜ìŠ¤ IDì™€ ì‹ ë¢°ë„
        cls_id = int(box.cls[0])
        confidence = float(box.conf[0])
        class_name = CLASS_NAMES.get(cls_id, 'unknown')

        # ë°”ìš´ë”© ë°•ìŠ¤ (xyxy í¬ë§·)
        x1, y1, x2, y2 = box.xyxy[0].tolist()

        # í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
        x = int(x1)
        y = int(y1)
        width = int(x2 - x1)
        height = int(y2 - y1)

        bbox = {
            'x': x,
            'y': y,
            'width': width,
            'height': height
        }

        # í´ë˜ìŠ¤ë³„ë¡œ ë¶„ë¥˜
        if cls_id <= 6:  # Dimensions (0-6)
            dimensions.append({
                'type': class_name,
                'value': '',  # OCR refinement í•„ìš”
                'unit': 'mm',
                'bbox': bbox,
                'confidence': confidence
            })

        elif cls_id <= 11:  # GD&T symbols (7-11)
            gdt.append({
                'type': class_name,
                'value': '',  # OCR refinement í•„ìš”
                'bbox': bbox,
                'confidence': confidence
            })

        elif cls_id == 12:  # Surface roughness
            surface_roughness.append({
                'value': '',  # OCR refinement í•„ìš”
                'bbox': bbox,
                'confidence': confidence
            })

        elif cls_id == 13:  # Text block
            text_blocks.append({
                'text': '',  # OCR refinement í•„ìš”
                'bbox': bbox,
                'confidence': confidence
            })

    return {
        'dimensions': dimensions,
        'gdt': gdt,
        'surface_roughness': surface_roughness,
        'text_blocks': text_blocks,
        'total_detections': len(boxes)
    }

def draw_detections(image, result):
    """ì´ë¯¸ì§€ì— ê²€ì¶œ ê²°ê³¼ ê·¸ë¦¬ê¸°"""
    annotated_img = image.copy()
    boxes = result.boxes

    # ìƒ‰ìƒ ì •ì˜ (BGR)
    colors = {
        'dimension': (255, 100, 0),     # Blue for dimensions
        'gdt': (0, 255, 100),           # Green for GD&T
        'surface': (0, 165, 255),       # Orange for surface
        'text': (255, 255, 0)           # Cyan for text
    }

    for box in boxes:
        cls_id = int(box.cls[0])
        confidence = float(box.conf[0])
        class_name = CLASS_NAMES.get(cls_id, 'unknown')

        # ë°”ìš´ë”© ë°•ìŠ¤
        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())

        # ìƒ‰ìƒ ì„ íƒ
        if cls_id <= 6:
            color = colors['dimension']
        elif cls_id <= 11:
            color = colors['gdt']
        elif cls_id == 12:
            color = colors['surface']
        else:
            color = colors['text']

        # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        cv2.rectangle(annotated_img, (x1, y1), (x2, y2), color, 2)

        # ë¼ë²¨ ê·¸ë¦¬ê¸°
        label = f"{class_name} {confidence:.2f}"
        (label_w, label_h), _ = cv2.getTextSize(
            label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1
        )

        cv2.rectangle(
            annotated_img,
            (x1, y1 - label_h - 10),
            (x1 + label_w, y1),
            color,
            -1
        )

        cv2.putText(
            annotated_img,
            label,
            (x1, y1 - 5),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.5,
            (255, 255, 255),
            1
        )

    return annotated_img

def run_inference(
    model_path,
    source,
    output_dir='runs/inference',
    conf_threshold=0.25,
    iou_threshold=0.7,
    imgsz=1280,
    save_images=True,
    save_json=True,
    device='0'
):
    """YOLO ì¶”ë¡  ì‹¤í–‰"""

    print("=" * 70)
    print("ğŸ” YOLOv11 Inference")
    print("=" * 70)
    print(f"Model: {model_path}")
    print(f"Source: {source}")
    print(f"Confidence threshold: {conf_threshold}")
    print(f"Image size: {imgsz}")
    print("=" * 70)

    # ëª¨ë¸ ë¡œë“œ
    model = YOLO(model_path)

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # ì¶”ë¡  ì‹¤í–‰
    start_time = time.time()

    results = model.predict(
        source=source,
        conf=conf_threshold,
        iou=iou_threshold,
        imgsz=imgsz,
        device=device,
        save=False,  # ìš°ë¦¬ê°€ ì§ì ‘ ì €ì¥
        verbose=True
    )

    elapsed_time = time.time() - start_time

    # ê²°ê³¼ ì²˜ë¦¬
    print(f"\nğŸ“Š Processing {len(results)} images...")

    all_results = []

    for i, result in enumerate(results):
        image_path = Path(result.path)
        image_name = image_path.stem

        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(str(image_path))

        # eDOCr í¬ë§·ìœ¼ë¡œ ë³€í™˜
        detection_result = yolo_to_edocr_format(result, image.shape)

        detection_result['image_name'] = image_name
        detection_result['image_path'] = str(image_path)
        detection_result['model'] = str(model_path)
        detection_result['inference_time'] = elapsed_time / len(results)

        all_results.append(detection_result)

        # í†µê³„ ì¶œë ¥
        print(f"âœ… {image_name}: {detection_result['total_detections']} detections")

        # ì–´ë…¸í…Œì´ì…˜ëœ ì´ë¯¸ì§€ ì €ì¥
        if save_images:
            annotated_img = draw_detections(image, result)
            save_path = output_path / f"{image_name}_annotated.jpg"
            cv2.imwrite(str(save_path), annotated_img)

        # JSON ì €ì¥
        if save_json:
            json_path = output_path / f"{image_name}_result.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(detection_result, f, indent=2, ensure_ascii=False)

    # ì „ì²´ í†µê³„
    total_detections = sum(r['total_detections'] for r in all_results)
    avg_time = elapsed_time / len(results)

    print("\n" + "=" * 70)
    print("âœ… Inference complete!")
    print("=" * 70)
    print(f"ğŸ“Š Statistics:")
    print(f"   - Total images: {len(results)}")
    print(f"   - Total detections: {total_detections}")
    print(f"   - Average detections/image: {total_detections / len(results):.1f}")
    print(f"   - Total time: {elapsed_time:.2f}s")
    print(f"   - Average time/image: {avg_time:.2f}s")
    print(f"   - FPS: {1/avg_time:.2f}")
    print(f"ğŸ“ Results saved to: {output_path}")

    # ì „ì²´ ìš”ì•½ ì €ì¥
    summary = {
        'total_images': len(results),
        'total_detections': total_detections,
        'average_detections_per_image': total_detections / len(results),
        'total_time': elapsed_time,
        'average_time_per_image': avg_time,
        'fps': 1 / avg_time,
        'results': all_results
    }

    summary_path = output_path / 'summary.json'
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    return summary

def main():
    parser = argparse.ArgumentParser(description='YOLOv11 Inference on engineering drawings')

    parser.add_argument('--model', type=str, required=True,
                        help='Path to trained model (best.pt)')
    parser.add_argument('--source', type=str, required=True,
                        help='Image file or directory')
    parser.add_argument('--output', type=str, default='runs/inference',
                        help='Output directory')
    parser.add_argument('--conf', type=float, default=0.25,
                        help='Confidence threshold')
    parser.add_argument('--iou', type=float, default=0.7,
                        help='NMS IoU threshold')
    parser.add_argument('--imgsz', type=int, default=1280,
                        help='Image size')
    parser.add_argument('--device', type=str, default='0',
                        help='CUDA device or cpu')
    parser.add_argument('--no-save-images', action='store_true',
                        help='Do not save annotated images')
    parser.add_argument('--no-save-json', action='store_true',
                        help='Do not save JSON results')

    args = parser.parse_args()

    run_inference(
        model_path=args.model,
        source=args.source,
        output_dir=args.output,
        conf_threshold=args.conf,
        iou_threshold=args.iou,
        imgsz=args.imgsz,
        save_images=not args.no_save_images,
        save_json=not args.no_save_json,
        device=args.device
    )

if __name__ == '__main__':
    main()
