#!/usr/bin/env python3
"""
YOLOv11 API Server for Engineering Drawing Analysis
Port: 5005

í†µí•© YOLO API - ì—¬ëŸ¬ ëª¨ë¸ì„ ë™ì ìœ¼ë¡œ ë¡œë”©í•˜ì—¬ ì‚¬ìš©
"""
import os
import time
import json
import uuid
import base64
import yaml
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List

import cv2
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch

from models.schemas import (
    Detection, DetectionResponse, HealthResponse,
    APIInfoResponse, ParameterSchema, IOSchema, BlueprintFlowMetadata
)
from services.inference import YOLOInferenceService
from utils.helpers import draw_detections_on_image

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# =====================
# Configuration
# =====================

YOLO_API_PORT = int(os.getenv('YOLO_API_PORT', '5005'))
YOLO_MODEL_PATH = os.getenv('YOLO_MODEL_PATH', '/app/models/best.pt')
MODELS_DIR = Path('/app/models')
MODEL_REGISTRY_PATH = MODELS_DIR / 'model_registry.yaml'
UPLOAD_DIR = Path('/tmp/yolo-api/uploads')
RESULTS_DIR = Path('/tmp/yolo-api/results')

# Create directories
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
RESULTS_DIR.mkdir(parents=True, exist_ok=True)


# =====================
# SAHI Sliced Inference
# =====================

# SAHI ìºì‹œ (ëª¨ë¸ ê²½ë¡œë³„)
_sahi_model_cache: Dict[str, Any] = {}


def run_sahi_inference(
    model_path: str,
    image_path: str,
    confidence: float = 0.25,
    slice_height: int = 512,
    slice_width: int = 512,
    overlap_ratio: float = 0.25
) -> List[Dict[str, Any]]:
    """
    SAHI ìŠ¬ë¼ì´ì‹± ê¸°ë°˜ ì¶”ë¡ 

    ëŒ€í˜• ì´ë¯¸ì§€ì—ì„œ ìž‘ì€ ê°ì²´ë¥¼ ê²€ì¶œí•˜ê¸° ìœ„í•œ ìŠ¬ë¼ì´ì‹± ê¸°ë²•
    """
    try:
        from sahi import AutoDetectionModel
        from sahi.predict import get_sliced_prediction

        # ìºì‹œì—ì„œ SAHI ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        if model_path not in _sahi_model_cache:
            logger.info(f"SAHI ëª¨ë¸ ë¡œë”©: {model_path}")
            sahi_model = AutoDetectionModel.from_pretrained(
                model_type="yolov8",
                model_path=model_path,
                confidence_threshold=confidence,
                device="cuda:0" if torch.cuda.is_available() else "cpu"
            )
            _sahi_model_cache[model_path] = sahi_model
        else:
            sahi_model = _sahi_model_cache[model_path]
            # confidence ì—…ë°ì´íŠ¸
            sahi_model.confidence_threshold = confidence

        # SAHI ìŠ¬ë¼ì´ì‹± ì¶”ë¡ 
        result = get_sliced_prediction(
            image_path,
            sahi_model,
            slice_height=slice_height,
            slice_width=slice_width,
            overlap_height_ratio=overlap_ratio,
            overlap_width_ratio=overlap_ratio,
            perform_standard_pred=True,
            postprocess_type="NMS",
            postprocess_match_threshold=0.5
        )

        # Detection ê°ì²´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        from models.schemas import Detection
        detections = []
        for i, pred in enumerate(result.object_prediction_list):
            bbox = pred.bbox.to_xyxy()
            x1, y1, x2, y2 = bbox

            # bboxë¥¼ Dict[str, int] í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (schemas.py ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜)
            detection = Detection(
                class_id=pred.category.id if pred.category else 0,
                class_name=pred.category.name if pred.category else "object",
                confidence=round(pred.score.value, 4),
                bbox={
                    'x': int(x1),
                    'y': int(y1),
                    'width': int(x2 - x1),
                    'height': int(y2 - y1)
                }
            )
            detections.append(detection)

        logger.info(f"SAHI ê²€ì¶œ ì™„ë£Œ: {len(detections)}ê°œ")
        return detections

    except ImportError:
        logger.warning("SAHI not installed, falling back to standard inference")
        return None
    except Exception as e:
        logger.error(f"SAHI inference error: {e}")
        import traceback
        traceback.print_exc()
        return None


# =====================
# Model Registry
# =====================

class ModelRegistry:
    """YAML ê¸°ë°˜ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê´€ë¦¬"""

    def __init__(self, registry_path: Path, models_dir: Path):
        self.registry_path = registry_path
        self.models_dir = models_dir
        self._registry: Dict[str, Any] = {}
        self._model_cache: Dict[str, YOLOInferenceService] = {}
        self.load_registry()

    def load_registry(self):
        """ë ˆì§€ìŠ¤íŠ¸ë¦¬ íŒŒì¼ ë¡œë“œ"""
        if self.registry_path.exists():
            with open(self.registry_path, 'r', encoding='utf-8') as f:
                self._registry = yaml.safe_load(f) or {}
            logger.info(f"ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë¡œë“œ: {len(self._registry.get('models', {}))}ê°œ ëª¨ë¸")
        else:
            self._registry = {'models': {}, 'default_model': 'engineering'}
            self.save_registry()
            logger.info("ìƒˆ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ìƒì„±")

    def save_registry(self):
        """ë ˆì§€ìŠ¤íŠ¸ë¦¬ íŒŒì¼ ì €ìž¥"""
        with open(self.registry_path, 'w', encoding='utf-8') as f:
            yaml.dump(self._registry, f, allow_unicode=True, default_flow_style=False)

    def get_models(self) -> Dict[str, Any]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡"""
        models = {}
        for model_id, info in self._registry.get('models', {}).items():
            file_path = self.models_dir / info.get('file', '')
            models[model_id] = {
                **info,
                'id': model_id,
                'file_exists': file_path.exists(),
                'file_size_mb': round(file_path.stat().st_size / 1024 / 1024, 2) if file_path.exists() else 0
            }
        return models

    def get_model(self, model_id: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ëª¨ë¸ ì •ë³´"""
        return self._registry.get('models', {}).get(model_id)

    def get_default_model(self) -> str:
        """ê¸°ë³¸ ëª¨ë¸ ID"""
        return self._registry.get('default_model', 'engineering')

    def add_model(self, model_id: str, info: Dict[str, Any]):
        """ëª¨ë¸ ë“±ë¡"""
        if 'models' not in self._registry:
            self._registry['models'] = {}
        self._registry['models'][model_id] = info
        self.save_registry()
        logger.info(f"ëª¨ë¸ ë“±ë¡: {model_id}")

    def update_model(self, model_id: str, info: Dict[str, Any]):
        """ëª¨ë¸ ì •ë³´ ì—…ë°ì´íŠ¸"""
        if model_id in self._registry.get('models', {}):
            self._registry['models'][model_id].update(info)
            self.save_registry()
            logger.info(f"ëª¨ë¸ ì—…ë°ì´íŠ¸: {model_id}")

    def delete_model(self, model_id: str) -> bool:
        """ëª¨ë¸ ì‚­ì œ (íŒŒì¼ì€ ìœ ì§€, ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œë§Œ ì œê±°)"""
        if model_id in self._registry.get('models', {}):
            del self._registry['models'][model_id]
            if model_id in self._model_cache:
                del self._model_cache[model_id]
            self.save_registry()
            logger.info(f"ëª¨ë¸ ì‚­ì œ: {model_id}")
            return True
        return False

    def get_inference_service(self, model_id: str) -> Optional[YOLOInferenceService]:
        """ëª¨ë¸ ë¡œë“œ (ìºì‹œ ì‚¬ìš©)"""
        # ìºì‹œì— ìžˆìœ¼ë©´ ë°˜í™˜
        if model_id in self._model_cache:
            return self._model_cache[model_id]

        # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ëª¨ë¸ ì •ë³´ ì¡°íšŒ
        model_info = self.get_model(model_id)
        if not model_info:
            logger.warning(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_id}")
            return None

        # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        model_path = self.models_dir / model_info.get('file', '')
        if not model_path.exists():
            logger.warning(f"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŒ: {model_path}")
            return None

        # ëª¨ë¸ ë¡œë“œ
        logger.info(f"ëª¨ë¸ ë¡œë”©: {model_id} ({model_path})")
        service = YOLOInferenceService(str(model_path))
        service.load_model()

        # ìºì‹œì— ì €ìž¥
        self._model_cache[model_id] = service
        return service


# Global model registry
model_registry: Optional[ModelRegistry] = None


# =====================
# FastAPI App
# =====================

app = FastAPI(
    title="YOLOv11 Drawing Analysis API",
    description="í†µí•© YOLO API - ë‹¤ì¤‘ ëª¨ë¸ ì§€ì› (ê¸°ê³„ë„ë©´, P&ID ë“±)",
    version="2.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global inference service (ê¸°ë³¸ ëª¨ë¸ìš©, í•˜ìœ„ í˜¸í™˜ì„±)
inference_service: Optional[YOLOInferenceService] = None


# =====================
# Startup / Shutdown
# =====================

@app.on_event("startup")
async def startup_event():
    """Load model registry and default model on startup"""
    global inference_service, model_registry

    print("=" * 70)
    print("ðŸš€ YOLOv11 í†µí•© API Server Starting...")
    print("=" * 70)

    # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”
    model_registry = ModelRegistry(MODEL_REGISTRY_PATH, MODELS_DIR)

    # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
    default_model = model_registry.get_default_model()
    inference_service = model_registry.get_inference_service(default_model)

    if inference_service is None:
        # í´ë°±: í™˜ê²½ë³€ìˆ˜ ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©
        logger.warning(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, í´ë°±: {YOLO_MODEL_PATH}")
        inference_service = YOLOInferenceService(YOLO_MODEL_PATH)
        inference_service.load_model()

    print(f"ðŸ“¦ ë“±ë¡ëœ ëª¨ë¸: {len(model_registry.get_models())}ê°œ")
    print(f"âœ… ê¸°ë³¸ ëª¨ë¸: {default_model}")

    print("=" * 70)
    print(f"âœ… Server ready on port {YOLO_API_PORT}")
    print("=" * 70)


@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    print("ðŸ›‘ Shutting down YOLOv11 API Server...")


# =====================
# API Endpoints
# =====================

@app.get("/api/v1/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    gpu_name = None
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)

    return HealthResponse(
        status="healthy",
        model_loaded=inference_service is not None and inference_service.model is not None,
        model_path=YOLO_MODEL_PATH,
        device=inference_service.device if inference_service else "unknown",
        gpu_available=torch.cuda.is_available(),
        gpu_name=gpu_name
    )


@app.get("/api/v1/info", response_model=APIInfoResponse)
async def get_api_info():
    """
    API ë©”íƒ€ë°ì´í„° ì—”ë“œí¬ì¸íŠ¸

    BlueprintFlow ë° Dashboardì—ì„œ APIë¥¼ ìžë™ìœ¼ë¡œ ë“±ë¡í•˜ê¸° ìœ„í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    return APIInfoResponse(
        id="yolo-detector",
        name="YOLO Detection API",
        display_name="YOLO ê°ì²´ ê²€ì¶œ",
        version="1.0.0",
        description="YOLOv11 ê¸°ë°˜ ë„ë©´ ì‹¬ë³¼/ì¹˜ìˆ˜/GD&T ê²€ì¶œ API",
        openapi_url="/openapi.json",
        base_url=f"http://localhost:{YOLO_API_PORT}",
        endpoint="/api/v1/detect",
        method="POST",
        requires_image=True,
        inputs=[
            IOSchema(
                name="file",
                type="file",
                description="ë¶„ì„í•  ë„ë©´ ì´ë¯¸ì§€ íŒŒì¼",
                required=True
            )
        ],
        outputs=[
            IOSchema(
                name="detections",
                type="array",
                description="ê²€ì¶œëœ ê°ì²´ ëª©ë¡ (ê° ê°ì²´ëŠ” class_id, class_name, confidence, bbox í¬í•¨)"
            ),
            IOSchema(
                name="total_detections",
                type="integer",
                description="ì´ ê²€ì¶œëœ ê°ì²´ ê°œìˆ˜"
            ),
            IOSchema(
                name="processing_time",
                type="float",
                description="ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)"
            ),
            IOSchema(
                name="visualized_image",
                type="string",
                description="ê²€ì¶œ ê²°ê³¼ê°€ í‘œì‹œëœ ì´ë¯¸ì§€ (base64)"
            )
        ],
        parameters=[
            ParameterSchema(
                name="model_type",
                type="select",
                default="yolo11n-general",
                description="ìš©ë„ë³„ íŠ¹í™” ëª¨ë¸ ì„ íƒ",
                required=False,
                options=[
                    "symbol-detector-v1",
                    "dimension-detector-v1",
                    "gdt-detector-v1",
                    "text-region-detector-v1",
                    "yolo11n-general"
                ]
            ),
            ParameterSchema(
                name="confidence",
                type="number",
                default=0.5,
                description="ê²€ì¶œ ì‹ ë¢°ë„ ìž„ê³„ê°’ (ë‚®ì„ìˆ˜ë¡ ë” ë§Žì´ ê²€ì¶œ)",
                required=False,
                min=0.0,
                max=1.0,
                step=0.05
            ),
            ParameterSchema(
                name="iou_threshold",
                type="number",
                default=0.45,
                description="NMS IoU ìž„ê³„ê°’ (ê²¹ì¹¨ ì œê±°, ë†’ì„ìˆ˜ë¡ ì—„ê²©)",
                required=False,
                min=0.0,
                max=1.0,
                step=0.05
            ),
            ParameterSchema(
                name="imgsz",
                type="select",
                default="640",
                description="ìž…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (í´ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)",
                required=False,
                options=["320", "640", "1280"]
            ),
            ParameterSchema(
                name="visualize",
                type="boolean",
                default=True,
                description="ê²€ì¶œ ê²°ê³¼ ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±",
                required=False
            ),
            ParameterSchema(
                name="task",
                type="select",
                default="detect",
                description="ê²€ì¶œ ëª¨ë“œ (ì „ì²´ ê²€ì¶œ vs ì„¸ê·¸ë©˜í…Œì´ì…˜)",
                required=False,
                options=["detect", "segment"]
            )
        ],
        blueprintflow=BlueprintFlowMetadata(
            icon="ðŸŽ¯",
            color="#3b82f6",
            category="detection"
        ),
        output_mappings={
            "detections": "detections",
            "total_detections": "total_detections",
            "processing_time": "processing_time",
            "visualized_image": "visualized_image"
        }
    )


@app.post("/api/v1/detect", response_model=DetectionResponse)
async def detect_objects(
    file: UploadFile = File(...),
    model_type: str = Form(default="yolo11n-general", description="Model type (engineering/pid_symbol/pid_class_agnostic/pid_class_aware)"),
    confidence: float = Form(default=0.5, description="Confidence threshold (0-1)", alias="conf_threshold"),
    iou_threshold: float = Form(default=0.45, description="NMS IoU threshold (0-1)"),
    imgsz: int = Form(default=640, description="Input image size (320/640/1280)"),
    visualize: bool = Form(default=True, description="Generate visualization image"),
    task: str = Form(default="detect", description="Task type (detect/segment)"),
    # SAHI ìŠ¬ë¼ì´ì‹± íŒŒë¼ë¯¸í„°
    use_sahi: bool = Form(default=False, description="Enable SAHI slicing for large images"),
    slice_height: int = Form(default=512, description="SAHI slice height"),
    slice_width: int = Form(default=512, description="SAHI slice width"),
    overlap_ratio: float = Form(default=0.25, description="SAHI slice overlap ratio (0-0.5)")
):
    """
    Object detection endpoint (all classes)

    Args:
        file: Image file
        model_type: Model type (engineering, pid_symbol, pid_class_agnostic, pid_class_aware)
        confidence: Confidence threshold (0-1)
        iou_threshold: NMS IoU threshold (0-1)
        imgsz: Input image size (320, 640, 1280)
        visualize: Generate visualization image
        task: Task type (detect or segment)
        use_sahi: Enable SAHI slicing for large images (recommended for P&ID)
        slice_height: SAHI slice height (256-2048)
        slice_width: SAHI slice width (256-2048)
        overlap_ratio: SAHI slice overlap ratio (0-0.5)

    Returns:
        DetectionResponse with detection results
    """
    start_time = time.time()

    try:
        # ëª¨ë¸ ì„ íƒ (ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê¸°ë°˜)
        # í•˜ìœ„ í˜¸í™˜ì„±: ê¸°ì¡´ model_type ê°’ì„ ìƒˆ ëª¨ë¸ IDë¡œ ë§¤í•‘
        model_id_map = {
            "symbol-detector-v1": "engineering",
            "dimension-detector-v1": "engineering",
            "gdt-detector-v1": "engineering",
            "text-region-detector-v1": "engineering",
            "yolo11n-general": "engineering",
            # ìƒˆ ëª¨ë¸ IDëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
            "engineering": "engineering",
            "pid_symbol": "pid_symbol",
            "pid_class_agnostic": "pid_class_agnostic",
            "pid_class_aware": "pid_class_aware",
        }
        model_id = model_id_map.get(model_type, model_type)

        # ì„ íƒëœ ëª¨ë¸ ë¡œë“œ (ìºì‹œ ì‚¬ìš©)
        selected_service = model_registry.get_inference_service(model_id) if model_registry else None
        if selected_service is None:
            # í´ë°±: ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
            logger.warning(f"ëª¨ë¸ '{model_id}' ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
            selected_service = inference_service

        if selected_service is None or selected_service.model is None:
            raise HTTPException(status_code=503, detail="Model not loaded")

        # Save uploaded file
        file_id = str(uuid.uuid4())
        file_ext = Path(file.filename).suffix
        file_path = UPLOAD_DIR / f"{file_id}{file_ext}"

        content = await file.read()
        with open(file_path, 'wb') as f:
            f.write(content)

        # Load image for size info
        image = cv2.imread(str(file_path))
        if image is None:
            raise HTTPException(status_code=400, detail="Invalid image file")

        img_height, img_width = image.shape[:2]

        # P&ID ëª¨ë¸ì´ë©´ì„œ use_sahi=Falseì¸ ê²½ìš° ìžë™ìœ¼ë¡œ SAHI í™œì„±í™”
        is_pid_model = model_id.startswith('pid_')
        if is_pid_model and not use_sahi:
            use_sahi = True
            logger.info(f"P&ID ëª¨ë¸ ìžë™ SAHI í™œì„±í™”: {model_id}")

        # SAHI ë˜ëŠ” ì¼ë°˜ ì¶”ë¡  ì„ íƒ
        if use_sahi:
            # SAHI ìŠ¬ë¼ì´ì‹± ê¸°ë°˜ ì¶”ë¡ 
            model_info = model_registry.get_model(model_id) if model_registry else None
            model_file = model_info.get('file', 'best.pt') if model_info else 'best.pt'
            model_path = str(MODELS_DIR / model_file)

            logger.info(f"SAHI ì¶”ë¡ : model={model_id}, slice={slice_height}x{slice_width}, overlap={overlap_ratio}")
            detections = run_sahi_inference(
                model_path=model_path,
                image_path=str(file_path),
                confidence=confidence,
                slice_height=slice_height,
                slice_width=slice_width,
                overlap_ratio=overlap_ratio
            )

            # SAHI ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ì¶”ë¡ ìœ¼ë¡œ í´ë°±
            if detections is None:
                logger.warning("SAHI ì¶”ë¡  ì‹¤íŒ¨, ì¼ë°˜ ì¶”ë¡ ìœ¼ë¡œ í´ë°±")
                detections = selected_service.predict(
                    image_path=str(file_path),
                    conf_threshold=confidence,
                    iou_threshold=iou_threshold,
                    imgsz=imgsz,
                    task=task
                )
        else:
            # ì¼ë°˜ YOLO ì¶”ë¡ 
            detections = selected_service.predict(
                image_path=str(file_path),
                conf_threshold=confidence,
                iou_threshold=iou_threshold,
                imgsz=imgsz,
                task=task
            )

        # Post-processing: filter text blocks and remove duplicates
        original_count = len(detections)
        # P&ID ëª¨ë¸ ë˜ëŠ” bom_detectorëŠ” í•„í„°ë§ ê±´ë„ˆë›°ê¸° (Streamlitê³¼ ë™ì¼í•˜ê²Œ)
        is_bom_model = model_id == 'bom_detector'
        if not is_pid_model and not is_bom_model:
            detections = selected_service.filter_text_blocks(detections, min_confidence=0.65)
            detections = selected_service.remove_duplicate_detections(detections, iou_threshold=0.3)
        filtered_count = len(detections)
        final_count = len(detections)

        # Generate visualization
        visualized_image_base64 = None
        if visualize and len(detections) > 0:
            annotated_img = draw_detections_on_image(image, detections)
            viz_path = RESULTS_DIR / f"{file_id}_annotated.jpg"
            cv2.imwrite(str(viz_path), annotated_img)

            # Encode to base64
            _, buffer = cv2.imencode('.jpg', annotated_img)
            visualized_image_base64 = base64.b64encode(buffer).decode('utf-8')

        # Save JSON result
        result_json = {
            'file_id': file_id,
            'detections': [det.dict() for det in detections],
            'total_detections': len(detections),
            'processing_time': time.time() - start_time,
            'model_used': model_id,
            'image_size': {'width': img_width, 'height': img_height},
            'filtering_stats': {
                'original_count': original_count,
                'after_text_filter': filtered_count,
                'final_count': final_count,
                'text_blocks_removed': original_count - filtered_count,
                'duplicates_removed': filtered_count - final_count
            }
        }

        json_path = RESULTS_DIR / f"{file_id}_result.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result_json, f, indent=2, ensure_ascii=False)

        processing_time = time.time() - start_time

        return DetectionResponse(
            status="success",
            file_id=file_id,
            detections=detections,
            total_detections=len(detections),
            processing_time=processing_time,
            model_used=model_id,
            image_size={'width': img_width, 'height': img_height},
            visualized_image=visualized_image_base64
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Detection failed: {str(e)}")


@app.post("/api/v1/extract_dimensions")
async def extract_dimensions(
    file: UploadFile = File(...),
    confidence: float = Form(default=0.35, alias="conf_threshold"),
    imgsz: int = Form(default=1280),
    model_type: str = Form(default="dimension-detector-v1")
):
    """
    Extract dimensions (dimensions, GD&T, surface roughness separated)

    Args:
        file: Image file
        confidence: Confidence threshold
        imgsz: Input image size
        model_type: Model type (defaults to dimension-detector-v1)

    Returns:
        Classified detection results
    """
    # Call detect API with dimension-optimized model
    detection_result = await detect_objects(
        file=file,
        model_type=model_type,
        confidence=confidence,
        imgsz=imgsz,
        visualize=True,
        iou_threshold=0.45,
        task="detect"
    )

    # Classify by class
    dimensions = [d for d in detection_result.detections if d.class_id <= 6]
    gdt_symbols = [d for d in detection_result.detections if 7 <= d.class_id <= 11]
    surface_roughness = [d for d in detection_result.detections if d.class_id == 12]
    text_blocks = [d for d in detection_result.detections if d.class_id == 13]

    return {
        'status': 'success',
        'file_id': detection_result.file_id,
        'dimensions': dimensions,
        'gdt_symbols': gdt_symbols,
        'surface_roughness': surface_roughness,
        'text_blocks': text_blocks,
        'total_detections': detection_result.total_detections,
        'processing_time': detection_result.processing_time,
        'model_used': detection_result.model_used
    }


@app.get("/api/v1/download/{file_id}")
async def download_result(
    file_id: str,
    result_type: str = "annotated"
):
    """
    Download result file

    Args:
        file_id: File ID
        result_type: Result type (annotated, json)

    Returns:
        File response
    """
    from fastapi.responses import FileResponse

    if result_type == "annotated":
        file_path = RESULTS_DIR / f"{file_id}_annotated.jpg"
        media_type = "image/jpeg"
    elif result_type == "json":
        file_path = RESULTS_DIR / f"{file_id}_result.json"
        media_type = "application/json"
    else:
        raise HTTPException(status_code=400, detail="Invalid result_type")

    if not file_path.exists():
        raise HTTPException(status_code=404, detail="File not found")

    return FileResponse(
        path=str(file_path),
        media_type=media_type,
        filename=file_path.name
    )


@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "service": "YOLOv11 Drawing Analysis API",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/api/v1/health",
            "detect": "/api/v1/detect",
            "extract_dimensions": "/api/v1/extract_dimensions",
            "download": "/api/v1/download/{file_id}",
            "docs": "/docs"
        }
    }


# =====================
# Model Registry API
# =====================

class ModelInfo(BaseModel):
    """ëª¨ë¸ ì •ë³´ ìŠ¤í‚¤ë§ˆ"""
    name: str
    description: str
    best_for: Optional[str] = None
    classes: Optional[int] = None
    file: Optional[str] = None


@app.get("/api/v1/models")
async def get_models():
    """
    ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ

    Returns:
        models: ëª¨ë¸ ëª©ë¡ (ID, ì´ë¦„, ì„¤ëª…, íŒŒì¼ í¬ê¸° ë“±)
        default_model: ê¸°ë³¸ ëª¨ë¸ ID
    """
    if model_registry is None:
        raise HTTPException(status_code=503, detail="Model registry not initialized")

    models = model_registry.get_models()
    return {
        "models": list(models.values()),
        "default_model": model_registry.get_default_model(),
        "total": len(models)
    }


@app.get("/api/v1/models/{model_id}")
async def get_model(model_id: str):
    """íŠ¹ì • ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    if model_registry is None:
        raise HTTPException(status_code=503, detail="Model registry not initialized")

    models = model_registry.get_models()
    if model_id not in models:
        raise HTTPException(status_code=404, detail=f"Model '{model_id}' not found")

    return models[model_id]


@app.post("/api/v1/models/{model_id}")
async def add_or_update_model(model_id: str, info: ModelInfo):
    """
    ëª¨ë¸ ë“±ë¡/ìˆ˜ì •

    ëª¨ë¸ íŒŒì¼(.pt)ì€ ë³„ë„ë¡œ ì—…ë¡œë“œí•´ì•¼ í•¨
    """
    if model_registry is None:
        raise HTTPException(status_code=503, detail="Model registry not initialized")

    model_data = info.dict(exclude_none=True)

    if model_registry.get_model(model_id):
        model_registry.update_model(model_id, model_data)
        return {"message": f"Model '{model_id}' updated", "model_id": model_id}
    else:
        model_registry.add_model(model_id, model_data)
        return {"message": f"Model '{model_id}' added", "model_id": model_id}


@app.delete("/api/v1/models/{model_id}")
async def delete_model(model_id: str):
    """ëª¨ë¸ ì‚­ì œ (ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œë§Œ ì œê±°, íŒŒì¼ì€ ìœ ì§€)"""
    if model_registry is None:
        raise HTTPException(status_code=503, detail="Model registry not initialized")

    if model_id == model_registry.get_default_model():
        raise HTTPException(status_code=400, detail="Cannot delete default model")

    if model_registry.delete_model(model_id):
        return {"message": f"Model '{model_id}' deleted"}
    else:
        raise HTTPException(status_code=404, detail=f"Model '{model_id}' not found")


@app.post("/api/v1/models/{model_id}/upload")
async def upload_model_file(
    model_id: str,
    file: UploadFile = File(..., description="YOLO ëª¨ë¸ íŒŒì¼ (.pt)")
):
    """ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ"""
    if model_registry is None:
        raise HTTPException(status_code=503, detail="Model registry not initialized")

    if not file.filename.endswith('.pt'):
        raise HTTPException(status_code=400, detail="Only .pt files are allowed")

    # íŒŒì¼ ì €ìž¥
    file_path = MODELS_DIR / f"{model_id}.pt"
    content = await file.read()
    with open(file_path, 'wb') as f:
        f.write(content)

    # ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— íŒŒì¼ ê²½ë¡œ ì—…ë°ì´íŠ¸
    if model_registry.get_model(model_id):
        model_registry.update_model(model_id, {"file": f"{model_id}.pt"})
    else:
        model_registry.add_model(model_id, {"file": f"{model_id}.pt", "name": model_id})

    # ìºì‹œ ë¬´íš¨í™” (ë‹¤ìŒ ìš”ì²­ì‹œ ìƒˆ ëª¨ë¸ ë¡œë“œ)
    if model_id in model_registry._model_cache:
        del model_registry._model_cache[model_id]

    file_size_mb = len(content) / 1024 / 1024
    return {
        "message": f"Model file uploaded: {file.filename}",
        "model_id": model_id,
        "file_size_mb": round(file_size_mb, 2)
    }


# =====================
# Main
# =====================

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=YOLO_API_PORT,
        reload=False,
        log_level="info"
    )
