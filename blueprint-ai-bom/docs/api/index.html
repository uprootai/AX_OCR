<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DrawingBOMExtractor - AI Models Research Documentation</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background: #ffffff;
            color: #333;
            line-height: 1.6;
        }

        .container {
            display: flex;
            min-height: 100vh;
        }

        /* Sidebar */
        .sidebar {
            width: 280px;
            background: #f8f9fa;
            border-right: 1px solid #e9ecef;
            padding: 0;
            position: fixed;
            height: 100vh;
            overflow-y: auto;
        }

        .logo {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px 20px;
            text-align: center;
        }

        .logo h1 {
            font-size: 22px;
            font-weight: 700;
            margin-bottom: 5px;
        }

        .logo p {
            font-size: 12px;
            opacity: 0.9;
        }

        .nav-section {
            padding: 15px;
        }

        .nav-section h3 {
            color: #6c757d;
            font-size: 11px;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 10px;
            padding: 0 10px;
        }

        .nav-item {
            display: flex;
            align-items: center;
            padding: 10px 15px;
            margin: 3px 0;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.2s ease;
            text-decoration: none;
            color: #495057;
            font-size: 14px;
        }

        .nav-item:hover {
            background: #e9ecef;
            color: #212529;
        }

        .nav-item.active {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }

        .nav-item .icon {
            margin-right: 10px;
            width: 20px;
            text-align: center;
        }

        /* Main Content */
        .content {
            flex: 1;
            margin-left: 280px;
            background: white;
        }

        .main-content {
            max-width: 1200px;
            margin: 0 auto;
            padding: 40px;
        }

        /* Hero Section */
        .hero {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 40px;
            border-radius: 20px;
            margin-bottom: 40px;
            text-align: center;
        }

        .hero h1 {
            font-size: 42px;
            margin-bottom: 15px;
        }

        .hero p {
            font-size: 18px;
            opacity: 0.95;
        }

        /* Stats Cards */
        .stats {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 40px 0;
        }

        .stat-card {
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 12px;
            padding: 25px;
            text-align: center;
            transition: transform 0.2s, box-shadow 0.2s;
        }

        .stat-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
        }

        .stat-card h3 {
            font-size: 32px;
            color: #667eea;
            margin-bottom: 5px;
        }

        .stat-card p {
            color: #6c757d;
            font-size: 14px;
        }

        /* Model Cards */
        .models-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
            gap: 25px;
            margin-top: 40px;
        }

        .model-card {
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 15px;
            overflow: hidden;
            transition: transform 0.2s, box-shadow 0.2s;
            cursor: pointer;
        }

        .model-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 40px rgba(0, 0, 0, 0.1);
        }

        .model-card-image {
            height: 180px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 48px;
            color: white;
            position: relative;
            overflow: hidden;
        }

        .model-card-image img {
            width: 100%;
            height: 100%;
            object-fit: cover;
            position: absolute;
            top: 0;
            left: 0;
        }

        .model-card-content {
            padding: 20px;
        }

        .model-card h3 {
            color: #212529;
            margin-bottom: 10px;
            font-size: 20px;
        }

        .model-card p {
            color: #6c757d;
            margin-bottom: 15px;
            font-size: 14px;
            line-height: 1.5;
        }

        .model-links {
            display: flex;
            gap: 10px;
            margin-top: 15px;
        }

        .model-link {
            display: inline-flex;
            align-items: center;
            padding: 6px 12px;
            background: #f8f9fa;
            border-radius: 6px;
            color: #495057;
            text-decoration: none;
            font-size: 12px;
            transition: background 0.2s;
        }

        .model-link:hover {
            background: #e9ecef;
        }

        .model-link i {
            margin-right: 5px;
        }

        .model-status {
            display: inline-block;
            padding: 4px 10px;
            border-radius: 20px;
            font-size: 11px;
            font-weight: 600;
        }

        .status-new {
            background: #d4edda;
            color: #155724;
        }

        .status-current {
            background: #d1ecf1;
            color: #0c5460;
        }

        .status-research {
            background: #fff3cd;
            color: #856404;
        }

        /* Content Display */
        .markdown-content {
            display: none;
            background: white;
            padding: 40px;
            max-width: 900px;
            margin: 0 auto;
        }

        .markdown-content.active {
            display: block;
        }

        .markdown-content h1 {
            color: #212529;
            font-size: 36px;
            margin-bottom: 30px;
            padding-bottom: 15px;
            border-bottom: 2px solid #e9ecef;
        }

        .markdown-content h2 {
            color: #495057;
            font-size: 28px;
            margin-top: 35px;
            margin-bottom: 20px;
        }

        .markdown-content h3 {
            color: #495057;
            font-size: 22px;
            margin-top: 25px;
            margin-bottom: 15px;
        }

        .markdown-content p {
            color: #495057;
            line-height: 1.8;
            margin-bottom: 15px;
        }

        .markdown-content code {
            background: #f8f9fa;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'Monaco', 'Menlo', monospace;
            font-size: 14px;
            color: #d73a49;
        }

        .markdown-content pre {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            border: 1px solid #e9ecef;
        }

        .markdown-content blockquote {
            border-left: 4px solid #667eea;
            padding-left: 20px;
            margin: 20px 0;
            color: #6c757d;
            font-style: italic;
        }

        .markdown-content table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        .markdown-content th, .markdown-content td {
            padding: 12px;
            text-align: left;
            border: 1px solid #e9ecef;
        }

        .markdown-content th {
            background: #f8f9fa;
            font-weight: 600;
        }

        .markdown-content a {
            color: #667eea;
            text-decoration: none;
        }

        .markdown-content a:hover {
            text-decoration: underline;
        }

        .back-to-main {
            display: none;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 8px;
            cursor: pointer;
            margin-bottom: 20px;
            font-size: 14px;
        }

        .back-to-main:hover {
            opacity: 0.9;
        }

        /* GitHub Links Section */
        .github-links {
            background: #f8f9fa;
            border-radius: 12px;
            padding: 30px;
            margin: 40px 0;
        }

        .github-links h2 {
            color: #212529;
            margin-bottom: 20px;
        }

        .github-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
        }

        .github-card {
            background: white;
            padding: 15px;
            border-radius: 8px;
            border: 1px solid #e9ecef;
            transition: transform 0.2s;
        }

        .github-card:hover {
            transform: translateY(-3px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        }

        .github-card a {
            color: #212529;
            text-decoration: none;
            display: flex;
            align-items: center;
        }

        .github-card i {
            color: #667eea;
            margin-right: 10px;
            font-size: 20px;
        }

        .github-card .repo-name {
            font-weight: 600;
            font-size: 14px;
        }

        .github-card .repo-desc {
            font-size: 12px;
            color: #6c757d;
            margin-top: 5px;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .sidebar {
                width: 100%;
                position: relative;
                height: auto;
            }

            .content {
                margin-left: 0;
            }

            .models-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="sidebar">
            <div class="logo">
                <h1>ğŸ¤– AI Research Hub</h1>
                <p>DrawingBOMExtractor Documentation</p>
            </div>

            <div class="nav-section">
                <h3>Overview</h3>
                <a href="#" class="nav-item active" data-content="overview">
                    <span class="icon">ğŸ </span>
                    <span>Home</span>
                </a>
            </div>

            <div class="nav-section">
                <h3>Foundation Models</h3>
                <a href="#" class="nav-item" data-content="dino-x">
                    <span class="icon">ğŸ¯</span>
                    <span>DINO-X</span>
                </a>
                <a href="#" class="nav-item" data-content="sam2">
                    <span class="icon">âœ‚ï¸</span>
                    <span>SAM 2</span>
                </a>
            </div>

            <div class="nav-section">
                <h3>Graph Models</h3>
                <a href="#" class="nav-item" data-content="vectorgraphnet">
                    <span class="icon">ğŸ“</span>
                    <span>VectorGraphNET</span>
                </a>
                <a href="#" class="nav-item" data-content="gat-cadnet">
                    <span class="icon">ğŸ•¸ï¸</span>
                    <span>GAT-CADNet</span>
                </a>
            </div>

            <div class="nav-section">
                <h3>Real-Time Models</h3>
                <a href="#" class="nav-item" data-content="rt-detr">
                    <span class="icon">âš¡</span>
                    <span>RT-DETR v2</span>
                </a>
                <a href="#" class="nav-item" data-content="yolov11">
                    <span class="icon">ğŸ¯</span>
                    <span>YOLOv11</span>
                </a>
            </div>

            <div class="nav-section">
                <h3>Document Analysis</h3>
                <a href="#" class="nav-item" data-content="document-transformer">
                    <span class="icon">ğŸ“‹</span>
                    <span>Document Transformer</span>
                </a>
            </div>

            <div class="nav-section">
                <h3>Implementation</h3>
                <a href="#" class="nav-item" data-content="recommendations">
                    <span class="icon">ğŸ’¡</span>
                    <span>Implementation Guide</span>
                </a>
                <a href="#" class="nav-item" data-content="comparison">
                    <span class="icon">âš–ï¸</span>
                    <span>Model Comparison</span>
                </a>
            </div>
        </div>

        <div class="content">
            <button class="back-to-main">â† Back to Overview</button>

            <div class="main-content overview-content">
                <div class="hero">
                    <h1>ğŸš€ AI Models for CAD Symbol Detection</h1>
                    <p>Comprehensive Research on 2024-2025 State-of-the-Art Deep Learning Models</p>
                </div>

                <div class="stats">
                    <div class="stat-card">
                        <h3>8+</h3>
                        <p>Latest AI Models</p>
                    </div>
                    <div class="stat-card">
                        <h3>50+</h3>
                        <p>Research Papers</p>
                    </div>
                    <div class="stat-card">
                        <h3>15</h3>
                        <p>GitHub Repos</p>
                    </div>
                    <div class="stat-card">
                        <h3>93%</h3>
                        <p>Max Accuracy</p>
                    </div>
                </div>

                <div class="models-grid">
                    <div class="model-card" data-content="dino-x">
                        <div class="model-card-image">
                            <i class="fas fa-bullseye"></i>
                        </div>
                        <div class="model-card-content">
                            <h3>ğŸ¯ DINO-X</h3>
                            <span class="model-status status-new">â˜… Top Choice</span>
                            <p>World's top-performing open-world object detection model. Detect any object with text prompts, zero-shot capabilities.</p>
                            <div class="model-links">
                                <a href="https://github.com/IDEA-Research/DINO-X-API" target="_blank" class="model-link">
                                    <i class="fab fa-github"></i> GitHub
                                </a>
                                <a href="https://arxiv.org/abs/2411.14347" target="_blank" class="model-link">
                                    <i class="fas fa-file-alt"></i> Paper
                                </a>
                                <a href="https://deepdataspace.com/en/blog/7/" target="_blank" class="model-link">
                                    <i class="fas fa-globe"></i> Blog
                                </a>
                            </div>
                        </div>
                    </div>

                    <div class="model-card" data-content="sam2">
                        <div class="model-card-image">
                            <i class="fas fa-cut"></i>
                        </div>
                        <div class="model-card-content">
                            <h3>âœ‚ï¸ SAM 2</h3>
                            <span class="model-status status-new">Meta AI</span>
                            <p>Segment Anything Model 2 - First unified model for images and videos. 6x more accurate than SAM 1.0.</p>
                            <div class="model-links">
                                <a href="https://github.com/facebookresearch/sam2" target="_blank" class="model-link">
                                    <i class="fab fa-github"></i> GitHub
                                </a>
                                <a href="https://ai.meta.com/sam2/" target="_blank" class="model-link">
                                    <i class="fas fa-globe"></i> Official
                                </a>
                                <a href="https://sam2.metademolab.com/" target="_blank" class="model-link">
                                    <i class="fas fa-play"></i> Demo
                                </a>
                            </div>
                        </div>
                    </div>

                    <div class="model-card" data-content="vectorgraphnet">
                        <div class="model-card-image">
                            <i class="fas fa-vector-square"></i>
                        </div>
                        <div class="model-card-content">
                            <h3>ğŸ“ VectorGraphNET</h3>
                            <span class="model-status status-research">Research</span>
                            <p>Revolutionary PDF vector analysis. Converts CAD to graphs for native vector processing. 89% weighted F1 score.</p>
                            <div class="model-links">
                                <a href="https://arxiv.org/html/2410.01336v1" target="_blank" class="model-link">
                                    <i class="fas fa-file-alt"></i> Paper
                                </a>
                                <a href="https://floorplancad.github.io/" target="_blank" class="model-link">
                                    <i class="fas fa-database"></i> Dataset
                                </a>
                            </div>
                        </div>
                    </div>

                    <div class="model-card" data-content="gat-cadnet">
                        <div class="model-card-image">
                            <i class="fas fa-project-diagram"></i>
                        </div>
                        <div class="model-card-content">
                            <h3>ğŸ•¸ï¸ GAT-CADNet</h3>
                            <span class="model-status status-research">CVPR 2022</span>
                            <p>Graph Attention Network for CAD. Understands symbol relationships and spatial connections.</p>
                            <div class="model-links">
                                <a href="https://github.com/Liberation-happy/GAT-CADNet" target="_blank" class="model-link">
                                    <i class="fab fa-github"></i> GitHub
                                </a>
                                <a href="https://arxiv.org/abs/2201.00625" target="_blank" class="model-link">
                                    <i class="fas fa-file-alt"></i> Paper
                                </a>
                            </div>
                        </div>
                    </div>

                    <div class="model-card" data-content="rt-detr">
                        <div class="model-card-image">
                            <i class="fas fa-bolt"></i>
                        </div>
                        <div class="model-card-content">
                            <h3>âš¡ RT-DETR v2</h3>
                            <span class="model-status status-current">Baidu</span>
                            <p>Real-time Detection Transformer. NMS-free architecture beats YOLOs. 54.8 AP @ 74 FPS.</p>
                            <div class="model-links">
                                <a href="https://github.com/lyuwenyu/RT-DETR" target="_blank" class="model-link">
                                    <i class="fab fa-github"></i> GitHub
                                </a>
                                <a href="https://docs.ultralytics.com/models/rtdetr/" target="_blank" class="model-link">
                                    <i class="fas fa-book"></i> Docs
                                </a>
                            </div>
                        </div>
                    </div>

                    <div class="model-card" data-content="yolov11">
                        <div class="model-card-image">
                            <i class="fas fa-eye"></i>
                        </div>
                        <div class="model-card-content">
                            <h3>ğŸ¯ YOLOv11</h3>
                            <span class="model-status status-current">Ultralytics</span>
                            <p>Latest YOLO version with enhanced accuracy. Currently powering our system with proven stability.</p>
                            <div class="model-links">
                                <a href="https://github.com/ultralytics/ultralytics" target="_blank" class="model-link">
                                    <i class="fab fa-github"></i> GitHub
                                </a>
                                <a href="https://docs.ultralytics.com/models/yolo11/" target="_blank" class="model-link">
                                    <i class="fas fa-book"></i> Docs
                                </a>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="github-links">
                    <h2>ğŸ“š Essential Resources</h2>
                    <div class="github-grid">
                        <div class="github-card">
                            <a href="https://github.com/IDEA-Research/DINO-X-API" target="_blank">
                                <i class="fab fa-github"></i>
                                <div>
                                    <div class="repo-name">DINO-X-API</div>
                                    <div class="repo-desc">Open-world detection API</div>
                                </div>
                            </a>
                        </div>
                        <div class="github-card">
                            <a href="https://github.com/facebookresearch/sam2" target="_blank">
                                <i class="fab fa-github"></i>
                                <div>
                                    <div class="repo-name">SAM 2</div>
                                    <div class="repo-desc">Segment Anything Model 2</div>
                                </div>
                            </a>
                        </div>
                        <div class="github-card">
                            <a href="https://github.com/IDEA-Research/GroundingDINO" target="_blank">
                                <i class="fab fa-github"></i>
                                <div>
                                    <div class="repo-name">Grounding DINO</div>
                                    <div class="repo-desc">Open-set object detection</div>
                                </div>
                            </a>
                        </div>
                        <div class="github-card">
                            <a href="https://github.com/ultralytics/ultralytics" target="_blank">
                                <i class="fab fa-github"></i>
                                <div>
                                    <div class="repo-name">Ultralytics</div>
                                    <div class="repo-desc">YOLOv11 & RT-DETR</div>
                                </div>
                            </a>
                        </div>
                        <div class="github-card">
                            <a href="https://github.com/VITA-Group/CADTransformer" target="_blank">
                                <i class="fab fa-github"></i>
                                <div>
                                    <div class="repo-name">CADTransformer</div>
                                    <div class="repo-desc">Panoptic symbol spotting</div>
                                </div>
                            </a>
                        </div>
                        <div class="github-card">
                            <a href="https://github.com/PaddlePaddle/PaddleDetection" target="_blank">
                                <i class="fab fa-github"></i>
                                <div>
                                    <div class="repo-name">PaddleDetection</div>
                                    <div class="repo-desc">RT-DETR official impl</div>
                                </div>
                            </a>
                        </div>
                    </div>
                </div>
            </div>

            <div class="markdown-content" id="content-display">
                <!-- Dynamic content will be loaded here -->
            </div>
        </div>
    </div>

    <script>

        const overviewContent = document.querySelector('.overview-content');
        const contentDisplay = document.getElementById('content-display');
        const backButton = document.querySelector('.back-to-main');

        // Embedded markdown content
        const markdownContent = {};
                markdownContent['dino-x'] = "# ğŸ¯ DINO-X: Open-World ê°ì²´ ê²€ì¶œì˜ í˜ì‹ \\n\\n## ğŸ“‹ ê°œìš”\\n\\n**DINO-X**ëŠ” IDEA Researchì—ì„œ 2024ë…„ 11ì›”ì— ë°œí‘œí•œ ìµœì‹  Foundation Modelë¡œ, í˜„ì¬ **Open-world ê°ì²´ ê²€ì¶œ ë¶„ì•¼ì—ì„œ ì„¸ê³„ ìµœê³  ì„±ëŠ¥**ì„ ë‹¬ì„±í•œ ëª¨ë¸ì…ë‹ˆë‹¤. ê¸°ì¡´ YOLO ì‹œë¦¬ì¦ˆì™€ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ì ‘ê·¼ë²•ìœ¼ë¡œ **í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë§Œìœ¼ë¡œ ìƒˆë¡œìš´ ê°ì²´ë¥¼ ê²€ì¶œ**í•  ìˆ˜ ìˆëŠ” í˜ì‹ ì  ëŠ¥ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\\n\\n## ğŸš€ í•µì‹¬ íŠ¹ì§•\\n\\n### 1. **Multi-Modal í”„ë¡¬í”„íŠ¸ ì§€ì›**\\n- **í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸**: \\\"ë³€ì••ê¸° ì‹¬ë³¼ ì°¾ê¸°\\\"\\n- **ì‹œê°ì  í”„ë¡¬í”„íŠ¸**: ì˜ˆì‹œ ì´ë¯¸ì§€ ì œê³µ\\n- **ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸**: í…ìŠ¤íŠ¸ + ì‹œê°ì  ê²°í•©\\n\\n### 2. **Zero-Shot ê²€ì¶œ ëŠ¥ë ¥**\\n- ìƒˆë¡œìš´ ë¶€í’ˆ í´ë˜ìŠ¤ë¥¼ ì¬í•™ìŠµ ì—†ì´ ì¦‰ì‹œ ê²€ì¶œ\\n- ì„¤ëª…ë§Œìœ¼ë¡œ ë³µì¡í•œ ì‚°ì—… ë¶€í’ˆ ì¸ì‹ ê°€ëŠ¥\\n- ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ ì´í•´\\n\\n### 3. **ë‹¤ì¤‘ ì¶œë ¥ í˜•ì‹**\\n- **Bounding Box**: ê¸°ë³¸ ê°ì²´ ìœ„ì¹˜\\n- **Segmentation Mask**: ì •ë°€í•œ í”½ì…€ ë‹¨ìœ„ ë¶„í• \\n- **Object Caption**: ê²€ì¶œëœ ê°ì²´ ì„¤ëª… ìƒì„±\\n\\n## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ\\n\\n### **SOTA ì„±ëŠ¥ ë‹¬ì„±**\\n\\`\\`\\`\\nğŸ† COCO Dataset: 56.0 AP\\nğŸ† LVIS-minival: 59.8 AP\\nğŸ† LVIS-val: 52.4 AP\\nğŸ† Rare Classes: 63.3 AP (LVIS-minival)\\n\\`\\`\\`\\n\\n### **ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ**\\n| ëª¨ë¸ | COCO AP | Zero-Shot | í”„ë¡¬í”„íŠ¸ ì§€ì› |\\n|------|---------|-----------|---------------|\\n| **DINO-X Pro** | **56.0** | âœ… | âœ… |\\n| YOLOv11X | 54.7 | âŒ | âŒ |\\n| Detectron2 | 46.9 | âŒ | âŒ |\\n| Grounding DINO | 52.5 | âœ… | ì¼ë¶€ |\\n\\n## ğŸ—ï¸ ì•„í‚¤í…ì²˜\\n\\n### **í•µì‹¬ êµ¬ì„±ìš”ì†Œ**\\n\\n\\`\\`\\`mermaid\\ngraph TB\\n    A[Input Image] --> B[Vision Encoder]\\n    C[Text Prompt] --> D[Text Encoder]\\n    E[Visual Prompt] --> F[Visual Encoder]\\n\\n    B --> G[Fusion Module]\\n    D --> G\\n    F --> G\\n\\n    G --> H[Transformer Decoder]\\n    H --> I[Multi-Head Output]\\n\\n    I --> J[Bounding Boxes]\\n    I --> K[Segmentation Masks]\\n    I --> L[Object Captions]\\n\\`\\`\\`\\n\\n### **í˜ì‹ ì  ê¸°ìˆ **\\n1. **Unified Architecture**: ë‹¨ì¼ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ ì¶œë ¥ í˜•ì‹ ì§€ì›\\n2. **Grounding-100M Dataset**: 1ì–µê°œ ê³ í’ˆì§ˆ ê·¸ë¼ìš´ë”© ìƒ˜í”Œë¡œ í•™ìŠµ\\n3. **Two-Stage Training**:\\n   - Stage 1: í…ìŠ¤íŠ¸/ì‹œê°ì  í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ê²€ì¶œ\\n   - Stage 2: í‚¤í¬ì¸íŠ¸ ë° ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ í™•ì¥\\n\\n## ğŸ’» êµ¬í˜„ ì •ë³´\\n\\n### **ê³µì‹ GitHub ì €ì¥ì†Œ**\\n- **ì£¼ ì €ì¥ì†Œ**: [IDEA-Research/DINO-X-API](https://github.com/IDEA-Research/DINO-X-API)\\n- **MCP ì„œë²„**: [IDEA-Research/DINO-X-MCP](https://github.com/IDEA-Research/DINO-X-MCP)\\n\\n### **ê´€ë ¨ ì €ì¥ì†Œ**\\n- [IDEA-Research/DINO](https://github.com/IDEA-Research/DINO) - ì›ë³¸ DINO\\n- [IDEA-Research/GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) - Grounding DINO\\n- [IDEA-Research/MaskDINO](https://github.com/IDEA-Research/MaskDINO) - Mask DINO\\n\\n## ğŸ”§ ì‹¤ì œ ì ìš© ë°©ë²•\\n\\n### **1. API ì ‘ê·¼ ë°©ì‹**\\n\\`\\`\\`python\\n# DINO-X API ì‚¬ìš© ì˜ˆì œ\\nimport requests\\n\\n# API í† í° ì„¤ì • (ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹ ì²­)\\napi_token = \\\"your_api_token_here\\\"\\n\\n# í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ê²€ì¶œ\\nresponse = requests.post(\\n    \\\"https://api.deepdataspace.com/dino-x/detect\\\",\\n    headers={\\\"Authorization\\\": f\\\"Bearer {api_token}\\\"},\\n    json={\\n        \\\"image\\\": \\\"base64_encoded_image\\\",\\n        \\\"prompt\\\": \\\"electrical transformer symbols\\\",\\n        \\\"confidence\\\": 0.3\\n    }\\n)\\n\\ndetections = response.json()\\n\\`\\`\\`\\n\\n### **2. DrawingBOMExtractor í†µí•©**\\n\\`\\`\\`python\\n# real_ai_app.pyì— DINO-X ëª¨ë¸ ì¶”ê°€\\nclass DinoXDetector:\\n    def __init__(self, api_token):\\n        self.api_token = api_token\\n        self.base_url = \\\"https://api.deepdataspace.com/dino-x\\\"\\n\\n    def detect_with_prompt(self, image, text_prompt):\\n        \\\"\\\"\\\"í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ë¶€í’ˆ ê²€ì¶œ\\\"\\\"\\\"\\n        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©\\n        image_b64 = self.encode_image(image)\\n\\n        # API í˜¸ì¶œ\\n        response = self.call_api({\\n            \\\"image\\\": image_b64,\\n            \\\"prompt\\\": text_prompt,\\n            \\\"output_format\\\": [\\\"bbox\\\", \\\"mask\\\", \\\"caption\\\"]\\n        })\\n\\n        return self.parse_response(response)\\n\\n    def detect_similar_symbols(self, image, example_region):\\n        \\\"\\\"\\\"ì‹œê°ì  ì˜ˆì‹œë¡œ ìœ ì‚¬í•œ ì‹¬ë³¼ ì°¾ê¸°\\\"\\\"\\\"\\n        # êµ¬í˜„ ì½”ë“œ\\n        pass\\n\\`\\`\\`\\n\\n## ğŸ¯ CAD ì‹¬ë³¼ ê²€ì¶œ ì ìš© ì‹œë‚˜ë¦¬ì˜¤\\n\\n### **ì‹œë‚˜ë¦¬ì˜¤ 1: ìƒˆë¡œìš´ ë¶€í’ˆ í´ë˜ìŠ¤ ì¶”ê°€**\\n\\`\\`\\`python\\n# ê¸°ì¡´: 27ê°œ í´ë˜ìŠ¤ë¡œ ì œí•œ\\n# DINO-X: ë¬´ì œí•œ í´ë˜ìŠ¤ ì§€ì›\\n\\n# ì˜ˆì‹œ: ìƒˆë¡œìš´ ì„¼ì„œ íƒ€ì… ê²€ì¶œ\\nnew_detections = dino_x.detect_with_prompt(\\n    image=cad_drawing,\\n    prompt=\\\"pressure sensor symbols in electrical diagrams\\\"\\n)\\n\\`\\`\\`\\n\\n### **ì‹œë‚˜ë¦¬ì˜¤ 2: ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ ê²€ì¶œ**\\n\\`\\`\\`python\\n# í•œêµ­ì–´ ë¶€í’ˆëª…ìœ¼ë¡œ ê²€ì¶œ\\nkorean_detections = dino_x.detect_with_prompt(\\n    image=drawing,\\n    prompt=\\\"ë³€ì••ê¸°, ì°¨ë‹¨ê¸°, ê³„ì „ê¸° ì‹¬ë³¼\\\"\\n)\\n\\n# ë¸Œëœë“œë³„ ë¶€í’ˆ ê²€ì¶œ\\nbrand_specific = dino_x.detect_with_prompt(\\n    image=drawing,\\n    prompt=\\\"Siemens PLC symbols and control modules\\\"\\n)\\n\\`\\`\\`\\n\\n### **ì‹œë‚˜ë¦¬ì˜¤ 3: Voting ì‹œìŠ¤í…œ í†µí•©**\\n\\`\\`\\`python\\ndef enhanced_voting_with_dino_x():\\n    # ê¸°ì¡´ 4ê°œ ëª¨ë¸ ê²°ê³¼\\n    yolo_results = detect_with_yolo(image)\\n    detectron_results = detect_with_detectron2(image)\\n\\n    # DINO-X ê²°ê³¼ ì¶”ê°€\\n    dino_x_results = dino_x.detect_with_prompt(\\n        image=image,\\n        prompt=\\\"all electrical components and symbols\\\"\\n    )\\n\\n    # 5ëª¨ë¸ Votingìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ\\n    final_results = apply_voting([\\n        yolo_results, detectron_results, dino_x_results\\n    ])\\n\\n    return final_results\\n\\`\\`\\`\\n\\n## ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼\\n\\n### **ì •ëŸ‰ì  ê°œì„ **\\n- **ê²€ì¶œ ì •í™•ë„**: +20% í–¥ìƒ ì˜ˆìƒ\\n- **ìƒˆ ë¶€í’ˆ ëŒ€ì‘**: ì¬í•™ìŠµ ì—†ì´ ì¦‰ì‹œ ì ìš©\\n- **ì‚¬ìš©ì ê²½í—˜**: í…ìŠ¤íŠ¸ë¡œ ì›í•˜ëŠ” ë¶€í’ˆ ì§ì ‘ ê²€ìƒ‰\\n\\n### **ì •ì„±ì  ê°œì„ **\\n- **ìœ ì—°ì„±**: ê³ ì •ëœ í´ë˜ìŠ¤ì—ì„œ ë²—ì–´ë‚¨\\n- **í™•ì¥ì„±**: ìƒˆë¡œìš´ ë„ë©´ íƒ€ì…ì— ì‰½ê²Œ ì ì‘\\n- **ì§ê´€ì„±**: ìì—°ì–´ë¡œ ë¶€í’ˆ ê²€ìƒ‰ ê°€ëŠ¥\\n\\n## âš ï¸ ì œí•œì‚¬í•­ ë° ê³ ë ¤ì‚¬í•­\\n\\n### **í˜„ì¬ ì œí•œì‚¬í•­**\\n- **API ê¸°ë°˜**: ì¸í„°ë„· ì—°ê²° í•„ìš”\\n- **ë¹„ìš©**: API í˜¸ì¶œë‹¹ ê³¼ê¸ˆ (ë¬´ë£Œ í‹°ì–´ ì¡´ì¬)\\n- **ì‘ë‹µ ì‹œê°„**: ë¡œì»¬ ëª¨ë¸ ëŒ€ë¹„ ì•½ê°„ ëŠë¦¼\\n\\n### **í•´ê²° ë°©ì•ˆ**\\n\\`\\`\\`python\\n# í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•\\ndef hybrid_detection_strategy(image):\\n    # 1ì°¨: ë¹ ë¥¸ ë¡œì»¬ ëª¨ë¸ë¡œ ê¸°ë³¸ ê²€ì¶œ\\n    local_results = yolo_detect(image)\\n\\n    # 2ì°¨: ë¶ˆí™•ì‹¤í•œ ì˜ì—­ë§Œ DINO-Xë¡œ ì •ë°€ ê²€ì¶œ\\n    uncertain_regions = filter_low_confidence(local_results)\\n\\n    if uncertain_regions:\\n        dino_x_results = dino_x.detect_specific_regions(\\n            image, uncertain_regions\\n        )\\n        final_results = merge_results(local_results, dino_x_results)\\n    else:\\n        final_results = local_results\\n\\n    return final_results\\n\\`\\`\\`\\n\\n## ğŸ”® í–¥í›„ ê°œë°œ ë¡œë“œë§µ\\n\\n### **ë‹¨ê¸° ëª©í‘œ (1-2ê°œì›”)**\\n1. DINO-X API í†µí•© ë° í…ŒìŠ¤íŠ¸\\n2. ê¸°ì¡´ Voting ì‹œìŠ¤í…œì— 5ë²ˆì§¸ ëª¨ë¸ë¡œ ì¶”ê°€\\n3. í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ UI ê°œë°œ\\n\\n### **ì¤‘ê¸° ëª©í‘œ (3-6ê°œì›”)**\\n1. ì˜¨í”„ë ˆë¯¸ìŠ¤ ë°°í¬ ì˜µì…˜ ê²€í† \\n2. í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ ìµœì í™”\\n3. ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°œë°œ\\n\\n### **ì¥ê¸° ëª©í‘œ (6ê°œì›”+)**\\n1. DINO-X ê¸°ë°˜ ìë™ BOM ìƒì„± ê³ ë„í™”\\n2. ì‹¤ì‹œê°„ í˜‘ì—… ë„ë©´ ë¶„ì„ ì‹œìŠ¤í…œ\\n3. AI ì–´ì‹œìŠ¤í„´íŠ¸ ê¸°ëŠ¥ í†µí•©\\n\\n## ğŸ“š ì¶”ê°€ ìë£Œ\\n\\n### **ê³µì‹ ë¬¸ì„œ**\\n- [DINO-X ë…¼ë¬¸ (arXiv:2411.14347)](https://arxiv.org/abs/2411.14347)\\n- [DeepDataSpace ê³µì‹ ë¸”ë¡œê·¸](https://deepdataspace.com/en/blog/7/)\\n- [Hugging Face Papers](https://huggingface.co/papers/2411.14347)\\n\\n### **êµ¬í˜„ ì˜ˆì œ**\\n- [Papers with Code](https://paperswithcode.com/paper/dino-x-a-unified-vision-model-for-open-world)\\n- [Medium íŠœí† ë¦¬ì–¼](https://medium.com/@ideacvr2024/exploring-the-dino-family-part-3-dino-x-a-unified-vision-model-for-open-world-object-detection-d9c32de26f24)\\n\\n---\\n\\n**ê²°ë¡ **: DINO-XëŠ” CAD ì‹¬ë³¼ ê²€ì¶œ ë¶„ì•¼ì— íŒ¨ëŸ¬ë‹¤ì„ì˜ ë³€í™”ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” í˜ì‹ ì  ëª¨ë¸ì…ë‹ˆë‹¤. íŠ¹íˆ **í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•œ ìœ ì—°í•œ ê²€ì¶œ**ê³¼ **Zero-shot ëŠ¥ë ¥**ì€ ê¸°ì¡´ ì‹œìŠ¤í…œì˜ í•œê³„ë¥¼ ë›°ì–´ë„˜ëŠ” ì°¨ì„¸ëŒ€ ì†”ë£¨ì…˜ì„ ì œê³µí•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.";
        markdownContent['sam2'] = "# âœ‚ï¸ SAM 2: Segment Anything Model 2\\n\\n## ğŸ“‹ ê°œìš”\\n\\n**SAM 2 (Segment Anything Model 2)**ëŠ” Meta AI Researchì—ì„œ 2024ë…„ 8ì›”ì— ë°œí‘œí•œ ì°¨ì„¸ëŒ€ **ì„¸ë¶„í™”(Segmentation) Foundation Model**ì…ë‹ˆë‹¤. ì›ë³¸ SAM ëŒ€ë¹„ **6ë°° í–¥ìƒëœ ì •í™•ë„**ì™€ **ì‹¤ì‹œê°„ ì„±ëŠ¥**ì„ ìë‘í•˜ë©°, ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ë¥¼ í†µí•©ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜ì‹ ì  ëª¨ë¸ì…ë‹ˆë‹¤.\\n\\n## ğŸš€ í•µì‹¬ íŠ¹ì§•\\n\\n### 1. **í†µí•© ì„¸ë¶„í™” ì•„í‚¤í…ì²˜**\\n- **ì´ë¯¸ì§€ + ë¹„ë””ì˜¤**: ë‹¨ì¼ ëª¨ë¸ë¡œ ì •ì /ë™ì  ì½˜í…ì¸  ì²˜ë¦¬\\n- **Streaming Memory**: ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ í”„ë ˆì„ ì¶”ì \\n- **í”„ë¡¬í”„íŠ¸ ê¸°ë°˜**: ì , ë°•ìŠ¤, ë§ˆìŠ¤í¬ ë“± ë‹¤ì–‘í•œ ì…ë ¥ ì§€ì›\\n\\n### 2. **ì••ë„ì  ì„±ëŠ¥ í–¥ìƒ**\\n- **ì •í™•ë„**: ì›ë³¸ SAM ëŒ€ë¹„ 6ë°° í–¥ìƒ\\n- **ì†ë„**: ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ 50ms ì‘ë‹µì‹œê°„\\n- **ì—£ì§€ ì •í™•ë„**: ì‘ì€ ê°ì²´ê¹Œì§€ ì •ë°€ ë¶„í• \\n\\n### 3. **í”„ë¡¬í”„íŠ¸ íš¨ìœ¨ì„±**\\n- **ì ì€ í”„ë¡¬í”„íŠ¸**: ìµœì†Œí•œì˜ ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ ì •í™•í•œ ê²°ê³¼\\n- **ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©**: ë¸Œë¼ìš°ì €ì—ì„œ ì¦‰ì‹œ í”¼ë“œë°±\\n- **ìë™ ë§ˆìŠ¤í¬ ìƒì„±**: í”„ë¡¬í”„íŠ¸ ì—†ì´ë„ ì „ì²´ ê°ì²´ ë¶„í• \\n\\n## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ\\n\\n### **ë²¤ì¹˜ë§ˆí¬ ì„±ê³¼**\\n\\`\\`\\`\\nğŸ¯ ì—£ì§€ ì •í™•ë„: 6x ê°œì„  (vs SAM 1.0)\\nâš¡ ì‘ë‹µ ì‹œê°„: ~50ms (ì‹¤ì‹œê°„)\\nğŸ” ì‘ì€ ê°ì²´: ëŒ€í­ ê°œì„ ëœ ê²€ì¶œë¥ \\nğŸ“± ê²½ëŸ‰í™”: ì›¹/ëª¨ë°”ì¼ ìµœì í™”\\n\\`\\`\\`\\n\\n### **SA-V Dataset**\\n- **ìµœëŒ€ ê·œëª¨**: í˜„ì¡´ ìµœëŒ€ ë¹„ë””ì˜¤ ì„¸ë¶„í™” ë°ì´í„°ì…‹\\n- **Model-in-the-Loop**: ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ìœ¼ë¡œ ì§€ì† ê°œì„ \\n- **ë‹¤ì–‘í•œ ë„ë©”ì¸**: ê´‘ë²”ìœ„í•œ ì‹œê°ì  ë„ë©”ì¸ ì»¤ë²„\\n\\n## ğŸ—ï¸ ì•„í‚¤í…ì²˜\\n\\n### **í•µì‹¬ êµ¬ì„±ìš”ì†Œ**\\n\\n\\`\\`\\`mermaid\\ngraph TB\\n    A[Input Image/Video] --> B[Image Encoder]\\n    C[Prompt] --> D[Prompt Encoder]\\n\\n    B --> E[Memory Bank]\\n    D --> E\\n    E --> F[Mask Decoder]\\n\\n    F --> G[Segmentation Masks]\\n    E --> H[Streaming Memory]\\n    H --> I[Temporal Consistency]\\n\\`\\`\\`\\n\\n### **í˜ì‹ ì  ê¸°ìˆ **\\n\\n1. **Streaming Memory Design**\\n   - ë¹„ë””ì˜¤ í”„ë ˆì„ ê°„ ì¼ê´€ì„± ìœ ì§€\\n   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ê°ì²´ ì¶”ì \\n   - ì‹¤ì‹œê°„ ì²˜ë¦¬ ìµœì í™”\\n\\n2. **Unified Architecture**\\n   - ì´ë¯¸ì§€ë¥¼ ë‹¨ì¼ í”„ë ˆì„ ë¹„ë””ì˜¤ë¡œ ì²˜ë¦¬\\n   - ì½”ë“œ ì¤‘ë³µ ì—†ëŠ” íš¨ìœ¨ì  êµ¬ì¡°\\n   - í™•ì¥ ê°€ëŠ¥í•œ ë””ìì¸\\n\\n3. **Optimized Inference**\\n   - torch.compile ì§€ì›ìœ¼ë¡œ ëŒ€í­ ê°€ì†í™”\\n   - VOS (Video Object Segmentation) ìµœì í™”\\n   - ë©€í‹° ê°ì²´ ì¶”ì  ê°œì„ \\n\\n## ğŸ’» êµ¬í˜„ ì •ë³´\\n\\n### **ê³µì‹ GitHub ì €ì¥ì†Œ**\\n- **ì£¼ ì €ì¥ì†Œ**: [facebookresearch/sam2](https://github.com/facebookresearch/sam2)\\n- **ì›ë³¸ SAM**: [facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything)\\n- **Hugging Face**: [huggingface/segment-anything-2](https://github.com/huggingface/segment-anything-2)\\n\\n### **ì„¤ì¹˜ ë° ìš”êµ¬ì‚¬í•­**\\n\\`\\`\\`bash\\n# SAM 2 ì €ì¥ì†Œ í´ë¡ \\ngit clone https://github.com/facebookresearch/sam2.git\\ncd sam2\\n\\n# PyTorch ì„¤ì¹˜ (CUDA 12.1 ê¶Œì¥)\\npip install torch torchvision\\n\\n# SAM 2 ì„¤ì¹˜\\npip install -e .\\n\\n# ì„ íƒ: CUDA í™•ì¥ ì—†ì´ ì„¤ì¹˜\\nSAM2_BUILD_CUDA=0 pip install -e .\\n\\`\\`\\`\\n\\n## ğŸ”§ ì‹¤ì œ ì ìš© ë°©ë²•\\n\\n### **1. ê¸°ë³¸ ì´ë¯¸ì§€ ì„¸ë¶„í™”**\\n\\`\\`\\`python\\nimport torch\\nfrom sam2.build_sam import build_sam2\\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\\n\\n# ëª¨ë¸ ë¡œë“œ\\ncheckpoint = \\\"./checkpoints/sam2_hiera_large.pt\\\"\\nmodel_cfg = \\\"sam2_hiera_l.yaml\\\"\\npredictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))\\n\\n# ì´ë¯¸ì§€ ì„¤ì •\\npredictor.set_image(image)\\n\\n# ì  í”„ë¡¬í”„íŠ¸ë¡œ ë§ˆìŠ¤í¬ ìƒì„±\\ninput_point = np.array([[500, 375]])\\ninput_label = np.array([1])\\n\\nmasks, scores, _ = predictor.predict(\\n    point_coords=input_point,\\n    point_labels=input_label,\\n    multimask_output=True,\\n)\\n\\`\\`\\`\\n\\n### **2. ìë™ ë§ˆìŠ¤í¬ ìƒì„±**\\n\\`\\`\\`python\\nfrom sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\\n\\n# ìë™ ë§ˆìŠ¤í¬ ìƒì„±ê¸° ì´ˆê¸°í™”\\nmask_generator = SAM2AutomaticMaskGenerator(\\n    model=build_sam2(model_cfg, checkpoint),\\n    points_per_side=32,\\n    pred_iou_thresh=0.7,\\n    stability_score_thresh=0.92,\\n    crop_n_layers=1,\\n    crop_n_points_downscale_factor=2,\\n    min_mask_region_area=100,\\n)\\n\\n# ì „ì²´ ì´ë¯¸ì§€ì—ì„œ ë§ˆìŠ¤í¬ ìƒì„±\\nmasks = mask_generator.generate(image)\\n\\`\\`\\`\\n\\n### **3. ë¹„ë””ì˜¤ ê°ì²´ ì¶”ì **\\n\\`\\`\\`python\\nfrom sam2.build_sam import build_sam2_video_predictor\\n\\n# ë¹„ë””ì˜¤ ì˜ˆì¸¡ê¸° ì´ˆê¸°í™”\\npredictor = build_sam2_video_predictor(model_cfg, checkpoint)\\n\\n# ë¹„ë””ì˜¤ ì´ˆê¸°í™”\\ninference_state = predictor.init_state(video_path=\\\"./videos/\\\")\\n\\n# ì²« í”„ë ˆì„ì— í”„ë¡¬í”„íŠ¸ ì¶”ê°€\\nann_frame_idx = 0  # ì²« ë²ˆì§¸ í”„ë ˆì„\\nann_obj_id = 1     # ê°ì²´ ID\\n\\n# ì  í”„ë¡¬í”„íŠ¸ ì¶”ê°€\\n_, out_obj_ids, out_mask_logits = predictor.add_new_points(\\n    inference_state=inference_state,\\n    frame_idx=ann_frame_idx,\\n    obj_id=ann_obj_id,\\n    points=points,\\n    labels=labels,\\n)\\n\\n# ì „ì²´ ë¹„ë””ì˜¤ì— ë§ˆìŠ¤í¬ ì „íŒŒ\\nvideo_segments = {}\\nfor out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\\n    video_segments[out_frame_idx] = {\\n        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\\n        for i, out_obj_id in enumerate(out_obj_ids)\\n    }\\n\\`\\`\\`\\n\\n## ğŸ¯ CAD ì‹¬ë³¼ ê²€ì¶œ ì ìš© ì‹œë‚˜ë¦¬ì˜¤\\n\\n### **ì‹œë‚˜ë¦¬ì˜¤ 1: ì •ë°€í•œ ì‹¬ë³¼ ê²½ê³„ ì¶”ì¶œ**\\n\\`\\`\\`python\\nclass CADSymbolSegmenter:\\n    def __init__(self, sam2_checkpoint):\\n        self.predictor = SAM2ImagePredictor(\\n            build_sam2(\\\"sam2_hiera_l.yaml\\\", sam2_checkpoint)\\n        )\\n\\n    def extract_precise_symbols(self, cad_image, bounding_boxes):\\n        \\\"\\\"\\\"YOLO ê²€ì¶œ ê²°ê³¼ë¥¼ SAM 2ë¡œ ì •ë°€ ë¶„í• \\\"\\\"\\\"\\n        self.predictor.set_image(cad_image)\\n\\n        precise_masks = []\\n        for bbox in bounding_boxes:\\n            # ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©\\n            input_box = np.array([bbox[\\'x1\\'], bbox[\\'y1\\'], bbox[\\'x2\\'], bbox[\\'y2\\']])\\n\\n            masks, scores, _ = self.predictor.predict(\\n                box=input_box[None, :],\\n                multimask_output=False,\\n            )\\n\\n            precise_masks.append({\\n                \\'mask\\': masks[0],\\n                \\'score\\': scores[0],\\n                \\'class\\': bbox[\\'class\\'],\\n                \\'bbox\\': bbox\\n            })\\n\\n        return precise_masks\\n\\`\\`\\`\\n\\n### **ì‹œë‚˜ë¦¬ì˜¤ 2: ê²¹ì¹˜ëŠ” ì‹¬ë³¼ ë¶„ë¦¬**\\n\\`\\`\\`python\\ndef separate_overlapping_symbols(image, overlapping_region):\\n    \\\"\\\"\\\"ê²¹ì¹˜ëŠ” ì‹¬ë³¼ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ ë¶„ë¦¬\\\"\\\"\\\"\\n    # ì—¬ëŸ¬ ì  í”„ë¡¬í”„íŠ¸ë¡œ ê° ì‹¬ë³¼ ë¶„ë¦¬\\n    points = [\\n        [region[\\'center_x\\'], region[\\'center_y\\']]  # ê° ì‹¬ë³¼ì˜ ì¤‘ì‹¬ì \\n        for region in overlapping_region[\\'symbols\\']\\n    ]\\n    labels = [1] * len(points)  # ëª¨ë‘ foreground\\n\\n    predictor.set_image(image)\\n    masks, scores, _ = predictor.predict(\\n        point_coords=np.array(points),\\n        point_labels=np.array(labels),\\n        multimask_output=True,\\n    )\\n\\n    # ê°œë³„ ì‹¬ë³¼ë¡œ ë¶„ë¦¬ëœ ë§ˆìŠ¤í¬ ë°˜í™˜\\n    return separate_masks_by_connectivity(masks)\\n\\`\\`\\`\\n\\n### **ì‹œë‚˜ë¦¬ì˜¤ 3: DrawingBOMExtractor í†µí•©**\\n\\`\\`\\`python\\n# real_ai_app.pyì˜ ê²€ì¶œ íŒŒì´í”„ë¼ì¸ì— SAM 2 í†µí•©\\nclass EnhancedDetectionPipeline:\\n    def __init__(self):\\n        self.yolo_models = load_yolo_models()\\n        self.sam2_predictor = SAM2ImagePredictor(\\n            build_sam2(\\\"sam2_hiera_l.yaml\\\", \\\"checkpoints/sam2_hiera_large.pt\\\")\\n        )\\n\\n    def detect_and_segment(self, image):\\n        # 1ë‹¨ê³„: YOLOë¡œ ë¹ ë¥¸ ê²€ì¶œ\\n        detections = self.run_yolo_detection(image)\\n\\n        # 2ë‹¨ê³„: SAM 2ë¡œ ì •ë°€ ì„¸ë¶„í™”\\n        precise_segments = self.refine_with_sam2(image, detections)\\n\\n        # 3ë‹¨ê³„: Voting + ì„¸ë¶„í™” ê²°í•©\\n        final_results = self.apply_voting_with_masks(precise_segments)\\n\\n        return final_results\\n\\n    def refine_with_sam2(self, image, detections):\\n        \\\"\\\"\\\"YOLO ê²€ì¶œ ê²°ê³¼ë¥¼ SAM 2ë¡œ ì •êµí™”\\\"\\\"\\\"\\n        self.sam2_predictor.set_image(image)\\n\\n        refined_detections = []\\n        for detection in detections:\\n            # ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ SAM 2 í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜\\n            bbox_prompt = self.convert_to_sam2_box(detection[\\'bbox\\'])\\n\\n            # ì •ë°€ ë§ˆìŠ¤í¬ ìƒì„±\\n            masks, scores, _ = self.sam2_predictor.predict(\\n                box=bbox_prompt,\\n                multimask_output=False,\\n            )\\n\\n            # ì›ë³¸ ê²€ì¶œ ê²°ê³¼ì— ì •ë°€ ë§ˆìŠ¤í¬ ì¶”ê°€\\n            enhanced_detection = detection.copy()\\n            enhanced_detection[\\'precise_mask\\'] = masks[0]\\n            enhanced_detection[\\'mask_confidence\\'] = scores[0]\\n\\n            refined_detections.append(enhanced_detection)\\n\\n        return refined_detections\\n\\`\\`\\`\\n\\n## ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼\\n\\n### **ì •ëŸ‰ì  ê°œì„ **\\n- **ì„¸ë¶„í™” ì •í™•ë„**: í”½ì…€ ë‹¨ìœ„ ì •ë°€ë„ë¡œ 6ë°° í–¥ìƒ\\n- **ì¤‘ë³µ ì œê±°**: ê²¹ì¹˜ëŠ” ì‹¬ë³¼ì˜ ì •í™•í•œ ë¶„ë¦¬\\n- **BOM í’ˆì§ˆ**: ë” ì •í™•í•œ ì‹¬ë³¼ ê°œìˆ˜ ë° ìœ„ì¹˜\\n\\n### **ì •ì„±ì  ê°œì„ **\\n- **ì‹œê°ì  í’ˆì§ˆ**: ì •êµí•œ ì‹¬ë³¼ ê²½ê³„ í‘œì‹œ\\n- **ì‚¬ìš©ì ê²½í—˜**: ì§ê´€ì ì¸ ë§ˆìŠ¤í¬ í¸ì§‘ ì¸í„°í˜ì´ìŠ¤\\n- **ì‹ ë¢°ì„±**: ì• ë§¤í•œ ê²½ê³„ ì˜ì—­ì˜ ëª…í™•í•œ ë¶„ë¦¬\\n\\n## ğŸ¨ UI/UX ê°œì„  ì•„ì´ë””ì–´\\n\\n### **ëŒ€í™”í˜• ì‹¬ë³¼ í¸ì§‘**\\n\\`\\`\\`python\\n# Streamlitì—ì„œ SAM 2 ë§ˆìŠ¤í¬ í¸ì§‘ ê¸°ëŠ¥\\ndef interactive_symbol_editor():\\n    st.header(\\\"ğŸ¨ ì •ë°€ ì‹¬ë³¼ í¸ì§‘\\\")\\n\\n    # ì‚¬ìš©ìê°€ ì  í´ë¦­ìœ¼ë¡œ ì‹¬ë³¼ ì¶”ê°€/ì œê±°\\n    if st.button(\\\"â• ì‹¬ë³¼ ì¶”ê°€ ëª¨ë“œ\\\"):\\n        st.session_state.edit_mode = \\\"add\\\"\\n    if st.button(\\\"â– ì‹¬ë³¼ ì œê±° ëª¨ë“œ\\\"):\\n        st.session_state.edit_mode = \\\"remove\\\"\\n\\n    # í´ë¦­ëœ ì¢Œí‘œë¥¼ SAM 2 í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜\\n    if \\'clicked_points\\' in st.session_state:\\n        points = st.session_state.clicked_points\\n        labels = [1 if mode == \\\"add\\\" else 0\\n                 for mode in st.session_state.point_modes]\\n\\n        # SAM 2ë¡œ ë§ˆìŠ¤í¬ ì—…ë°ì´íŠ¸\\n        new_masks = sam2_predictor.predict(\\n            point_coords=np.array(points),\\n            point_labels=np.array(labels),\\n            multimask_output=False,\\n        )\\n\\n        # ì—…ë°ì´íŠ¸ëœ ë§ˆìŠ¤í¬ í‘œì‹œ\\n        display_updated_masks(new_masks)\\n\\`\\`\\`\\n\\n## âš ï¸ ì œí•œì‚¬í•­ ë° í•´ê²°ì±…\\n\\n### **í˜„ì¬ ì œí•œì‚¬í•­**\\n1. **ê³„ì‚° ë¹„ìš©**: ê³ í•´ìƒë„ ì´ë¯¸ì§€ì—ì„œ ëŠë¦° ì²˜ë¦¬\\n2. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡±\\n3. **ëª¨ë¸ í¬ê¸°**: ë¡œì»¬ ë°°í¬ ì‹œ ìš©ëŸ‰ ê³ ë ¤ í•„ìš”\\n\\n### **ìµœì í™” ì „ëµ**\\n\\`\\`\\`python\\n# íš¨ìœ¨ì ì¸ SAM 2 ì‚¬ìš©ë²•\\ndef optimized_sam2_processing(image, detections):\\n    # ì´ë¯¸ì§€ í•´ìƒë„ ìµœì í™”\\n    if image.shape[0] > 1080 or image.shape[1] > 1080:\\n        image = resize_image_smart(image, max_size=1080)\\n\\n    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± í–¥ìƒ\\n    batch_size = 8\\n    for i in range(0, len(detections), batch_size):\\n        batch = detections[i:i+batch_size]\\n        process_detection_batch(image, batch)\\n\\n    # torch.compile í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ\\n    if hasattr(predictor.model, \\'compile\\'):\\n        predictor.model = torch.compile(predictor.model)\\n\\n    return processed_results\\n\\`\\`\\`\\n\\n## ğŸ”® SAM 2.1 ë° í–¥í›„ ë°œì „\\n\\n### **SAM 2.1 ê°œì„ ì‚¬í•­** (2024ë…„ 9ì›”)\\n- **ìƒˆë¡œìš´ ì²´í¬í¬ì¸íŠ¸**: í–¥ìƒëœ ëª¨ë¸ ê°€ì¤‘ì¹˜\\n- **í›ˆë ¨ ì½”ë“œ**: ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ íŒŒì¸íŠœë‹ ì§€ì›\\n- **ì›¹ ë°ëª¨**: ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥\\n\\n### **ë¯¸ë˜ ë°œì „ ë°©í–¥**\\n1. **ì‹¤ì‹œê°„ í˜‘ì—…**: ì—¬ëŸ¬ ì‚¬ìš©ìê°€ ë™ì‹œì— ë§ˆìŠ¤í¬ í¸ì§‘\\n2. **3D í™•ì¥**: 3D CAD ëª¨ë¸ì˜ ì„¸ë¶„í™” ì§€ì›\\n3. **ëª¨ë°”ì¼ ìµœì í™”**: íƒœë¸”ë¦¿ì—ì„œì˜ í„°ì¹˜ ê¸°ë°˜ í¸ì§‘\\n\\n## ğŸ“š ì¶”ê°€ ìë£Œ\\n\\n### **ê³µì‹ ë¬¸ì„œ**\\n- [SAM 2 ë…¼ë¬¸ (arXiv:2408.00714)](https://arxiv.org/abs/2408.00714)\\n- [Meta AI ê³µì‹ ë¸”ë¡œê·¸](https://ai.meta.com/blog/segment-anything-2/)\\n- [GitHub ê³µì‹ ë¬¸ì„œ](https://github.com/facebookresearch/sam2/blob/main/README.md)\\n\\n### **íŠœí† ë¦¬ì–¼ ë° ì˜ˆì œ**\\n- [Google Colab ë…¸íŠ¸ë¶](https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/automatic_mask_generator_example.ipynb)\\n- [LearnOpenCV íŠœí† ë¦¬ì–¼](https://learnopencv.com/sam-2/)\\n- [Modal ë°°í¬ ê°€ì´ë“œ](https://modal.com/docs/examples/segment_anything)\\n\\n---\\n\\n**ê²°ë¡ **: SAM 2ëŠ” ê¸°ì¡´ ë°”ìš´ë”© ë°•ìŠ¤ ê¸°ë°˜ ê²€ì¶œì˜ í•œê³„ë¥¼ ë›°ì–´ë„˜ì–´ **í”½ì…€ ìˆ˜ì¤€ì˜ ì •ë°€í•œ ì‹¬ë³¼ ë¶„í• **ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. DrawingBOMExtractorì— í†µí•©í•˜ë©´ **ë” ì •í™•í•œ BOM ìƒì„±**ê³¼ **ì§ê´€ì ì¸ ì‚¬ìš©ì ê²½í—˜**ì„ ì œê³µí•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.";
        markdownContent['vectorgraphnet'] = "# ğŸ“ VectorGraphNET: PDF ë²¡í„° ê·¸ë˜í”„ ë¶„ì„ì˜ í˜ì‹ \\n\\n## ğŸ“‹ ê°œìš”\\n\\n**VectorGraphNET**ì€ 2024ë…„ 10ì›”ì— ë°œí‘œëœ í˜ì‹ ì ì¸ ëª¨ë¸ë¡œ, **PDF ê¸°ìˆ  ë„ë©´ì„ SVG í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•œ í›„ ê·¸ë˜í”„ êµ¬ì¡°ë¡œ ë¶„ì„**í•˜ëŠ” ì™„ì „íˆ ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë˜ìŠ¤í„° ì´ë¯¸ì§€ ê¸°ë°˜ ë¶„ì„ê³¼ëŠ” ë‹¬ë¦¬, **ë²¡í„° ë°ì´í„°ë¥¼ ì§ì ‘ ì²˜ë¦¬**í•˜ì—¬ CAD ë„ë©´ì— ìµœì í™”ëœ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.\\n\\n## ğŸš€ í•µì‹¬ íŠ¹ì§•\\n\\n### 1. **ë²¡í„° ë„¤ì´í‹°ë¸Œ ì²˜ë¦¬**\\n- **PDF â†’ SVG**: ì›ë³¸ ë²¡í„° í’ˆì§ˆ ë³´ì¡´\\n- **ê¸°í•˜í•™ì  ê´€ê³„**: ë¼ì¸ ê°„ ì—°ê²°ì„± ë¶„ì„\\n- **ì†ì„± ë³´ì¡´**: ìŠ¤íŠ¸ë¡œí¬ ë‘ê»˜, ìƒ‰ìƒ, ìŠ¤íƒ€ì¼ ì •ë³´ ìœ ì§€\\n\\n### 2. **ê·¸ë˜í”„ ì–´í…ì…˜ ë„¤íŠ¸ì›Œí¬**\\n- **Graph Attention v2**: ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì´ì›ƒì— ì„ íƒì  ì§‘ì¤‘\\n- **ê³„ì¸µì  ë¶„ë¥˜**: 3ë‹¨ê³„ ë ˆë²¨ì˜ ì„¸ë°€í•œ ë¶„ë¥˜ ì²´ê³„\\n- **ê²½ëŸ‰ ì•„í‚¤í…ì²˜**: ë‹¨ 1.3M íŒŒë¼ë¯¸í„°ë¡œ íš¨ìœ¨ì  ë™ì‘\\n\\n### 3. **í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°**\\n- **Weighted F1**: 89.0% ë‹¬ì„±ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥\\n- **í¬ì†Œ í´ë˜ìŠ¤**: ì˜ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” ìš”ì†Œë„ ì •í™•íˆ ì¸ì‹\\n- **ì‹¤ìš©ì  ì„±ëŠ¥**: ì‹¤ì œ ê±´ì¶•/ê³µí•™ ë„ë©´ì—ì„œ ê²€ì¦\\n\\n## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ\\n\\n### **FloorplanCAD ë²¤ì¹˜ë§ˆí¬**\\n\\`\\`\\`\\nğŸ† Weighted F1 Score: 89.0% (SOTA)\\nğŸ¯ ì¼ë°˜ F1 Score: 79.4%\\nâš¡ ëª¨ë¸ í¬ê¸°: 1.3M íŒŒë¼ë¯¸í„°\\nğŸ“Š í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘: ìµœìš°ìˆ˜\\n\\`\\`\\`\\n\\n### **ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ**\\n| ëª¨ë¸ | F1 Score | wF1 Score | íŒŒë¼ë¯¸í„° ìˆ˜ |\\n|------|----------|-----------|-------------|\\n| **VectorGraphNET** | 79.4% | **89.0%** | **1.3M** |\\n| SymPoint | **86.8%** | 85.2% | 35M |\\n| PanCADNet | 82.1% | 81.5% | 42M |\\n| CADTransformer | 85.3% | 84.8% | 65M |\\n| PointTâ€¡Cluster | 78.9% | 78.1% | 31M |\\n\\n## ğŸ—ï¸ ì•„í‚¤í…ì²˜\\n\\n### **ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**\\n\\n\\`\\`\\`mermaid\\ngraph TB\\n    A[PDF Technical Drawing] --> B[PDF-to-SVG Conversion]\\n    B --> C[Geometry Extraction]\\n    C --> D[Graph Construction]\\n\\n    D --> E[Node Features]\\n    D --> F[Edge Relationships]\\n\\n    E --> G[Graph Attention v2]\\n    F --> G\\n\\n    G --> H[Hierarchical Classification]\\n    H --> I[Level 1: Primary Category]\\n    H --> J[Level 2: Semantic Group]\\n    H --> K[Level 3: Detailed Element]\\n\\`\\`\\`\\n\\n### **ê·¸ë˜í”„ êµ¬ì¡°**\\n\\n\\`\\`\\`python\\n# ê·¸ë˜í”„ ë…¸ë“œ êµ¬ì„±\\nnode_features = {\\n    \\'geometric\\': {\\n        \\'length\\': path_length,\\n        \\'curvature\\': path_curvature,\\n        \\'orientation\\': line_angle,\\n        \\'position\\': [x, y]\\n    },\\n    \\'stylistic\\': {\\n        \\'stroke_width\\': line_width,\\n        \\'stroke_color\\': color_rgb,\\n        \\'fill_color\\': fill_rgb,\\n        \\'dash_pattern\\': dash_style\\n    },\\n    \\'svg_attributes\\': {\\n        \\'path_commands\\': [\\'M\\', \\'L\\', \\'C\\', \\'Z\\'],\\n        \\'transform_matrix\\': transformation,\\n        \\'layer_info\\': layer_id\\n    }\\n}\\n\\n# ê·¸ë˜í”„ ì—£ì§€ êµ¬ì„±\\nedge_relationships = {\\n    \\'geometric_proximity\\': distance < threshold,\\n    \\'connection_point\\': shared_endpoint,\\n    \\'parallel_lines\\': angle_similarity,\\n    \\'perpendicular\\': orthogonal_relationship\\n}\\n\\`\\`\\`\\n\\n## ğŸ’» êµ¬í˜„ ì •ë³´\\n\\n### **ë…¼ë¬¸ ë° ìë£Œ**\\n- **ë…¼ë¬¸**: [VectorGraphNET (arXiv:2410.01336v1)](https://arxiv.org/html/2410.01336v1)\\n- **ë°ì´í„°ì…‹**: [FloorplanCAD](https://floorplancad.github.io/)\\n- **ê´€ë ¨ ì—°êµ¬**: CAD ë²¡í„° ê·¸ë˜í”„ ë¶„ì„ ë¶„ì•¼\\n\\n### **êµ¬í˜„ ìƒíƒœ**\\n> âš ï¸ **ì£¼ì˜**: í˜„ì¬ ê³µì‹ GitHub ì €ì¥ì†ŒëŠ” ê³µê°œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (2024ë…„ 10ì›” ë…¼ë¬¸).\\n> êµ¬í˜„ì„ ìœ„í•´ì„œëŠ” ë…¼ë¬¸ì˜ ë°©ë²•ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ ìì²´ ê°œë°œì´ í•„ìš”í•©ë‹ˆë‹¤.\\n\\n## ğŸ”§ ìì²´ êµ¬í˜„ ë°©ë²•\\n\\n### **1. PDF to SVG ë³€í™˜**\\n\\`\\`\\`python\\nimport fitz  # PyMuPDF\\nfrom xml.etree import ElementTree as ET\\n\\nclass PDFToSVGConverter:\\n    def __init__(self):\\n        self.svg_namespace = \\\"http://www.w3.org/2000/svg\\\"\\n\\n    def convert_pdf_to_svg(self, pdf_path):\\n        \\\"\\\"\\\"PDFë¥¼ SVGë¡œ ë³€í™˜í•˜ê³  ë²¡í„° ì •ë³´ ì¶”ì¶œ\\\"\\\"\\\"\\n        doc = fitz.open(pdf_path)\\n\\n        svg_elements = []\\n        for page_num in range(doc.page_count):\\n            page = doc[page_num]\\n\\n            # ë²¡í„° ê²½ë¡œ ì¶”ì¶œ\\n            paths = page.get_drawings()\\n            for path in paths:\\n                svg_path = self.convert_path_to_svg(path)\\n                svg_elements.append(svg_path)\\n\\n        return self.create_svg_document(svg_elements)\\n\\n    def convert_path_to_svg(self, fitz_path):\\n        \\\"\\\"\\\"PyMuPDF ê²½ë¡œë¥¼ SVG ê²½ë¡œë¡œ ë³€í™˜\\\"\\\"\\\"\\n        svg_commands = []\\n\\n        for item in fitz_path[\\\"items\\\"]:\\n            if item[0] == \\\"l\\\":  # line\\n                x1, y1, x2, y2 = item[1:]\\n                svg_commands.append(f\\\"M {x1} {y1} L {x2} {y2}\\\")\\n            elif item[0] == \\\"c\\\":  # curve\\n                # ë² ì§€ì–´ ê³¡ì„  ë³€í™˜\\n                points = item[1:]\\n                svg_commands.append(f\\\"C {\\' \\'.join(map(str, points))}\\\")\\n\\n        return {\\n            \\'path\\': \\' \\'.join(svg_commands),\\n            \\'stroke\\': fitz_path.get(\\'stroke\\', \\'#000000\\'),\\n            \\'stroke_width\\': fitz_path.get(\\'width\\', 1),\\n            \\'fill\\': fitz_path.get(\\'fill\\', \\'none\\')\\n        }\\n\\`\\`\\`\\n\\n### **2. ê·¸ë˜í”„ êµ¬ì„±**\\n\\`\\`\\`python\\nimport networkx as nx\\nimport numpy as np\\nfrom shapely.geometry import LineString, Point\\n\\nclass CADGraphBuilder:\\n    def __init__(self):\\n        self.graph = nx.Graph()\\n        self.proximity_threshold = 5.0  # í”½ì…€ ë‹¨ìœ„\\n\\n    def build_graph_from_svg(self, svg_elements):\\n        \\\"\\\"\\\"SVG ìš”ì†Œë“¤ë¡œë¶€í„° ê·¸ë˜í”„ êµ¬ì„±\\\"\\\"\\\"\\n        nodes = []\\n\\n        # ê° SVG ê²½ë¡œë¥¼ ë…¸ë“œë¡œ ë³€í™˜\\n        for idx, element in enumerate(svg_elements):\\n            node_features = self.extract_node_features(element)\\n            nodes.append((idx, node_features))\\n            self.graph.add_node(idx, **node_features)\\n\\n        # ë…¸ë“œ ê°„ ê´€ê³„ë¥¼ ì—£ì§€ë¡œ êµ¬ì„±\\n        self.create_edges(nodes)\\n\\n        return self.graph\\n\\n    def extract_node_features(self, svg_element):\\n        \\\"\\\"\\\"SVG ìš”ì†Œì—ì„œ ë…¸ë“œ íŠ¹ì§• ì¶”ì¶œ\\\"\\\"\\\"\\n        path_data = svg_element[\\'path\\']\\n\\n        # ê¸°í•˜í•™ì  íŠ¹ì§•\\n        geometry = self.parse_svg_path(path_data)\\n        length = self.calculate_path_length(geometry)\\n        curvature = self.calculate_curvature(geometry)\\n\\n        # ìŠ¤íƒ€ì¼ íŠ¹ì§•\\n        stroke_width = float(svg_element.get(\\'stroke_width\\', 1))\\n        color_rgb = self.parse_color(svg_element.get(\\'stroke\\', \\'#000000\\'))\\n\\n        return {\\n            \\'length\\': length,\\n            \\'curvature\\': curvature,\\n            \\'stroke_width\\': stroke_width,\\n            \\'color\\': color_rgb,\\n            \\'geometry\\': geometry\\n        }\\n\\n    def create_edges(self, nodes):\\n        \\\"\\\"\\\"ë…¸ë“œ ê°„ ê´€ê³„ ê¸°ë°˜ ì—£ì§€ ìƒì„±\\\"\\\"\\\"\\n        for i, (node_i, features_i) in enumerate(nodes):\\n            for j, (node_j, features_j) in enumerate(nodes[i+1:], i+1):\\n\\n                # ê¸°í•˜í•™ì  ê·¼ì ‘ì„± ê²€ì‚¬\\n                if self.is_geometrically_close(features_i, features_j):\\n                    edge_weight = self.calculate_relationship_strength(\\n                        features_i, features_j\\n                    )\\n                    self.graph.add_edge(node_i, node_j, weight=edge_weight)\\n\\`\\`\\`\\n\\n### **3. ê·¸ë˜í”„ ì–´í…ì…˜ ë„¤íŠ¸ì›Œí¬**\\n\\`\\`\\`python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nfrom torch_geometric.nn import GATv2Conv\\n\\nclass VectorGraphNET(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, num_classes, num_heads=8):\\n        super(VectorGraphNET, self).__init__()\\n\\n        # Graph Attention v2 ë ˆì´ì–´ë“¤\\n        self.gat1 = GATv2Conv(\\n            input_dim, hidden_dim,\\n            heads=num_heads, dropout=0.1\\n        )\\n        self.gat2 = GATv2Conv(\\n            hidden_dim * num_heads, hidden_dim,\\n            heads=1, dropout=0.1\\n        )\\n\\n        # ê³„ì¸µì  ë¶„ë¥˜ê¸°\\n        self.classifier_l1 = nn.Linear(hidden_dim, num_classes[\\'level1\\'])\\n        self.classifier_l2 = nn.Linear(hidden_dim, num_classes[\\'level2\\'])\\n        self.classifier_l3 = nn.Linear(hidden_dim, num_classes[\\'level3\\'])\\n\\n        self.dropout = nn.Dropout(0.1)\\n\\n    def forward(self, x, edge_index, batch=None):\\n        # ì²« ë²ˆì§¸ GAT ë ˆì´ì–´\\n        x = self.gat1(x, edge_index)\\n        x = F.elu(x)\\n        x = self.dropout(x)\\n\\n        # ë‘ ë²ˆì§¸ GAT ë ˆì´ì–´\\n        x = self.gat2(x, edge_index)\\n        x = F.elu(x)\\n        x = self.dropout(x)\\n\\n        # ê³„ì¸µì  ë¶„ë¥˜\\n        out_l1 = self.classifier_l1(x)\\n        out_l2 = self.classifier_l2(x)\\n        out_l3 = self.classifier_l3(x)\\n\\n        return {\\n            \\'level1\\': out_l1,\\n            \\'level2\\': out_l2,\\n            \\'level3\\': out_l3\\n        }\\n\\`\\`\\`\\n\\n## ğŸ¯ CAD ì‹¬ë³¼ ê²€ì¶œ ì ìš©\\n\\n### **DrawingBOMExtractor í†µí•© ì‹œë‚˜ë¦¬ì˜¤**\\n\\n\\`\\`\\`python\\nclass VectorGraphCADAnalyzer:\\n    def __init__(self, model_path):\\n        self.pdf_converter = PDFToSVGConverter()\\n        self.graph_builder = CADGraphBuilder()\\n        self.model = self.load_trained_model(model_path)\\n\\n    def analyze_cad_drawing(self, pdf_path):\\n        \\\"\\\"\\\"PDF CAD ë„ë©´ì„ ë²¡í„° ê·¸ë˜í”„ë¡œ ë¶„ì„\\\"\\\"\\\"\\n\\n        # 1ë‹¨ê³„: PDF â†’ SVG ë³€í™˜\\n        svg_elements = self.pdf_converter.convert_pdf_to_svg(pdf_path)\\n\\n        # 2ë‹¨ê³„: ê·¸ë˜í”„ êµ¬ì„±\\n        cad_graph = self.graph_builder.build_graph_from_svg(svg_elements)\\n\\n        # 3ë‹¨ê³„: ê·¸ë˜í”„ ì–´í…ì…˜ ë„¤íŠ¸ì›Œí¬ ì ìš©\\n        predictions = self.model(cad_graph)\\n\\n        # 4ë‹¨ê³„: ê²°ê³¼ í•´ì„\\n        detected_symbols = self.interpret_predictions(predictions, svg_elements)\\n\\n        return detected_symbols\\n\\n    def interpret_predictions(self, predictions, svg_elements):\\n        \\\"\\\"\\\"ëª¨ë¸ ì˜ˆì¸¡ì„ ì‹¬ë³¼ ê²€ì¶œ ê²°ê³¼ë¡œ ë³€í™˜\\\"\\\"\\\"\\n        detected_symbols = []\\n\\n        for idx, element in enumerate(svg_elements):\\n            # ê³„ì¸µì  ë¶„ë¥˜ ê²°ê³¼ ê²°í•©\\n            l1_pred = predictions[\\'level1\\'][idx].argmax()\\n            l2_pred = predictions[\\'level2\\'][idx].argmax()\\n            l3_pred = predictions[\\'level3\\'][idx].argmax()\\n\\n            # ì‹¬ë³¼ í´ë˜ìŠ¤ ë§¤í•‘\\n            symbol_class = self.map_hierarchical_to_symbol(\\n                l1_pred, l2_pred, l3_pred\\n            )\\n\\n            if symbol_class != \\'background\\':\\n                # SVG ê²½ë¡œë¥¼ ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ë³€í™˜\\n                bbox = self.svg_path_to_bbox(element[\\'path\\'])\\n\\n                detected_symbols.append({\\n                    \\'class_name\\': symbol_class,\\n                    \\'confidence\\': predictions[\\'level3\\'][idx].max().item(),\\n                    \\'bbox\\': bbox,\\n                    \\'vector_path\\': element[\\'path\\'],\\n                    \\'style_info\\': {\\n                        \\'stroke_width\\': element[\\'stroke_width\\'],\\n                        \\'color\\': element[\\'stroke\\']\\n                    }\\n                })\\n\\n        return detected_symbols\\n\\`\\`\\`\\n\\n## ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼\\n\\n### **ë²¡í„° ë°ì´í„° í™œìš©ì˜ ì¥ì **\\n1. **í•´ìƒë„ ë…ë¦½ì„±**: í™•ëŒ€/ì¶•ì†Œí•´ë„ í’ˆì§ˆ ì†ì‹¤ ì—†ìŒ\\n2. **ì •í™•í•œ ì¹˜ìˆ˜**: í”½ì…€ ê·¼ì‚¬ ëŒ€ì‹  ì •í™•í•œ ê¸°í•˜í•™ì  ì •ë³´\\n3. **ìŠ¤íƒ€ì¼ ì •ë³´**: ë¼ì¸ íƒ€ì…, ìƒ‰ìƒ, ë‘ê»˜ ë“± ë©”íƒ€ë°ì´í„° í™œìš©\\n4. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ë³µì¡í•œ ë„ë©´ë„ ì ì€ ë©”ëª¨ë¦¬ë¡œ ì²˜ë¦¬\\n\\n### **CAD ë„ë©´ íŠ¹í™” ì´ì **\\n\\`\\`\\`python\\n# ë²¡í„° ê¸°ë°˜ ë¶„ì„ì˜ ì‹¤ì œ í™œìš© ì˜ˆ\\ndef analyze_electrical_schematic(svg_graph):\\n    \\\"\\\"\\\"ì „ê¸° íšŒë¡œë„ íŠ¹í™” ë¶„ì„\\\"\\\"\\\"\\n\\n    # ì—°ê²°ì„  ì¶”ì \\n    wire_connections = find_connected_components(\\n        svg_graph,\\n        node_type=\\'wire\\'\\n    )\\n\\n    # ì‹¬ë³¼ ê°„ ì „ê¸°ì  ì—°ê²° ê´€ê³„\\n    electrical_connections = []\\n    for wire in wire_connections:\\n        connected_symbols = find_connected_symbols(wire)\\n        electrical_connections.append({\\n            \\'wire\\': wire,\\n            \\'symbols\\': connected_symbols,\\n            \\'connection_type\\': \\'electrical\\'\\n        })\\n\\n    # BOMì— ì—°ê²° ì •ë³´ í¬í•¨\\n    enhanced_bom = generate_bom_with_connections(\\n        detected_symbols, electrical_connections\\n    )\\n\\n    return enhanced_bom\\n\\`\\`\\`\\n\\n## âš ï¸ êµ¬í˜„ ë„ì „ê³¼ì œ\\n\\n### **ê¸°ìˆ ì  ê³¼ì œ**\\n1. **PDF íŒŒì‹±**: ë³µì¡í•œ PDF êµ¬ì¡°ì˜ ì •í™•í•œ ë²¡í„° ì¶”ì¶œ\\n2. **ê·¸ë˜í”„ ìŠ¤ì¼€ì¼ë§**: ëŒ€ê·œëª¨ ë„ë©´ì˜ íš¨ìœ¨ì  ì²˜ë¦¬\\n3. **ë„ë©”ì¸ ì ì‘**: FloorplanCADì—ì„œ ì‚°ì—… ë„ë©´ìœ¼ë¡œì˜ ì „ì´\\n\\n### **í•´ê²° ë°©ì•ˆ**\\n\\`\\`\\`python\\n# íš¨ìœ¨ì ì¸ ëŒ€ê·œëª¨ ê·¸ë˜í”„ ì²˜ë¦¬\\nclass ScalableVectorGraphNET:\\n    def __init__(self):\\n        self.max_nodes_per_batch = 1000\\n        self.clustering_algorithm = \\\"leiden\\\"\\n\\n    def process_large_drawing(self, large_graph):\\n        \\\"\\\"\\\"ëŒ€ê·œëª¨ ë„ë©´ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬\\\"\\\"\\\"\\n\\n        # 1. ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì§€ì—­ ë¶„í• \\n        clusters = self.cluster_graph(large_graph)\\n\\n        # 2. ê° í´ëŸ¬ìŠ¤í„° ë…ë¦½ ì²˜ë¦¬\\n        cluster_results = []\\n        for cluster in clusters:\\n            if len(cluster.nodes) <= self.max_nodes_per_batch:\\n                result = self.model(cluster)\\n                cluster_results.append(result)\\n            else:\\n                # ì¬ê·€ì  ë¶„í• \\n                sub_results = self.process_large_drawing(cluster)\\n                cluster_results.extend(sub_results)\\n\\n        # 3. ê²°ê³¼ ë³‘í•©\\n        merged_results = self.merge_cluster_results(cluster_results)\\n\\n        return merged_results\\n\\`\\`\\`\\n\\n## ğŸ”® í–¥í›„ ê°œë°œ ê³„íš\\n\\n### **ë‹¨ê¸° ê°œë°œ (1-3ê°œì›”)**\\n1. **PDF íŒŒì‹± ì—”ì§„**: PyMuPDF ê¸°ë°˜ ê³ ê¸‰ ë²¡í„° ì¶”ì¶œê¸° ê°œë°œ\\n2. **ê¸°ë³¸ ê·¸ë˜í”„ êµ¬ì„±**: ë…¼ë¬¸ ë°©ë²•ë¡  ê¸°ë°˜ í”„ë¡œí† íƒ€ì… êµ¬í˜„\\n3. **FloorplanCAD ë°ì´í„°ì…‹**: í•™ìŠµ ë° ê²€ì¦ í™˜ê²½ êµ¬ì¶•\\n\\n### **ì¤‘ê¸° ê°œë°œ (3-6ê°œì›”)**\\n1. **ì‚°ì—… ë„ë©´ ì ì‘**: CAD ì „ê¸°/ê¸°ê³„ ë„ë©´ íŠ¹í™” ì „ì²˜ë¦¬\\n2. **í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼**: ê¸°ì¡´ YOLO + VectorGraphNET ê²°í•©\\n3. **ì„±ëŠ¥ ìµœì í™”**: ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥í•œ ê²½ëŸ‰í™”\\n\\n### **ì¥ê¸° ë¹„ì „ (6ê°œì›”+)**\\n1. **3D CAD í™•ì¥**: 3D ëª¨ë¸ì˜ ë²¡í„° ê·¸ë˜í”„ ë¶„ì„\\n2. **í˜‘ì—… í”Œë«í¼**: ë²¡í„° ê¸°ë°˜ ì‹¤ì‹œê°„ ë„ë©´ í¸ì§‘\\n3. **í‘œì¤€í™”**: CAD ë²¡í„° ê·¸ë˜í”„ ë¶„ì„ì˜ ì—…ê³„ í‘œì¤€ êµ¬ì¶•\\n\\n## ğŸ“š ê´€ë ¨ ì—°êµ¬ ë° ìë£Œ\\n\\n### **í•µì‹¬ ë…¼ë¬¸**\\n- [VectorGraphNET ì›ë³¸ ë…¼ë¬¸](https://arxiv.org/html/2410.01336v1)\\n- [FloorPlanCAD Dataset (ICCV 2021)](https://ar5iv.labs.arxiv.org/html/2105.07147)\\n- [CADTransformer (CVPR 2022)](https://github.com/VITA-Group/CADTransformer)\\n\\n### **ê´€ë ¨ ë°ì´í„°ì…‹**\\n- **FloorPlanCAD**: [ê³µì‹ ì‚¬ì´íŠ¸](https://floorplancad.github.io/)\\n- **CubiCasa5k**: [GitHub](https://github.com/CubiCasa/CubiCasa5k)\\n- **ê±´ì¶• ë„ë©´ ë°ì´í„°ì…‹**: ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ í‰ë©´ë„\\n\\n### **êµ¬í˜„ ì°¸ê³  ìë£Œ**\\n- **PyMuPDF**: PDF ë²¡í„° ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬\\n- **PyTorch Geometric**: ê·¸ë˜í”„ ì‹ ê²½ë§ í”„ë ˆì„ì›Œí¬\\n- **Shapely**: ê¸°í•˜í•™ì  ì—°ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬\\n\\n---\\n\\n**ê²°ë¡ **: VectorGraphNETì€ **ë²¡í„° ë°ì´í„°ì˜ ë³¸ì§ˆì  íŠ¹ì„±**ì„ í™œìš©í•˜ì—¬ CAD ë„ë©´ ë¶„ì„ì— ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•©ë‹ˆë‹¤. ë¹„ë¡ êµ¬í˜„ì— ë„ì „ì´ ë”°ë¥´ì§€ë§Œ, **ì •í™•ì„±ê³¼ íš¨ìœ¨ì„±** ë©´ì—ì„œ ê¸°ì¡´ ë˜ìŠ¤í„° ê¸°ë°˜ ì ‘ê·¼ë²•ì„ í¬ê²Œ ë›°ì–´ë„˜ì„ ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.";
        markdownContent['gat-cadnet'] = "# ğŸ•¸ï¸ GAT-CADNet: ê·¸ë˜í”„ ì–´í…ì…˜ìœ¼ë¡œ CAD ì‹¬ë³¼ ê´€ê³„ ë¶„ì„\\n\\n## ğŸ“‹ ê°œìš”\\n\\n**GAT-CADNet**ì€ CVPR 2022ì—ì„œ ë°œí‘œëœ **ê·¸ë˜í”„ ì–´í…ì…˜ ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ CAD ë„ë©´ ë¶„ì„ ëª¨ë¸**ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ì´ë¯¸ì§€ ê¸°ë°˜ ì ‘ê·¼ë²•ê³¼ ë‹¬ë¦¬ **CAD ë„ë©´ì„ ê·¸ë˜í”„ë¡œ ëª¨ë¸ë§**í•˜ì—¬ ì‹¬ë³¼ê°„ ê´€ê³„ì™€ ê³µê°„ì  ì—°ê²°ì„±ì„ ì´í•´í•˜ëŠ” í˜ì‹ ì  ì ‘ê·¼ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.\\n\\n## ğŸš€ í•µì‹¬ íŠ¹ì§•\\n\\n### 1. **ê·¸ë˜í”„ ê¸°ë°˜ CAD ëª¨ë¸ë§**\\n- **ë²¡í„° ê·¸ë˜í”„**: CAD ë„ë©´ì„ ê¸°í•˜í•™ì  ê¸°ë³¸ ìš”ì†Œ(segments, arcs, circles)ì˜ ê·¸ë˜í”„ë¡œ ë³€í™˜\\n- **ê´€ê³„ ì¤‘ì‹¬**: ì‹¬ë³¼ ê°„ ê³µê°„ì , ì˜ë¯¸ì  ê´€ê³„ ë¶„ì„\\n- **ì„œë¸Œê·¸ë˜í”„ ê²€ì¶œ**: ì¸ìŠ¤í„´ìŠ¤ ì‹¬ë³¼ ìŠ¤íŒŸíŒ…ì„ ì„œë¸Œê·¸ë˜í”„ ê²€ì¶œ ë¬¸ì œë¡œ ì •ì˜\\n\\n### 2. **í˜ì‹ ì  ëª¨ë“ˆ**\\n- **RSE (Relative Spatial Encoding)**: ì •ì  ê°„ ìƒëŒ€ì  ìœ„ì¹˜ì™€ ê¸°í•˜í•™ì  ê´€ê³„ ì¸ì½”ë”©\\n- **CEE (Cascaded Edge Encoding)**: ë‹¤ì¤‘ GAT ë‹¨ê³„ì˜ ì–´í…ì…˜ì„ ì—£ì§€ ì¸ì½”ë”©ìœ¼ë¡œ í™œìš©\\n- **ì¸ì ‘ í–‰ë ¬ ì˜ˆì¸¡**: ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì§ì ‘ ì˜ˆì¸¡í•˜ì—¬ ì‹¬ë³¼ ì¸ìŠ¤í„´ìŠ¤ ë¶„í• \\n\\n### 3. **í†µí•© ë„¤íŠ¸ì›Œí¬**\\n- **ë‹¨ì¼ ëª¨ë¸**: ì˜ë¯¸ì  ë¶„í• ê³¼ ì¸ìŠ¤í„´ìŠ¤ ë¶„í• ì„ í•˜ë‚˜ì˜ ë„¤íŠ¸ì›Œí¬ë¡œ í•´ê²°\\n- **End-to-End**: ê·¸ë˜í”„ êµ¬ì„±ë¶€í„° ìµœì¢… ì‹¬ë³¼ ê²€ì¶œê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ìµœì í™”\\n\\n## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ\\n\\n### **FloorplanCAD ë²¤ì¹˜ë§ˆí¬**\\n\\`\\`\\`\\nğŸ† ê¸°ì¡´ SOTA ëŒ€ë¹„ ëŒ€í­ í–¥ìƒ\\nğŸ¯ Panoptic Symbol Spotting: ìµœê³  ì„±ëŠ¥\\nğŸ“Š ê·¸ë˜í”„ ê¸°ë°˜ ì ‘ê·¼ë²•ì˜ ìš°ìˆ˜ì„± ì…ì¦\\nğŸ” ë³µì¡í•œ CAD ë„ë©´ì—ì„œ ì•ˆì •ì  ì„±ëŠ¥\\n\\`\\`\\`\\n\\n### **ê¸°ì¡´ ë°©ë²• ëŒ€ë¹„ ìš°ìœ„**\\n| ì ‘ê·¼ë²• | ê·¸ë˜í”„ í™œìš© | ê´€ê³„ ë¶„ì„ | í™•ì¥ì„± |\\n|--------|-------------|-----------|--------|\\n| **GAT-CADNet** | âœ… ì „ë©´ì  | âœ… ê³ ê¸‰ | âœ… ìš°ìˆ˜ |\\n| CNN ê¸°ë°˜ | âŒ ì—†ìŒ | âŒ ì œí•œì  | âŒ ì œí•œ |\\n| Transformer | ì¼ë¶€ | âš¡ ê¸°ë³¸ | âš¡ ë³´í†µ |\\n\\n## ğŸ—ï¸ ì•„í‚¤í…ì²˜\\n\\n### **ì „ì²´ íŒŒì´í”„ë¼ì¸**\\n\\n\\`\\`\\`mermaid\\ngraph TB\\n    A[CAD Drawing] --> B[Vectorization]\\n    B --> C[Graph Construction]\\n\\n    C --> D[Node Features]\\n    C --> E[Edge Features]\\n\\n    D --> F[GAT Branch]\\n    E --> F\\n\\n    F --> G[RSE Module]\\n    G --> H[Vertex Attention]\\n\\n    H --> I[CEE Module]\\n    I --> J[Edge Encoding]\\n\\n    J --> K[Adjacency Matrix Prediction]\\n    K --> L[Instance Segmentation]\\n\\n    H --> M[Semantic Classification]\\n\\n    L --> N[Final Symbol Detection]\\n    M --> N\\n\\n    style A fill:#e1f5fe\\n    style N fill:#c8e6c9\\n    style F fill:#fff3e0\\n    style G fill:#fff3e0\\n    style I fill:#fff3e0\\n\\`\\`\\`\\n\\n### **í•µì‹¬ êµ¬ì„±ìš”ì†Œ**\\n\\n#### **1. ê·¸ë˜í”„ êµ¬ì„±**\\n\\`\\`\\`python\\nclass CADGraphConstructor:\\n    def __init__(self):\\n        self.connectivity_threshold = 5.0  # í”½ì…€ ë‹¨ìœ„\\n\\n    def build_cad_graph(self, cad_primitives):\\n        \\\"\\\"\\\"CAD ê¸°í•˜í•™ì  ê¸°ë³¸ ìš”ì†Œë¥¼ ê·¸ë˜í”„ë¡œ ë³€í™˜\\\"\\\"\\\"\\n        graph = nx.Graph()\\n\\n        # ë…¸ë“œ ì¶”ê°€ (ê° ê¸°ë³¸ ìš”ì†Œ)\\n        for idx, primitive in enumerate(cad_primitives):\\n            features = self.extract_primitive_features(primitive)\\n            graph.add_node(idx, **features)\\n\\n        # ì—£ì§€ ì¶”ê°€ (ê¸°ë³¸ ìš”ì†Œ ê°„ ê´€ê³„)\\n        for i in range(len(cad_primitives)):\\n            for j in range(i+1, len(cad_primitives)):\\n                if self.are_connected(cad_primitives[i], cad_primitives[j]):\\n                    relation = self.compute_relationship(\\n                        cad_primitives[i], cad_primitives[j]\\n                    )\\n                    graph.add_edge(i, j, **relation)\\n\\n        return graph\\n\\n    def extract_primitive_features(self, primitive):\\n        \\\"\\\"\\\"ê¸°í•˜í•™ì  ê¸°ë³¸ ìš”ì†Œì˜ íŠ¹ì§• ì¶”ì¶œ\\\"\\\"\\\"\\n        if primitive.type == \\'line_segment\\':\\n            return {\\n                \\'type\\': \\'segment\\',\\n                \\'length\\': primitive.length,\\n                \\'angle\\': primitive.angle,\\n                \\'midpoint\\': primitive.midpoint,\\n                \\'endpoints\\': [primitive.start, primitive.end]\\n            }\\n        elif primitive.type == \\'arc\\':\\n            return {\\n                \\'type\\': \\'arc\\',\\n                \\'radius\\': primitive.radius,\\n                \\'center\\': primitive.center,\\n                \\'start_angle\\': primitive.start_angle,\\n                \\'end_angle\\': primitive.end_angle\\n            }\\n        # ... ê¸°íƒ€ ê¸°ë³¸ ìš”ì†Œë“¤\\n\\`\\`\\`\\n\\n#### **2. RSE (Relative Spatial Encoding)**\\n\\`\\`\\`python\\nclass RelativeSpatialEncoding(nn.Module):\\n    def __init__(self, embedding_dim):\\n        super().__init__()\\n        self.embedding_dim = embedding_dim\\n        self.distance_encoder = nn.Linear(1, embedding_dim)\\n        self.angle_encoder = nn.Linear(1, embedding_dim)\\n        self.type_encoder = nn.Embedding(10, embedding_dim)  # ê´€ê³„ íƒ€ì…\\n\\n    def forward(self, node_features, edge_indices):\\n        \\\"\\\"\\\"ìƒëŒ€ì  ê³µê°„ ê´€ê³„ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ì¸ì½”ë”©\\\"\\\"\\\"\\n        spatial_encodings = []\\n\\n        for edge_idx, (i, j) in enumerate(edge_indices.T):\\n            # ê±°ë¦¬ ì¸ì½”ë”©\\n            distance = torch.norm(\\n                node_features[i][\\'position\\'] - node_features[j][\\'position\\']\\n            )\\n            distance_enc = self.distance_encoder(distance.unsqueeze(0))\\n\\n            # ê°ë„ ì¸ì½”ë”©\\n            angle = self.compute_relative_angle(\\n                node_features[i], node_features[j]\\n            )\\n            angle_enc = self.angle_encoder(angle.unsqueeze(0))\\n\\n            # ê´€ê³„ íƒ€ì… ì¸ì½”ë”©\\n            relation_type = self.classify_relationship(\\n                node_features[i], node_features[j]\\n            )\\n            type_enc = self.type_encoder(relation_type)\\n\\n            # í†µí•© ì¸ì½”ë”©\\n            spatial_encoding = distance_enc + angle_enc + type_enc\\n            spatial_encodings.append(spatial_encoding)\\n\\n        return torch.stack(spatial_encodings)\\n\\`\\`\\`\\n\\n#### **3. GAT with CEE Module**\\n\\`\\`\\`python\\nclass GATCADNet(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, num_classes):\\n        super().__init__()\\n\\n        # Multi-stage GAT layers\\n        self.gat_layers = nn.ModuleList([\\n            GATConv(input_dim, hidden_dim, heads=8),\\n            GATConv(hidden_dim*8, hidden_dim, heads=8),\\n            GATConv(hidden_dim*8, hidden_dim, heads=1)\\n        ])\\n\\n        # RSE Module\\n        self.rse = RelativeSpatialEncoding(hidden_dim)\\n\\n        # CEE Module for edge encoding\\n        self.cee = CascadedEdgeEncoding(hidden_dim)\\n\\n        # Classification heads\\n        self.semantic_classifier = nn.Linear(hidden_dim, num_classes)\\n        self.adjacency_predictor = nn.Linear(hidden_dim*2, 1)\\n\\n    def forward(self, x, edge_index, edge_attr):\\n        # Multi-stage GAT processing\\n        attention_maps = []\\n\\n        for gat_layer in self.gat_layers:\\n            x, attention = gat_layer(x, edge_index, return_attention_weights=True)\\n            attention_maps.append(attention)\\n            x = F.elu(x)\\n\\n        # Semantic classification\\n        semantic_logits = self.semantic_classifier(x)\\n\\n        # Cascaded Edge Encoding\\n        edge_features = self.cee(attention_maps, edge_index)\\n\\n        # Adjacency matrix prediction\\n        adjacency_logits = self.predict_adjacency(x, edge_features, edge_index)\\n\\n        return {\\n            \\'semantic\\': semantic_logits,\\n            \\'adjacency\\': adjacency_logits,\\n            \\'node_features\\': x\\n        }\\n\\`\\`\\`\\n\\n## ğŸ’» êµ¬í˜„ ì •ë³´\\n\\n### **ê³µì‹ ì €ì¥ì†Œ**\\n- **ë¹„ê³µì‹ êµ¬í˜„**: [Liberation-happy/GAT-CADNet](https://github.com/Liberation-happy/GAT-CADNet)\\n- **ë…¼ë¬¸**: [CVPR 2022 Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_GAT-CADNet_Graph_Attention_Network_for_Panoptic_Symbol_Spotting_in_CAD_CVPR_2022_paper.pdf)\\n- **arXiv**: [2201.00625](https://arxiv.org/abs/2201.00625)\\n\\n### **í™˜ê²½ ìš”êµ¬ì‚¬í•­**\\n\\`\\`\\`bash\\n# ê°œë°œ í™˜ê²½\\nOS: Ubuntu 22.04\\nPython: 3.11\\nPyTorch: Latest\\nPyTorch Geometric: Latest\\n\\n# ë°ì´í„°ì…‹\\nFloorPlanCAD: ì˜¤í”ˆì†ŒìŠ¤ ë²¤ì¹˜ë§ˆí¬\\n\\`\\`\\`\\n\\n### **ì„¤ì¹˜ ë° ì‹¤í–‰**\\n\\`\\`\\`bash\\n# ë¹„ê³µì‹ êµ¬í˜„ í´ë¡ \\ngit clone https://github.com/Liberation-happy/GAT-CADNet.git\\ncd GAT-CADNet\\n\\n# ì˜ì¡´ì„± ì„¤ì¹˜\\npip install torch torchvision\\npip install torch-geometric\\npip install networkx shapely\\n\\n# FloorPlanCAD ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\\n# https://floorplancad.github.io/ ì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\\n\\n# í•™ìŠµ ì‹¤í–‰\\npython train.py --config configs/gatcadnet_config.yaml\\n\\`\\`\\`\\n\\n## ğŸ”§ DrawingBOMExtractor í†µí•© ë°©ì•ˆ\\n\\n### **1. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•**\\n\\`\\`\\`python\\nclass HybridCADDetector:\\n    def __init__(self):\\n        # ê¸°ì¡´ YOLO/Detectron2 ëª¨ë¸ë“¤\\n        self.yolo_models = load_yolo_models()\\n\\n        # GAT-CADNet ì¶”ê°€\\n        self.gat_cadnet = GATCADNet.from_pretrained(\\\"gatcadnet_checkpoint.pt\\\")\\n\\n        # ê·¸ë˜í”„ êµ¬ì„±ê¸°\\n        self.graph_builder = CADGraphBuilder()\\n\\n    def detect_with_graph_analysis(self, image):\\n        \\\"\\\"\\\"ê¸°ì¡´ ê²€ì¶œ + ê·¸ë˜í”„ ê´€ê³„ ë¶„ì„\\\"\\\"\\\"\\n\\n        # 1ë‹¨ê³„: ê¸°ì¡´ ëª¨ë¸ë“¤ë¡œ ê¸°ë³¸ ê²€ì¶œ\\n        basic_detections = []\\n        for model in self.yolo_models:\\n            detections = model.detect(image)\\n            basic_detections.extend(detections)\\n\\n        # 2ë‹¨ê³„: ê²€ì¶œëœ ì‹¬ë³¼ë“¤ì„ ê·¸ë˜í”„ë¡œ êµ¬ì„±\\n        symbol_graph = self.build_symbol_relationship_graph(\\n            image, basic_detections\\n        )\\n\\n        # 3ë‹¨ê³„: GAT-CADNetìœ¼ë¡œ ê´€ê³„ ë¶„ì„ ë° ì •êµí™”\\n        refined_detections = self.refine_with_graph_analysis(\\n            symbol_graph, basic_detections\\n        )\\n\\n        # 4ë‹¨ê³„: í–¥ìƒëœ Voting ë©”ì»¤ë‹ˆì¦˜\\n        final_results = self.graph_aware_voting(refined_detections)\\n\\n        return final_results\\n\\n    def build_symbol_relationship_graph(self, image, detections):\\n        \\\"\\\"\\\"ê²€ì¶œëœ ì‹¬ë³¼ë“¤ì˜ ê´€ê³„ ê·¸ë˜í”„ êµ¬ì„±\\\"\\\"\\\"\\n        graph = nx.Graph()\\n\\n        # ì‹¬ë³¼ì„ ë…¸ë“œë¡œ ì¶”ê°€\\n        for idx, detection in enumerate(detections):\\n            graph.add_node(idx, **{\\n                \\'bbox\\': detection[\\'bbox\\'],\\n                \\'class\\': detection[\\'class_name\\'],\\n                \\'confidence\\': detection[\\'confidence\\'],\\n                \\'center\\': self.get_bbox_center(detection[\\'bbox\\'])\\n            })\\n\\n        # ì‹¬ë³¼ ê°„ ê´€ê³„ë¥¼ ì—£ì§€ë¡œ ì¶”ê°€\\n        for i in range(len(detections)):\\n            for j in range(i+1, len(detections)):\\n                relationship = self.analyze_symbol_relationship(\\n                    detections[i], detections[j]\\n                )\\n\\n                if relationship[\\'connected\\']:\\n                    graph.add_edge(i, j, **relationship)\\n\\n        return graph\\n\\`\\`\\`\\n\\n### **2. ì „ê¸° íšŒë¡œë„ íŠ¹í™” ë¶„ì„**\\n\\`\\`\\`python\\nclass ElectricalSchematicAnalyzer:\\n    def __init__(self, gat_cadnet_model):\\n        self.model = gat_cadnet_model\\n        self.electrical_rules = ElectricalConnectionRules()\\n\\n    def analyze_electrical_connections(self, symbol_graph):\\n        \\\"\\\"\\\"ì „ê¸°ì  ì—°ê²° ê´€ê³„ ë¶„ì„\\\"\\\"\\\"\\n\\n        # ì™€ì´ì–´/ì—°ê²°ì„  ì‹ë³„\\n        wire_nodes = self.identify_wire_segments(symbol_graph)\\n\\n        # ì‹¬ë³¼ ê°„ ì „ê¸°ì  ì—°ê²° ì¶”ì \\n        electrical_connections = []\\n\\n        for wire in wire_nodes:\\n            connected_symbols = self.trace_electrical_path(wire, symbol_graph)\\n\\n            # ì „ê¸°ì  ê·œì¹™ ê²€ì¦\\n            valid_connection = self.electrical_rules.validate_connection(\\n                connected_symbols\\n            )\\n\\n            if valid_connection:\\n                electrical_connections.append({\\n                    \\'wire\\': wire,\\n                    \\'symbols\\': connected_symbols,\\n                    \\'connection_type\\': \\'electrical\\',\\n                    \\'voltage_level\\': self.infer_voltage_level(connected_symbols)\\n                })\\n\\n        return electrical_connections\\n\\n    def enhance_bom_with_connections(self, bom, connections):\\n        \\\"\\\"\\\"BOMì— ì—°ê²° ì •ë³´ ì¶”ê°€\\\"\\\"\\\"\\n        enhanced_bom = bom.copy()\\n\\n        # ê° ë¶€í’ˆì— ì—°ê²°ëœ ë‹¤ë¥¸ ë¶€í’ˆë“¤ ì •ë³´ ì¶”ê°€\\n        for item in enhanced_bom:\\n            item[\\'connections\\'] = self.find_connected_components(\\n                item, connections\\n            )\\n            item[\\'electrical_properties\\'] = self.infer_electrical_properties(\\n                item, connections\\n            )\\n\\n        return enhanced_bom\\n\\`\\`\\`\\n\\n### **3. ì‹¬ë³¼ ê·¸ë£¹í™” ë° ê³„ì¸µ êµ¬ì¡°**\\n\\`\\`\\`python\\ndef analyze_symbol_hierarchy(symbol_graph):\\n    \\\"\\\"\\\"ì‹¬ë³¼ì˜ ê³„ì¸µì  êµ¬ì¡° ë¶„ì„\\\"\\\"\\\"\\n\\n    # ì œì–´ íšŒë¡œ ì‹ë³„\\n    control_circuits = find_control_circuits(symbol_graph)\\n\\n    # ì „ë ¥ íšŒë¡œ ì‹ë³„\\n    power_circuits = find_power_circuits(symbol_graph)\\n\\n    # ë³´í˜¸ íšŒë¡œ ì‹ë³„\\n    protection_circuits = find_protection_circuits(symbol_graph)\\n\\n    # ê³„ì¸µì  BOM êµ¬ì„±\\n    hierarchical_bom = {\\n        \\'control_system\\': {\\n            \\'components\\': control_circuits,\\n            \\'total_cost\\': calculate_subsystem_cost(control_circuits)\\n        },\\n        \\'power_system\\': {\\n            \\'components\\': power_circuits,\\n            \\'total_cost\\': calculate_subsystem_cost(power_circuits)\\n        },\\n        \\'protection_system\\': {\\n            \\'components\\': protection_circuits,\\n            \\'total_cost\\': calculate_subsystem_cost(protection_circuits)\\n        }\\n    }\\n\\n    return hierarchical_bom\\n\\`\\`\\`\\n\\n## ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼\\n\\n### **ê´€ê³„ ì¸ì‹ì˜ ì¥ì **\\n1. **ë¬¸ë§¥ì  ì´í•´**: ê°œë³„ ì‹¬ë³¼ì´ ì•„ë‹Œ ì‹œìŠ¤í…œ ì „ì²´ì˜ ë§¥ë½ì—ì„œ ì´í•´\\n2. **ì˜¤íƒ ê°ì†Œ**: ê´€ê³„ ì •ë³´ë¥¼ í†µí•œ ê²€ì¶œ ê²°ê³¼ ê²€ì¦\\n3. **ëˆ„ë½ ë³´ì™„**: ì—°ê²°ëœ ì‹¬ë³¼ ì •ë³´ë¡œ ëˆ„ë½ëœ ë¶€í’ˆ ì¶”ë¡ \\n4. **ê³„ì¸µì  ë¶„ì„**: ì„œë¸Œì‹œìŠ¤í…œ ë‹¨ìœ„ì˜ ì²´ê³„ì  BOM êµ¬ì„±\\n\\n### **ì‹¤ì œ ì ìš© ì‚¬ë¡€**\\n\\`\\`\\`python\\n# ì˜ˆì‹œ: ë³€ì••ê¸°-ì°¨ë‹¨ê¸° ì—°ê²° ê²€ì¦\\ndef validate_transformer_breaker_connection(graph, transformer_id, breaker_id):\\n    \\\"\\\"\\\"ë³€ì••ê¸°ì™€ ì°¨ë‹¨ê¸°ì˜ ì—°ê²° íƒ€ë‹¹ì„± ê²€ì¦\\\"\\\"\\\"\\n\\n    # ë‘ ì‹¬ë³¼ ê°„ ê²½ë¡œ ì°¾ê¸°\\n    path = nx.shortest_path(graph, transformer_id, breaker_id)\\n\\n    # ì—°ê²° ê·œì¹™ ê²€ì¦\\n    if len(path) <= 3:  # ì§ì ‘ ì—°ê²° ë˜ëŠ” í•œ ë‹¨ê³„ ê²½ìœ \\n        transformer_rating = graph.nodes[transformer_id][\\'power_rating\\']\\n        breaker_rating = graph.nodes[breaker_id][\\'current_rating\\']\\n\\n        # ì •ê²© ë§¤ì¹­ í™•ì¸\\n        if is_rating_compatible(transformer_rating, breaker_rating):\\n            return {\\n                \\'valid\\': True,\\n                \\'confidence\\': 0.95,\\n                \\'reason\\': \\'ì •ê²©ì´ í˜¸í™˜ë˜ëŠ” ì§ì ‘ ì—°ê²°\\'\\n            }\\n\\n    return {\\n        \\'valid\\': False,\\n        \\'confidence\\': 0.1,\\n        \\'reason\\': \\'ë¶€ì ì ˆí•œ ì—°ê²° ë˜ëŠ” ì •ê²© ë¶ˆì¼ì¹˜\\'\\n    }\\n\\`\\`\\`\\n\\n## âš ï¸ ì œí•œì‚¬í•­ ë° í•´ê²°ì±…\\n\\n### **í˜„ì¬ ì œí•œì‚¬í•­**\\n1. **ë©”ëª¨ë¦¬ ì œì•½**: ëŒ€ê·œëª¨ ë„ë©´ì—ì„œ ê·¸ë˜í”„ í¬ê¸° ì œí•œ\\n2. **ì²˜ë¦¬ ì‹œê°„**: ë³µì¡í•œ ê·¸ë˜í”„ ë¶„ì„ìœ¼ë¡œ ì¸í•œ ì†ë„ ì €í•˜\\n3. **ë„ë©”ì¸ íŠ¹í™”**: FloorplanCADì— íŠ¹í™”ë˜ì–´ ì‚°ì—… ë„ë©´ ì ì‘ í•„ìš”\\n\\n### **ìµœì í™” ì „ëµ**\\n\\`\\`\\`python\\nclass ScalableGATCADNet:\\n    def __init__(self, max_nodes=500):\\n        self.max_nodes = max_nodes\\n        self.clustering_method = \\\"spectral\\\"\\n\\n    def process_large_drawing(self, large_graph):\\n        \\\"\\\"\\\"ëŒ€ê·œëª¨ ë„ë©´ì˜ ë¶„í•  ì²˜ë¦¬\\\"\\\"\\\"\\n\\n        if len(large_graph.nodes) <= self.max_nodes:\\n            return self.model(large_graph)\\n\\n        # ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„°ë§\\n        clusters = self.cluster_graph(large_graph)\\n\\n        # ê° í´ëŸ¬ìŠ¤í„° ë…ë¦½ ì²˜ë¦¬\\n        cluster_results = []\\n        for cluster in clusters:\\n            result = self.model(cluster)\\n            cluster_results.append(result)\\n\\n        # í´ëŸ¬ìŠ¤í„° ê²½ê³„ ë¶€ë¶„ í›„ì²˜ë¦¬\\n        boundary_refinement = self.refine_cluster_boundaries(\\n            cluster_results, large_graph\\n        )\\n\\n        # ì „ì²´ ê²°ê³¼ í†µí•©\\n        final_result = self.merge_cluster_results(\\n            cluster_results, boundary_refinement\\n        )\\n\\n        return final_result\\n\\`\\`\\`\\n\\n## ğŸ”® í–¥í›„ ë°œì „ ë°©í–¥\\n\\n### **ë‹¨ê¸° ê°œë°œ (1-3ê°œì›”)**\\n1. **ì‚°ì—… ë„ë©´ ì ì‘**: ì „ê¸°/ê¸°ê³„ ë„ë©´ìš© ê·¸ë˜í”„ êµ¬ì„± ê·œì¹™ ê°œë°œ\\n2. **ì„±ëŠ¥ ìµœì í™”**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ê·¸ë˜í”„ ì²˜ë¦¬ ì•Œê³ ë¦¬ì¦˜\\n3. **ì‹¤ì‹œê°„ ë¶„ì„**: ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì˜ ì ì§„ì  ê·¸ë˜í”„ ë¶„ì„\\n\\n### **ì¤‘ê¸° ê°œë°œ (3-6ê°œì›”)**\\n1. **ë‹¤ì¤‘ ìŠ¤ì¼€ì¼**: ì„¸ë¶€/ì „ì²´ ë ˆë²¨ì˜ ê³„ì¸µì  ê·¸ë˜í”„ ë¶„ì„\\n2. **ë™ì  ê·¸ë˜í”„**: ë„ë©´ í¸ì§‘ ì‹œ ì‹¤ì‹œê°„ ê´€ê³„ ì—…ë°ì´íŠ¸\\n3. **ê·œì¹™ ì—”ì§„**: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì—°ê²° ê·œì¹™ ì‹œìŠ¤í…œ\\n\\n### **ì¥ê¸° ë¹„ì „ (6ê°œì›”+)**\\n1. **3D í™•ì¥**: 3D CAD ëª¨ë¸ì˜ ê·¸ë˜í”„ ê¸°ë°˜ ë¶„ì„\\n2. **ì„¤ê³„ ê²€ì¦**: ê·¸ë˜í”„ ë¶„ì„ì„ í†µí•œ ì„¤ê³„ ì˜¤ë¥˜ ìë™ ê²€ì¶œ\\n3. **ìƒì„± ëª¨ë¸**: ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì´ìš©í•œ CAD ë„ë©´ ìë™ ìƒì„±\\n\\n## ğŸ“š ì¶”ê°€ ìë£Œ\\n\\n### **í•µì‹¬ ë…¼ë¬¸**\\n- [GAT-CADNet CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_GAT-CADNet_Graph_Attention_Network_for_Panoptic_Symbol_Spotting_in_CAD_CVPR_2022_paper.pdf)\\n- [Graph Attention Networks](https://arxiv.org/abs/1710.10903)\\n- [FloorPlanCAD Dataset](https://ar5iv.labs.arxiv.org/html/2105.07147)\\n\\n### **ê´€ë ¨ êµ¬í˜„**\\n- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/)\\n- [NetworkX](https://networkx.org/) - ê·¸ë˜í”„ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬\\n- [DGL](https://www.dgl.ai/) - ë”¥ ê·¸ë˜í”„ ë¼ì´ë¸ŒëŸ¬ë¦¬\\n\\n### **ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹**\\n- **FloorplanCAD**: 15,000+ í‰ë©´ë„\\n- **CubiCasa5k**: 5,000 ì£¼íƒ í‰ë©´ë„\\n- **ì‹¤ì œ ì‚°ì—… ë„ë©´**: ì „ê¸°/ê¸°ê³„/í™”í•™ ê³µì •ë„\\n\\n---\\n\\n**ê²°ë¡ **: GAT-CADNetì€ **ì‹¬ë³¼ ê°„ ê´€ê³„ì™€ ì—°ê²°ì„±**ì„ ì´í•´í•˜ëŠ” í˜ì‹ ì  ì ‘ê·¼ë²•ìœ¼ë¡œ, ë‹¨ìˆœí•œ ê°ì²´ ê²€ì¶œì„ ë„˜ì–´ **ì‹œìŠ¤í…œ ë ˆë²¨ì˜ ì´í•´**ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë” ì •í™•í•˜ê³  ì˜ë¯¸ìˆëŠ” BOM ìƒì„±ì´ ê°€ëŠ¥í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.";
        markdownContent['rt-detr'] = "# âš¡ RT-DETR v2: ì‹¤ì‹œê°„ Detection Transformer\\n\\n## ğŸ“‹ ê°œìš”\\n\\n**RT-DETR (Real-Time Detection Transformer)**ëŠ” Baiduì—ì„œ ê°œë°œí•œ **ì‹¤ì‹œê°„ ê°ì²´ ê²€ì¶œ ëª¨ë¸**ë¡œ, **\\\"DETRs Beat YOLOs on Real-time Object Detection\\\"**ì´ë¼ëŠ” ìŠ¬ë¡œê±´ìœ¼ë¡œ CVPR 2024ì—ì„œ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤. 2024ë…„ 7ì›”ì—ëŠ” **RTDETRv2**ê°€ ì¶œì‹œë˜ì–´ ë”ìš± í–¥ìƒëœ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\\n\\n## ğŸš€ í•µì‹¬ íŠ¹ì§•\\n\\n### 1. **NMS-Free ì•„í‚¤í…ì²˜**\\n- **í›„ì²˜ë¦¬ ì—†ìŒ**: Non-Maximum Suppression ë¶ˆí•„ìš”\\n- **End-to-End**: ì›ìƒ·ìœ¼ë¡œ ìµœì¢… ê²€ì¶œ ê²°ê³¼ ì¶œë ¥\\n- **ì•µì»¤ í”„ë¦¬**: ì•µì»¤ ë°•ìŠ¤ ì—†ì´ ì§ì ‘ ê°ì²´ ìœ„ì¹˜ ì˜ˆì¸¡\\n\\n### 2. **ì‹¤ì‹œê°„ ì„±ëŠ¥**\\n- **ì†ë„**: YOLO ì‹œë¦¬ì¦ˆì™€ ê²½ìŸí•  ìˆ˜ ìˆëŠ” ì‹¤ì‹œê°„ ì²˜ë¦¬\\n- **ì •í™•ë„**: íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ìœ¼ë¡œ ë†’ì€ ì •í™•ë„ ìœ ì§€\\n- **íš¨ìœ¨ì„±**: í•˜ì´ë¸Œë¦¬ë“œ ì¸ì½”ë”ë¡œ ê³„ì‚° ë¹„ìš© ìµœì í™”\\n\\n### 3. **ìœ ì—°í•œ ì¶”ë¡ **\\n- **ë™ì  ì¡°ì •**: ì¶”ë¡  ì‹œ ë””ì½”ë” ë ˆì´ì–´ ìˆ˜ ì¡°ì • ê°€ëŠ¥\\n- **ì¬í•™ìŠµ ë¶ˆí•„ìš”**: ì†ë„/ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì‹¤ì‹œê°„ ì¡°ì •\\n- **í™•ì¥ì„±**: ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ í™˜ê²½ì— ì ì‘\\n\\n## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ\\n\\n### **ë²¤ì¹˜ë§ˆí¬ ì„±ê³¼**\\n\\`\\`\\`\\nğŸ† RT-DETR-L: COCO 53.0 AP @ 114 FPS (T4 GPU)\\nğŸ† RT-DETR-X: COCO 54.8 AP @ 74 FPS (T4 GPU)\\nâš¡ RTDETRv2: 2024ë…„ 7ì›” ì¶”ê°€ ì„±ëŠ¥ í–¥ìƒ\\nğŸ¯ YOLO ì‹œë¦¬ì¦ˆ ëŒ€ë¹„ ìš°ìˆ˜í•œ ì •í™•ë„-ì†ë„ ê· í˜•\\n\\`\\`\\`\\n\\n### **YOLO vs RT-DETR ë¹„êµ**\\n| ëª¨ë¸ | COCO AP | FPS (T4) | ì•„í‚¤í…ì²˜ | NMS |\\n|------|---------|----------|----------|------|\\n| **RT-DETR-X** | **54.8** | **74** | Transformer | âŒ |\\n| YOLOv8x | 53.9 | 76 | CNN | âœ… í•„ìš” |\\n| YOLOv11x | 54.7 | 78 | CNN | âœ… í•„ìš” |\\n\\n## ğŸ—ï¸ ì•„í‚¤í…ì²˜\\n\\n### **ì „ì²´ êµ¬ì¡°**\\n\\n\\`\\`\\`mermaid\\ngraph TB\\n    A[Input Image] --> B[Backbone CNN]\\n    B --> C[Efficient Hybrid Encoder]\\n\\n    C --> D[Intra-scale Interaction]\\n    C --> E[Cross-scale Fusion]\\n\\n    D --> F[Multi-scale Features]\\n    E --> F\\n\\n    F --> G[Transformer Decoder]\\n    G --> H[IoU-aware Query Selection]\\n\\n    H --> I[Classification Head]\\n    H --> J[Regression Head]\\n\\n    I --> K[Final Detection]\\n    J --> K\\n\\`\\`\\`\\n\\n### **í•µì‹¬ í˜ì‹  ê¸°ìˆ **\\n\\n#### **1. íš¨ìœ¨ì  í•˜ì´ë¸Œë¦¬ë“œ ì¸ì½”ë”**\\n\\`\\`\\`python\\nclass EfficientHybridEncoder(nn.Module):\\n    def __init__(self, input_dim, hidden_dim):\\n        super().__init__()\\n\\n        # Intra-scale feature interaction\\n        self.intra_scale_layers = nn.ModuleList([\\n            ConvBlock(input_dim, hidden_dim),\\n            ConvBlock(hidden_dim, hidden_dim),\\n        ])\\n\\n        # Cross-scale feature fusion\\n        self.cross_scale_fusion = CrossScaleFusion(hidden_dim)\\n\\n        # Multi-scale feature processing\\n        self.multi_scale_processor = MultiScaleProcessor(hidden_dim)\\n\\n    def forward(self, multi_scale_features):\\n        # ê° ìŠ¤ì¼€ì¼ ë‚´ë¶€ì˜ íŠ¹ì§• ìƒí˜¸ì‘ìš©\\n        intra_features = []\\n        for features in multi_scale_features:\\n            processed = features\\n            for layer in self.intra_scale_layers:\\n                processed = layer(processed)\\n            intra_features.append(processed)\\n\\n        # ìŠ¤ì¼€ì¼ ê°„ íŠ¹ì§• ìœµí•©\\n        fused_features = self.cross_scale_fusion(intra_features)\\n\\n        # ìµœì¢… ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ìƒì„±\\n        output_features = self.multi_scale_processor(fused_features)\\n\\n        return output_features\\n\\`\\`\\`\\n\\n#### **2. IoU-aware Query Selection**\\n\\`\\`\\`python\\nclass IoUAwareQuerySelection(nn.Module):\\n    def __init__(self, num_queries, feature_dim):\\n        super().__init__()\\n        self.num_queries = num_queries\\n        self.query_embed = nn.Embedding(num_queries, feature_dim)\\n        self.iou_predictor = nn.Linear(feature_dim, 1)\\n\\n    def forward(self, encoder_features):\\n        # ì´ˆê¸° ì¿¼ë¦¬ ì„ë² ë”©\\n        queries = self.query_embed.weight.unsqueeze(0).repeat(\\n            encoder_features.size(0), 1, 1\\n        )\\n\\n        # ê° ì¿¼ë¦¬ì˜ IoU ì˜ˆì¸¡\\n        iou_scores = self.iou_predictor(queries)\\n\\n        # IoU ê¸°ë°˜ ì¿¼ë¦¬ ì„ íƒ ë° ì´ˆê¸°í™”\\n        selected_queries = self.select_top_queries(queries, iou_scores)\\n\\n        return selected_queries\\n\\n    def select_top_queries(self, queries, iou_scores):\\n        \\\"\\\"\\\"ë†’ì€ IoUë¥¼ ê°€ì§„ ì¿¼ë¦¬ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒ\\\"\\\"\\\"\\n        # Top-k ì¿¼ë¦¬ ì„ íƒ ë¡œì§\\n        top_k_indices = torch.topk(iou_scores.squeeze(-1),\\n                                   self.num_queries//2, dim=-1)[1]\\n\\n        selected_queries = torch.gather(\\n            queries, 1,\\n            top_k_indices.unsqueeze(-1).expand(-1, -1, queries.size(-1))\\n        )\\n\\n        return selected_queries\\n\\`\\`\\`\\n\\n#### **3. ìœ ì—°í•œ ì¶”ë¡  ë©”ì»¤ë‹ˆì¦˜**\\n\\`\\`\\`python\\nclass FlexibleRTDETR(nn.Module):\\n    def __init__(self, backbone, encoder, decoder, num_decoder_layers=6):\\n        super().__init__()\\n        self.backbone = backbone\\n        self.encoder = encoder\\n        self.decoder = decoder\\n        self.num_decoder_layers = num_decoder_layers\\n\\n    def forward(self, x, inference_layers=None):\\n        # ì¶”ë¡  ì‹œ ë””ì½”ë” ë ˆì´ì–´ ìˆ˜ ë™ì  ì¡°ì •\\n        if inference_layers is None:\\n            inference_layers = self.num_decoder_layers\\n\\n        # ë°±ë³¸ê³¼ ì¸ì½”ë”ëŠ” ê³ ì •\\n        features = self.backbone(x)\\n        encoded_features = self.encoder(features)\\n\\n        # ë””ì½”ë” ë ˆì´ì–´ ìˆ˜ ì¡°ì •\\n        decoder_output = self.decoder(\\n            encoded_features,\\n            num_layers=inference_layers\\n        )\\n\\n        return decoder_output\\n\\`\\`\\`\\n\\n## ğŸ’» êµ¬í˜„ ì •ë³´\\n\\n### **ê³µì‹ ì €ì¥ì†Œ**\\n- **ì£¼ ì €ì¥ì†Œ**: [lyuwenyu/RT-DETR](https://github.com/lyuwenyu/RT-DETR)\\n- **Ultralytics í†µí•©**: [ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)\\n- **PyTorch êµ¬í˜„**: [pranavdurai10/rtdetr-pytorch](https://github.com/pranavdurai10/rtdetr-pytorch)\\n\\n### **ì„¤ì¹˜ ë° ì‚¬ìš©**\\n\\`\\`\\`bash\\n# ê³µì‹ ì €ì¥ì†Œ ì„¤ì¹˜\\ngit clone https://github.com/lyuwenyu/RT-DETR.git\\ncd RT-DETR\\n\\n# PaddlePaddle ë²„ì „\\npip install paddlepaddle-gpu\\npip install -r requirements.txt\\n\\n# PyTorch ë²„ì „\\npip install torch torchvision\\npip install -r requirements_pytorch.txt\\n\\n# Ultralytics í†µí•© ë²„ì „\\npip install ultralytics\\n\\`\\`\\`\\n\\n### **Ultralytics ì‚¬ìš©ë²•**\\n\\`\\`\\`python\\nfrom ultralytics import RTDETR\\n\\n# ëª¨ë¸ ë¡œë“œ\\nmodel = RTDETR(\\'rtdetr-l.pt\\')\\n\\n# ì¶”ë¡ \\nresults = model(\\'path/to/image.jpg\\')\\n\\n# í•™ìŠµ\\nmodel.train(data=\\'coco128.yaml\\', epochs=100, imgsz=640)\\n\\n# ê²€ì¦\\nmetrics = model.val()\\n\\n# ë‚´ë³´ë‚´ê¸°\\nmodel.export(format=\\'onnx\\')\\n\\`\\`\\`\\n\\n## ğŸ”§ DrawingBOMExtractor í†µí•© ë°©ì•ˆ\\n\\n### **1. 5ë²ˆì§¸ ëª¨ë¸ë¡œ ì¶”ê°€**\\n\\`\\`\\`python\\nclass RTDETRIntegration:\\n    def __init__(self):\\n        # ê¸°ì¡´ 4ê°œ ëª¨ë¸ + RT-DETR ì¶”ê°€\\n        self.models = {\\n            \\'yolo_v11l\\': YOLOv11Model(\\'yolov11l.pt\\'),\\n            \\'yolo_v11x\\': YOLOv11Model(\\'yolov11x.pt\\'),\\n            \\'yolo_v8\\': YOLOv8Model(\\'yolov8x.pt\\'),\\n            \\'detectron2\\': Detectron2Model(),\\n            \\'rtdetr\\': RTDETRModel(\\'rtdetr-x.pt\\')  # ìƒˆë¡œ ì¶”ê°€\\n        }\\n\\n        # RT-DETR ê°€ì¤‘ì¹˜ ì„¤ì •\\n        self.model_weights = {\\n            \\'yolo_v11l\\': 1.2,\\n            \\'yolo_v11x\\': 1.3,\\n            \\'yolo_v8\\': 1.0,\\n            \\'detectron2\\': 1.1,\\n            \\'rtdetr\\': 1.25  # ë†’ì€ ì •í™•ë„ë¡œ ì¸í•œ ë†’ì€ ê°€ì¤‘ì¹˜\\n        }\\n\\n    def detect_with_rtdetr(self, image, confidence_threshold=0.25):\\n        \\\"\\\"\\\"RT-DETRë¡œ ê²€ì¶œ ìˆ˜í–‰\\\"\\\"\\\"\\n        # ëª¨ë¸ ì„¤ì •\\n        model = RTDETR(\\'rtdetr-x.pt\\')\\n\\n        # ì¶”ë¡  ì‹¤í–‰\\n        results = model(image, conf=confidence_threshold)\\n\\n        # ê²°ê³¼ ë³€í™˜\\n        detections = []\\n        for result in results:\\n            boxes = result.boxes\\n            for i in range(len(boxes)):\\n                detection = {\\n                    \\'bbox\\': boxes.xyxy[i].cpu().numpy(),\\n                    \\'confidence\\': boxes.conf[i].item(),\\n                    \\'class_id\\': int(boxes.cls[i].item()),\\n                    \\'class_name\\': model.names[int(boxes.cls[i].item())],\\n                    \\'model_id\\': \\'rtdetr\\'\\n                }\\n                detections.append(detection)\\n\\n        return detections\\n\\`\\`\\`\\n\\n### **2. ë™ì  ì„±ëŠ¥ ì¡°ì •**\\n\\`\\`\\`python\\nclass AdaptiveRTDETR:\\n    def __init__(self):\\n        self.model = RTDETR(\\'rtdetr-x.pt\\')\\n        self.performance_profiles = {\\n            \\'fast\\': {\\'decoder_layers\\': 2, \\'conf_threshold\\': 0.5},\\n            \\'balanced\\': {\\'decoder_layers\\': 4, \\'conf_threshold\\': 0.35},\\n            \\'accurate\\': {\\'decoder_layers\\': 6, \\'conf_threshold\\': 0.25}\\n        }\\n\\n    def detect_adaptive(self, image, performance_mode=\\'balanced\\'):\\n        \\\"\\\"\\\"ì„±ëŠ¥ í”„ë¡œíŒŒì¼ì— ë”°ë¥¸ ì ì‘ì  ê²€ì¶œ\\\"\\\"\\\"\\n        profile = self.performance_profiles[performance_mode]\\n\\n        # ëª¨ë¸ ì„¤ì • ë™ì  ì¡°ì •\\n        results = self.model(\\n            image,\\n            conf=profile[\\'conf_threshold\\'],\\n            # Note: decoder_layersëŠ” ëª¨ë¸ ì¬ì„¤ê³„ í•„ìš”\\n        )\\n\\n        return results\\n\\n    def auto_performance_mode(self, image_size, system_load):\\n        \\\"\\\"\\\"ì‹œìŠ¤í…œ ìƒí™©ì— ë”°ë¥¸ ìë™ ì„±ëŠ¥ ëª¨ë“œ ì„ íƒ\\\"\\\"\\\"\\n        if system_load > 0.8 or image_size > (2000, 2000):\\n            return \\'fast\\'\\n        elif system_load < 0.3 and image_size < (1000, 1000):\\n            return \\'accurate\\'\\n        else:\\n            return \\'balanced\\'\\n\\`\\`\\`\\n\\n### **3. í–¥ìƒëœ Voting ì‹œìŠ¤í…œ**\\n\\`\\`\\`python\\nclass EnhancedVotingWithRTDETR:\\n    def __init__(self):\\n        self.confidence_weights = {\\n            \\'rtdetr\\': 1.3,  # Transformer ê¸°ë°˜ìœ¼ë¡œ ë†’ì€ ì‹ ë¢°ì„±\\n            \\'yolo_v11x\\': 1.2,\\n            \\'detectron2\\': 1.1,\\n            \\'yolo_v11l\\': 1.0,\\n            \\'yolo_v8\\': 0.9\\n        }\\n\\n    def weighted_ensemble_voting(self, all_detections):\\n        \\\"\\\"\\\"ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì•™ìƒë¸” íˆ¬í‘œ\\\"\\\"\\\"\\n        detection_groups = self.group_detections_by_iou(all_detections)\\n\\n        final_detections = []\\n        for group in detection_groups:\\n            if len(group) >= 2:  # ìµœì†Œ 2ê°œ ëª¨ë¸ ë™ì˜\\n                # RT-DETRì˜ ë†’ì€ ì •í™•ë„ ê³ ë ¤\\n                best_detection = self.select_best_detection(group)\\n\\n                # ì‹ ë¢°ë„ ê°€ì¤‘ í‰ê·  ê³„ì‚°\\n                weighted_confidence = self.calculate_weighted_confidence(group)\\n\\n                # ìµœì¢… ê²€ì¶œ ê²°ê³¼\\n                final_detection = best_detection.copy()\\n                final_detection[\\'confidence\\'] = weighted_confidence\\n                final_detection[\\'voter_count\\'] = len(group)\\n                final_detection[\\'voting_models\\'] = [d[\\'model_id\\'] for d in group]\\n\\n                final_detections.append(final_detection)\\n\\n        return final_detections\\n\\n    def select_best_detection(self, detection_group):\\n        \\\"\\\"\\\"ê·¸ë£¹ì—ì„œ ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²€ì¶œ ì„ íƒ\\\"\\\"\\\"\\n        best_score = 0\\n        best_detection = None\\n\\n        for detection in detection_group:\\n            model_weight = self.confidence_weights.get(detection[\\'model_id\\'], 1.0)\\n            weighted_score = detection[\\'confidence\\'] * model_weight\\n\\n            if weighted_score > best_score:\\n                best_score = weighted_score\\n                best_detection = detection\\n\\n        return best_detection\\n\\`\\`\\`\\n\\n## ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼\\n\\n### **RT-DETR ë„ì…ì˜ ì¥ì **\\n1. **í›„ì²˜ë¦¬ ìµœì í™”**: NMS ì—†ì´ ê¹”ë”í•œ ê²°ê³¼\\n2. **ì¼ê´€ëœ ì„±ëŠ¥**: íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ì˜ ì•ˆì •ì  ê²€ì¶œ\\n3. **ì‹¤ì‹œê°„ ì ì‘**: ìƒí™©ì— ë”°ë¥¸ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ ì¡°ì •\\n4. **ë†’ì€ ì •í™•ë„**: ê¸°ì¡´ YOLO ëŒ€ë¹„ í–¥ìƒëœ AP ì„±ëŠ¥\\n\\n### **ì •ëŸ‰ì  ê°œì„  ì˜ˆìƒ**\\n\\`\\`\\`python\\n# ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ\\nperformance_improvement = {\\n    \\'detection_accuracy\\': \\'+3~5%\\',  # AP ê¸°ì¤€\\n    \\'false_positive_rate\\': \\'-10~15%\\',  # NMS-free íš¨ê³¼\\n    \\'inference_consistency\\': \\'+20%\\',  # ì¼ê´€ëœ ê²°ê³¼\\n    \\'post_processing_time\\': \\'-50%\\',  # NMS ì œê±°\\n}\\n\\`\\`\\`\\n\\n## ğŸ¯ ì‹¤ì œ ì ìš© ì‹œë‚˜ë¦¬ì˜¤\\n\\n### **ì‹œë‚˜ë¦¬ì˜¤ 1: ê³ ì •ë°€ ê²€ì¶œ ëª¨ë“œ**\\n\\`\\`\\`python\\ndef precision_focused_detection(image):\\n    \\\"\\\"\\\"ì •ë°€ë„ ì¤‘ì‹¬ì˜ ê²€ì¶œ (ëŠë¦¬ì§€ë§Œ ì •í™•)\\\"\\\"\\\"\\n    # RT-DETR + SAM 2 ì¡°í•©\\n    rtdetr_results = rtdetr_model.detect(\\n        image,\\n        conf_threshold=0.15,  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì€ í›„ë³´\\n        decoder_layers=6      # ìµœëŒ€ ì •í™•ë„\\n    )\\n\\n    # SAM 2ë¡œ ì •ë°€ ì„¸ë¶„í™”\\n    refined_results = sam2_model.refine_detections(image, rtdetr_results)\\n\\n    return refined_results\\n\\`\\`\\`\\n\\n### **ì‹œë‚˜ë¦¬ì˜¤ 2: ì‹¤ì‹œê°„ ì²˜ë¦¬ ëª¨ë“œ**\\n\\`\\`\\`python\\ndef realtime_processing_mode(image):\\n    \\\"\\\"\\\"ì‹¤ì‹œê°„ ì²˜ë¦¬ ì¤‘ì‹¬ (ë¹ ë¥´ì§€ë§Œ í•©ë¦¬ì  ì •í™•ë„)\\\"\\\"\\\"\\n    # ë¹ ë¥¸ RT-DETR ì„¤ì •\\n    rtdetr_results = rtdetr_model.detect(\\n        image,\\n        conf_threshold=0.4,   # ë†’ì€ ì„ê³„ê°’ìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬\\n        decoder_layers=3      # ì¤„ì–´ë“  ë ˆì´ì–´ë¡œ ì†ë„ í–¥ìƒ\\n    )\\n\\n    # ê¸°ë³¸ í›„ì²˜ë¦¬ë§Œ ì ìš©\\n    processed_results = basic_post_processing(rtdetr_results)\\n\\n    return processed_results\\n\\`\\`\\`\\n\\n## âš ï¸ ì œí•œì‚¬í•­ ë° ê³ ë ¤ì‚¬í•­\\n\\n### **í˜„ì¬ ì œí•œì‚¬í•­**\\n1. **ëª¨ë¸ í¬ê¸°**: YOLO ëŒ€ë¹„ í° ëª¨ë¸ ì‚¬ì´ì¦ˆ\\n2. **ë©”ëª¨ë¦¬ ì‚¬ìš©**: íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ë¡œ ì¸í•œ ë†’ì€ ë©”ëª¨ë¦¬ ìš”êµ¬\\n3. **ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹**: ì‚°ì—… ë„ë©´ ë°ì´í„°ë¡œì˜ íŒŒì¸íŠœë‹ í•„ìš”\\n\\n### **í•´ê²° ì „ëµ**\\n\\`\\`\\`python\\n# ë©”ëª¨ë¦¬ íš¨ìœ¨ì  RT-DETR ì‚¬ìš©\\nclass EfficientRTDETR:\\n    def __init__(self):\\n        self.model = None\\n        self.current_mode = None\\n\\n    def load_model_on_demand(self, mode=\\'balanced\\'):\\n        \\\"\\\"\\\"í•„ìš”ì‹œì—ë§Œ ëª¨ë¸ ë¡œë“œ\\\"\\\"\\\"\\n        if self.current_mode != mode:\\n            # ê¸°ì¡´ ëª¨ë¸ í•´ì œ\\n            if self.model is not None:\\n                del self.model\\n                torch.cuda.empty_cache()\\n\\n            # ìƒˆ ëª¨ë¸ ë¡œë“œ\\n            model_size = self.get_model_size_for_mode(mode)\\n            self.model = RTDETR(model_size)\\n            self.current_mode = mode\\n\\n    def batch_inference_optimization(self, images):\\n        \\\"\\\"\\\"ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± í–¥ìƒ\\\"\\\"\\\"\\n        # ì´ë¯¸ì§€ í¬ê¸°ë³„ ê·¸ë£¹í™”\\n        size_groups = self.group_images_by_size(images)\\n\\n        all_results = []\\n        for size, image_batch in size_groups.items():\\n            # ê°™ì€ í¬ê¸° ì´ë¯¸ì§€ë“¤ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬\\n            batch_results = self.model(image_batch)\\n            all_results.extend(batch_results)\\n\\n        return all_results\\n\\`\\`\\`\\n\\n## ğŸ”® í–¥í›„ ë°œì „ ë°©í–¥\\n\\n### **RT-DETRì˜ ë¯¸ë˜**\\n1. **ê²½ëŸ‰í™”**: Mobile/Edge í™˜ê²½ìš© RT-DETR-Nano\\n2. **ë‹¤ì¤‘ ëª¨ë‹¬**: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì§€ì› (DINO-Xì™€ ìœ ì‚¬)\\n3. **3D í™•ì¥**: 3D ê°ì²´ ê²€ì¶œë¡œì˜ í™•ì¥\\n\\n### **í†µí•© ë¡œë“œë§µ**\\n- **1ê°œì›”**: RT-DETR ê¸°ë³¸ í†µí•© ë° í…ŒìŠ¤íŠ¸\\n- **2ê°œì›”**: 5-ëª¨ë¸ Voting ì‹œìŠ¤í…œ ìµœì í™”\\n- **3ê°œì›”**: ì‚°ì—… ë„ë©´ íŠ¹í™” íŒŒì¸íŠœë‹\\n- **6ê°œì›”**: ì‹¤ì‹œê°„ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì‹œìŠ¤í…œ ì™„ì„±\\n\\n## ğŸ“š ì¶”ê°€ ìë£Œ\\n\\n### **ê³µì‹ ë¬¸ì„œ**\\n- [RT-DETR ë…¼ë¬¸ (CVPR 2024)](https://arxiv.org/abs/2304.08069)\\n- [Ultralytics RT-DETR ë¬¸ì„œ](https://docs.ultralytics.com/models/rtdetr/)\\n- [PaddlePaddle RT-DETR](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/rtdetr)\\n\\n### **ë²¤ì¹˜ë§ˆí¬ ë° ë¹„êµ**\\n- [Papers with Code - RT-DETR](https://paperswithcode.com/paper/detrs-beat-yolos-on-real-time-object)\\n- [RT-DETR vs YOLOv8 ë¹„êµ](https://docs.ultralytics.com/compare/rtdetr-vs-yolov8/)\\n- [ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬](https://github.com/lyuwenyu/RT-DETR#results)\\n\\n---\\n\\n**ê²°ë¡ **: RT-DETRì€ **íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì •í™•ì„±**ê³¼ **ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥**ì„ ê²°í•©í•œ í˜ì‹ ì  ëª¨ë¸ë¡œ, DrawingBOMExtractorì˜ **5ë²ˆì§¸ ëª¨ë¸**ë¡œ ì¶”ê°€í•˜ì—¬ ì „ì²´ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í•œ ë‹¨ê³„ ëŒì–´ì˜¬ë¦´ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.";
        markdownContent['yolov11'] = "# ğŸ¯ YOLOv11: ì°¨ì„¸ëŒ€ ì‹¤ì‹œê°„ ê°ì²´ ê²€ì¶œ\\n\\n## ğŸ“‹ ê°œìš”\\n\\n**YOLOv11**ì€ 2024ë…„ì— ì¶œì‹œëœ Ultralytics YOLO ì‹œë¦¬ì¦ˆì˜ ìµœì‹  ë²„ì „ìœ¼ë¡œ, **í–¥ìƒëœ ì •í™•ë„ì™€ íš¨ìœ¨ì„±**ì„ ìë‘í•˜ëŠ” ì°¨ì„¸ëŒ€ ì‹¤ì‹œê°„ ê°ì²´ ê²€ì¶œ ëª¨ë¸ì…ë‹ˆë‹¤. í˜„ì¬ DrawingBOMExtractorì—ì„œ ì´ë¯¸ í™œìš©í•˜ê³  ìˆì§€ë§Œ, ìµœì‹  ê¸°ëŠ¥ê³¼ ìµœì í™” ë°©ë²•ì„ ì¶”ê°€ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” ì—¬ì§€ê°€ ë§ìŠµë‹ˆë‹¤.\\n\\n## ğŸš€ í•µì‹¬ íŠ¹ì§•\\n\\n### 1. **ê°œì„ ëœ ì•„í‚¤í…ì²˜**\\n- **í–¥ìƒëœ ë°±ë³¸**: ë” íš¨ìœ¨ì ì¸ íŠ¹ì§• ì¶”ì¶œ\\n- **ìµœì í™”ëœ FPN**: ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ìœµí•© ê°œì„ \\n- **ê²½ëŸ‰í™”**: íŒŒë¼ë¯¸í„° ìˆ˜ ê°ì†Œì™€ ì„±ëŠ¥ í–¥ìƒì˜ ê· í˜•\\n\\n### 2. **ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°**\\n- **YOLOv11n**: Nano - ì´ˆê²½ëŸ‰ ëª¨ë¸\\n- **YOLOv11s**: Small - ê· í˜•ì¡íŒ ì„±ëŠ¥\\n- **YOLOv11m**: Medium - ì¤‘ê°„ ì„±ëŠ¥\\n- **YOLOv11l**: Large - ë†’ì€ ì •í™•ë„ (í˜„ì¬ ì‚¬ìš© ì¤‘)\\n- **YOLOv11x**: Extra Large - ìµœê³  ì •í™•ë„ (í˜„ì¬ ì‚¬ìš© ì¤‘)\\n\\n### 3. **ë©€í‹°íƒœìŠ¤í¬ ì§€ì›**\\n- **ê²€ì¶œ**: ê°ì²´ ìœ„ì¹˜ì™€ í´ë˜ìŠ¤ ì˜ˆì¸¡\\n- **ì„¸ë¶„í™”**: ì¸ìŠ¤í„´ìŠ¤ ë° ì˜ë¯¸ì  ë¶„í• \\n- **ë¶„ë¥˜**: ì´ë¯¸ì§€ ë¶„ë¥˜\\n- **í¬ì¦ˆ ì¶”ì •**: í‚¤í¬ì¸íŠ¸ ê²€ì¶œ\\n\\n## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ\\n\\n### **COCO ë²¤ì¹˜ë§ˆí¬ ì„±ê³¼**\\n\\`\\`\\`\\nğŸ† YOLOv11n: 39.5 AP @ 1.55M params\\nğŸ† YOLOv11s: 47.0 AP @ 9.4M params\\nğŸ† YOLOv11m: 51.5 AP @ 20.1M params\\nğŸ† YOLOv11l: 53.4 AP @ 25.3M params (í˜„ì¬ ì‚¬ìš©)\\nğŸ† YOLOv11x: 54.7 AP @ 56.9M params (í˜„ì¬ ì‚¬ìš©)\\n\\`\\`\\`\\n\\n### **ê¸°ì¡´ ë²„ì „ ëŒ€ë¹„ ê°œì„ **\\n| ëª¨ë¸ | YOLOv8 AP | YOLOv11 AP | ê°œì„ ìœ¨ |\\n|------|-----------|------------|--------|\\n| Small | 44.9 | 47.0 | +2.1 |\\n| Medium | 50.2 | 51.5 | +1.3 |\\n| Large | 52.9 | 53.4 | +0.5 |\\n| XLarge | 53.9 | 54.7 | +0.8 |\\n\\n## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ê°œì„ ì‚¬í•­\\n\\n### **ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ ìµœì í™”**\\n\\`\\`\\`python\\n# YOLOv11ì˜ ê°œì„ ëœ ë°±ë³¸ êµ¬ì¡° (ê°œë…ì )\\nclass YOLOv11Backbone(nn.Module):\\n    def __init__(self, input_channels=3):\\n        super().__init__()\\n\\n        # ê°œì„ ëœ ì´ˆê¸° ì»¨ë³¼ë£¨ì…˜\\n        self.stem = ImprovedStemBlock(input_channels, 64)\\n\\n        # íš¨ìœ¨ì ì¸ íŠ¹ì§• ì¶”ì¶œ ë¸”ë¡ë“¤\\n        self.stage1 = EfficientBlock(64, 128, num_blocks=3)\\n        self.stage2 = EfficientBlock(128, 256, num_blocks=6)\\n        self.stage3 = EfficientBlock(256, 512, num_blocks=9)\\n        self.stage4 = EfficientBlock(512, 1024, num_blocks=3)\\n\\n        # í¬ë¡œìŠ¤ ìŠ¤í…Œì´ì§€ íŒŒì…œ ì—°ê²° (CSP)\\n        self.csp_connections = CSPConnections()\\n\\n    def forward(self, x):\\n        # ê³„ì¸µì  íŠ¹ì§• ì¶”ì¶œ\\n        x = self.stem(x)\\n\\n        f1 = self.stage1(x)\\n        f2 = self.stage2(f1)\\n        f3 = self.stage3(f2)\\n        f4 = self.stage4(f3)\\n\\n        # CSP ì—°ê²°ë¡œ íŠ¹ì§• ìœµí•©\\n        enhanced_features = self.csp_connections([f2, f3, f4])\\n\\n        return enhanced_features\\n\\`\\`\\`\\n\\n### **ê°œì„ ëœ FPN (Feature Pyramid Network)**\\n\\`\\`\\`python\\nclass EnhancedFPN(nn.Module):\\n    def __init__(self, in_channels_list, out_channels):\\n        super().__init__()\\n\\n        # ìƒí–¥ì‹ ê²½ë¡œ (Bottom-up pathway)\\n        self.lateral_convs = nn.ModuleList([\\n            nn.Conv2d(in_ch, out_channels, 1)\\n            for in_ch in in_channels_list\\n        ])\\n\\n        # í•˜í–¥ì‹ ê²½ë¡œ (Top-down pathway)\\n        self.fpn_convs = nn.ModuleList([\\n            nn.Conv2d(out_channels, out_channels, 3, padding=1)\\n            for _ in in_channels_list\\n        ])\\n\\n        # PANet ìŠ¤íƒ€ì¼ ìƒí–¥ì‹ ê°•í™”\\n        self.panet_convs = nn.ModuleList([\\n            nn.Conv2d(out_channels, out_channels, 3, padding=1)\\n            for _ in in_channels_list\\n        ])\\n\\n    def forward(self, inputs):\\n        # ê¸°ë³¸ FPN ì²˜ë¦¬\\n        laterals = [conv(feat) for conv, feat in zip(self.lateral_convs, inputs)]\\n\\n        # Top-down ê²½ë¡œ\\n        for i in range(len(laterals) - 2, -1, -1):\\n            laterals[i] = laterals[i] + F.interpolate(\\n                laterals[i + 1], scale_factor=2, mode=\\'nearest\\'\\n            )\\n\\n        # FPN ì¶œë ¥\\n        fpn_outs = [conv(lateral) for conv, lateral in zip(self.fpn_convs, laterals)]\\n\\n        # PANet ìƒí–¥ì‹ ê°•í™”\\n        for i in range(1, len(fpn_outs)):\\n            fpn_outs[i] = fpn_outs[i] + F.max_pool2d(fpn_outs[i - 1], 2, 2)\\n            fpn_outs[i] = self.panet_convs[i](fpn_outs[i])\\n\\n        return fpn_outs\\n\\`\\`\\`\\n\\n## ğŸ”§ í˜„ì¬ ì‹œìŠ¤í…œ ìµœì í™” ë°©ì•ˆ\\n\\n### **1. ëª¨ë¸ ì•™ìƒë¸” ê°œì„ **\\n\\`\\`\\`python\\nclass OptimizedYOLOv11Ensemble:\\n    def __init__(self):\\n        # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ë“¤ ìµœì í™”\\n        self.models = {\\n            \\'yolov11l\\': self.load_optimized_model(\\'yolov11l.pt\\'),\\n            \\'yolov11x\\': self.load_optimized_model(\\'yolov11x.pt\\'),\\n        }\\n\\n        # TensorRT ìµœì í™” ì ìš©\\n        self.tensorrt_enabled = self.check_tensorrt_availability()\\n\\n    def load_optimized_model(self, model_path):\\n        \\\"\\\"\\\"ìµœì í™”ëœ ëª¨ë¸ ë¡œë”©\\\"\\\"\\\"\\n        model = YOLO(model_path)\\n\\n        # FP16 ì •ë°€ë„ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”\\n        if torch.cuda.is_available():\\n            model.model.half()\\n\\n        # torch.compile ì ìš© (PyTorch 2.0+)\\n        if hasattr(torch, \\'compile\\'):\\n            model.model = torch.compile(\\n                model.model,\\n                mode=\\'max-autotune\\'\\n            )\\n\\n        return model\\n\\n    def batch_inference(self, images, batch_size=4):\\n        \\\"\\\"\\\"ë°°ì¹˜ ì¶”ë¡ ìœ¼ë¡œ íš¨ìœ¨ì„± í–¥ìƒ\\\"\\\"\\\"\\n        results = []\\n\\n        for i in range(0, len(images), batch_size):\\n            batch = images[i:i+batch_size]\\n\\n            # ë‘ ëª¨ë¸ ë³‘ë ¬ ì‹¤í–‰\\n            yolov11l_results = self.models[\\'yolov11l\\'](batch)\\n            yolov11x_results = self.models[\\'yolov11x\\'](batch)\\n\\n            # ê²°ê³¼ í†µí•©\\n            combined_results = self.merge_model_results(\\n                yolov11l_results, yolov11x_results\\n            )\\n            results.extend(combined_results)\\n\\n        return results\\n\\`\\`\\`\\n\\n### **2. ë°ì´í„° ì¦ê°• ë° ì „ì²˜ë¦¬ ìµœì í™”**\\n\\`\\`\\`python\\nclass CADSpecificAugmentation:\\n    def __init__(self):\\n        self.augmentations = {\\n            \\'geometric\\': [\\n                \\'rotate_90\\',    # 90ë„ íšŒì „ (ë„ë©´ ë°©í–¥ ë³€ê²½)\\n                \\'flip_h\\',       # ìˆ˜í‰ ë°˜ì „\\n                \\'flip_v\\',       # ìˆ˜ì§ ë°˜ì „\\n                \\'scale_0.8_1.2\\' # ìŠ¤ì¼€ì¼ë§\\n            ],\\n            \\'photometric\\': [\\n                \\'brightness_0.7_1.3\\',  # ë°ê¸° ì¡°ì •\\n                \\'contrast_0.8_1.2\\',    # ëŒ€ë¹„ ì¡°ì •\\n                \\'noise_gaussian\\'        # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ\\n            ],\\n            \\'cad_specific\\': [\\n                \\'line_thickness_var\\',   # ì„  ë‘ê»˜ ë³€í™”\\n                \\'grid_overlay\\',         # ê²©ì ì˜¤ë²„ë ˆì´\\n                \\'paper_texture\\'         # ì¢…ì´ ì§ˆê°\\n            ]\\n        }\\n\\n    def apply_cad_augmentation(self, image, annotations):\\n        \\\"\\\"\\\"CAD ë„ë©´ íŠ¹í™” ë°ì´í„° ì¦ê°•\\\"\\\"\\\"\\n        augmented_images = []\\n        augmented_annotations = []\\n\\n        # ê¸°ë³¸ ì´ë¯¸ì§€\\n        augmented_images.append(image)\\n        augmented_annotations.append(annotations)\\n\\n        # 90ë„ ë‹¨ìœ„ íšŒì „ (CAD ë„ë©´ì˜ ë°©í–¥ì„± ê³ ë ¤)\\n        for angle in [90, 180, 270]:\\n            rotated_img, rotated_ann = self.rotate_image_and_annotations(\\n                image, annotations, angle\\n            )\\n            augmented_images.append(rotated_img)\\n            augmented_annotations.append(rotated_ann)\\n\\n        # ì„  ë‘ê»˜ ë³€í™” (ë‹¤ì–‘í•œ ìŠ¤ìº” í’ˆì§ˆ ì‹œë®¬ë ˆì´ì…˜)\\n        thick_line_img = self.vary_line_thickness(image, factor=1.2)\\n        thin_line_img = self.vary_line_thickness(image, factor=0.8)\\n\\n        augmented_images.extend([thick_line_img, thin_line_img])\\n        augmented_annotations.extend([annotations, annotations])\\n\\n        return augmented_images, augmented_annotations\\n\\`\\`\\`\\n\\n### **3. ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**\\n\\`\\`\\`python\\nclass YOLOv11PerformanceMonitor:\\n    def __init__(self):\\n        self.metrics = {\\n            \\'inference_times\\': [],\\n            \\'memory_usage\\': [],\\n            \\'gpu_utilization\\': [],\\n            \\'accuracy_scores\\': []\\n        }\\n\\n    def monitor_inference(self, model, image):\\n        \\\"\\\"\\\"ì¶”ë¡  ì„±ëŠ¥ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§\\\"\\\"\\\"\\n        start_time = time.time()\\n        start_memory = torch.cuda.memory_allocated()\\n\\n        # GPU ì‚¬ìš©ë¥  ì¸¡ì • ì‹œì‘\\n        gpu_monitor = GPUMonitor()\\n        gpu_monitor.start()\\n\\n        # ëª¨ë¸ ì¶”ë¡ \\n        with torch.cuda.amp.autocast():  # Mixed precision\\n            results = model(image)\\n\\n        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘\\n        end_time = time.time()\\n        end_memory = torch.cuda.memory_allocated()\\n        gpu_utilization = gpu_monitor.stop()\\n\\n        # ë©”íŠ¸ë¦­ ì €ì¥\\n        self.metrics[\\'inference_times\\'].append(end_time - start_time)\\n        self.metrics[\\'memory_usage\\'].append(end_memory - start_memory)\\n        self.metrics[\\'gpu_utilization\\'].append(gpu_utilization)\\n\\n        return results\\n\\n    def get_performance_report(self):\\n        \\\"\\\"\\\"ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±\\\"\\\"\\\"\\n        return {\\n            \\'avg_inference_time\\': np.mean(self.metrics[\\'inference_times\\']),\\n            \\'memory_efficiency\\': np.mean(self.metrics[\\'memory_usage\\']),\\n            \\'gpu_utilization\\': np.mean(self.metrics[\\'gpu_utilization\\']),\\n            \\'throughput\\': 1.0 / np.mean(self.metrics[\\'inference_times\\'])\\n        }\\n\\`\\`\\`\\n\\n## ğŸ¯ CAD ì‹¬ë³¼ ê²€ì¶œ íŠ¹í™” ê°œì„ \\n\\n### **1. í´ë˜ìŠ¤ë³„ ìµœì í™”**\\n\\`\\`\\`python\\nclass CADSymbolOptimizer:\\n    def __init__(self):\\n        # ì‚°ì—… ë¶€í’ˆë³„ íŠ¹í™” ì„¤ì •\\n        self.class_specific_settings = {\\n            \\'transformer\\': {\\n                \\'conf_threshold\\': 0.35,  # ë³€ì••ê¸°ëŠ” ëª…í™•í•œ í˜•íƒœ\\n                \\'nms_threshold\\': 0.45,\\n                \\'augmentation_strength\\': 0.8\\n            },\\n            \\'motor\\': {\\n                \\'conf_threshold\\': 0.25,  # ëª¨í„°ëŠ” ë³€í˜•ì´ ë§ìŒ\\n                \\'nms_threshold\\': 0.5,\\n                \\'augmentation_strength\\': 1.2\\n            },\\n            \\'switch\\': {\\n                \\'conf_threshold\\': 0.4,   # ìŠ¤ìœ„ì¹˜ëŠ” ë‹¨ìˆœí•œ í˜•íƒœ\\n                \\'nms_threshold\\': 0.3,\\n                \\'augmentation_strength\\': 0.6\\n            },\\n            # ... ê¸°íƒ€ 27ê°œ í´ë˜ìŠ¤\\n        }\\n\\n    def optimize_for_symbol_class(self, model, symbol_class):\\n        \\\"\\\"\\\"íŠ¹ì • ì‹¬ë³¼ í´ë˜ìŠ¤ì— ìµœì í™”ëœ ì„¤ì • ì ìš©\\\"\\\"\\\"\\n        settings = self.class_specific_settings.get(symbol_class, {})\\n\\n        # í´ë˜ìŠ¤ë³„ ì„ê³„ê°’ ì¡°ì •\\n        model.conf = settings.get(\\'conf_threshold\\', 0.25)\\n        model.iou = settings.get(\\'nms_threshold\\', 0.45)\\n\\n        return model\\n\\`\\`\\`\\n\\n### **2. ë©€í‹°ìŠ¤ì¼€ì¼ ê²€ì¶œ ê°•í™”**\\n\\`\\`\\`python\\nclass MultiScaleCADDetector:\\n    def __init__(self):\\n        self.scale_factors = [0.5, 0.75, 1.0, 1.25, 1.5]\\n        self.models = {\\n            \\'yolov11l\\': YOLO(\\'yolov11l.pt\\'),\\n            \\'yolov11x\\': YOLO(\\'yolov11x.pt\\')\\n        }\\n\\n    def multi_scale_inference(self, image):\\n        \\\"\\\"\\\"ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì¶”ë¡ ìœ¼ë¡œ í¬ê¸° ë³€í™”ì— ê°•ì¸í•œ ê²€ì¶œ\\\"\\\"\\\"\\n        all_detections = []\\n\\n        original_size = image.shape[:2]\\n\\n        for scale in self.scale_factors:\\n            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •\\n            scaled_size = (int(original_size[0] * scale),\\n                          int(original_size[1] * scale))\\n            scaled_image = cv2.resize(image, scaled_size[::-1])\\n\\n            # ê° ëª¨ë¸ë¡œ ê²€ì¶œ\\n            for model_name, model in self.models.items():\\n                detections = model(scaled_image)\\n\\n                # ì›ë³¸ í¬ê¸°ë¡œ ì¢Œí‘œ ë³€í™˜\\n                scaled_detections = self.scale_detections_back(\\n                    detections, scale, original_size\\n                )\\n\\n                # ë©”íƒ€ë°ì´í„° ì¶”ê°€\\n                for det in scaled_detections:\\n                    det[\\'model_id\\'] = model_name\\n                    det[\\'scale_factor\\'] = scale\\n\\n                all_detections.extend(scaled_detections)\\n\\n        # ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ê²°ê³¼ í†µí•©\\n        final_detections = self.merge_multiscale_detections(all_detections)\\n\\n        return final_detections\\n\\`\\`\\`\\n\\n## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” ì „ëµ\\n\\n### **1. TensorRT ê°€ì†í™”**\\n\\`\\`\\`python\\nclass TensorRTOptimizer:\\n    def __init__(self):\\n        self.engine_cache = {}\\n\\n    def convert_to_tensorrt(self, model_path, precision=\\'fp16\\'):\\n        \\\"\\\"\\\"YOLOv11 ëª¨ë¸ì„ TensorRTë¡œ ë³€í™˜\\\"\\\"\\\"\\n        model = YOLO(model_path)\\n\\n        # TensorRT ì—”ì§„ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°\\n        engine_path = model_path.replace(\\'.pt\\', f\\'_{precision}.engine\\')\\n\\n        model.export(\\n            format=\\'engine\\',\\n            half=True if precision == \\'fp16\\' else False,\\n            dynamic=False,  # ê³ ì • ì…ë ¥ í¬ê¸°ë¡œ ìµœì í™”\\n            workspace=4,    # 4GB ì›Œí¬ìŠ¤í˜ì´ìŠ¤\\n        )\\n\\n        return engine_path\\n\\n    def load_tensorrt_model(self, engine_path):\\n        \\\"\\\"\\\"TensorRT ì—”ì§„ ë¡œë“œ ë° ì¶”ë¡ \\\"\\\"\\\"\\n        if engine_path not in self.engine_cache:\\n            model = YOLO(engine_path)\\n            self.engine_cache[engine_path] = model\\n\\n        return self.engine_cache[engine_path]\\n\\`\\`\\`\\n\\n### **2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ **\\n\\`\\`\\`python\\nclass MemoryEfficientYOLO:\\n    def __init__(self):\\n        self.model = None\\n        self.current_batch_size = 1\\n\\n    def adaptive_batch_processing(self, images):\\n        \\\"\\\"\\\"ë©”ëª¨ë¦¬ ìƒí™©ì— ë”°ë¥¸ ì ì‘ì  ë°°ì¹˜ ì²˜ë¦¬\\\"\\\"\\\"\\n        available_memory = torch.cuda.get_device_properties(0).total_memory\\n        used_memory = torch.cuda.memory_allocated()\\n        free_memory = available_memory - used_memory\\n\\n        # ì—¬ìœ  ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •\\n        if free_memory > 8e9:  # 8GB ì´ìƒ\\n            batch_size = 8\\n        elif free_memory > 4e9:  # 4GB ì´ìƒ\\n            batch_size = 4\\n        elif free_memory > 2e9:  # 2GB ì´ìƒ\\n            batch_size = 2\\n        else:\\n            batch_size = 1\\n\\n        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬\\n        results = []\\n        for i in range(0, len(images), batch_size):\\n            batch = images[i:i+batch_size]\\n\\n            with torch.cuda.amp.autocast():  # Mixed precision\\n                batch_results = self.model(batch)\\n\\n            results.extend(batch_results)\\n\\n            # ì¤‘ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬\\n            torch.cuda.empty_cache()\\n\\n        return results\\n\\`\\`\\`\\n\\n## ğŸ”® YOLOv11ì˜ ë¯¸ë˜ ë°œì „\\n\\n### **í–¥í›„ ì—…ë°ì´íŠ¸ ì˜ˆìƒ**\\n1. **ë” íš¨ìœ¨ì ì¸ ì•„í‚¤í…ì²˜**: MobileNet, EfficientNet ê¸°ë°˜ ê²½ëŸ‰í™”\\n2. **Vision Transformer í†µí•©**: CNN + Transformer í•˜ì´ë¸Œë¦¬ë“œ\\n3. **3D ê²€ì¶œ ì§€ì›**: 3D ë°”ìš´ë”© ë°•ìŠ¤ ì˜ˆì¸¡\\n4. **ì‹¤ì‹œê°„ ì„¸ë¶„í™”**: YOLO + SAM í†µí•© ëª¨ë¸\\n\\n### **ì§€ì†ì  ìµœì í™” ë°©í–¥**\\n\\`\\`\\`python\\n# ë¯¸ë˜ YOLOv11 ìµœì í™” ë°©í–¥\\nfuture_optimizations = {\\n    \\'architecture\\': [\\n        \\'attention_mechanisms\\',      # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ í†µí•©\\n        \\'neural_architecture_search\\', # NAS ê¸°ë°˜ ìë™ ì„¤ê³„\\n        \\'pruning_and_quantization\\'   # ëª¨ë¸ ê²½ëŸ‰í™”\\n    ],\\n    \\'training\\': [\\n        \\'self_supervised_learning\\',  # ìê¸°ì§€ë„ í•™ìŠµ\\n        \\'active_learning\\',          # ëŠ¥ë™ í•™ìŠµ\\n        \\'few_shot_learning\\'         # ì†Œìƒ· í•™ìŠµ\\n    ],\\n    \\'deployment\\': [\\n        \\'edge_optimization\\',        # ì—£ì§€ ë””ë°”ì´ìŠ¤ ìµœì í™”\\n        \\'real_time_adaptation\\',     # ì‹¤ì‹œê°„ ëª¨ë¸ ì ì‘\\n        \\'distributed_inference\\'     # ë¶„ì‚° ì¶”ë¡ \\n    ]\\n}\\n\\`\\`\\`\\n\\n## ğŸ“š ê´€ë ¨ ìë£Œ\\n\\n### **ê³µì‹ ë¬¸ì„œ ë° ìë£Œ**\\n- [Ultralytics YOLOv11 ê³µì‹ ë¬¸ì„œ](https://docs.ultralytics.com/models/yolo11/)\\n- [YOLOv11 GitHub ì €ì¥ì†Œ](https://github.com/ultralytics/ultralytics)\\n- [YOLOv11 ë…¼ë¬¸ (ì¶œê°„ ì˜ˆì •)](https://arxiv.org/list/cs.CV/recent)\\n\\n### **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**\\n- [COCO ë¦¬ë”ë³´ë“œ](https://cocodataset.org/#detection-leaderboard)\\n- [Papers with Code - Object Detection](https://paperswithcode.com/task/object-detection)\\n- [YOLOv11 vs ê¸°ì¡´ ëª¨ë¸ ë¹„êµ](https://github.com/ultralytics/ultralytics#performance)\\n\\n### **ìµœì í™” ë„êµ¬**\\n- [TensorRT](https://developer.nvidia.com/tensorrt) - NVIDIA GPU ê°€ì†í™”\\n- [OpenVINO](https://docs.openvino.ai/) - Intel í•˜ë“œì›¨ì–´ ìµœì í™”\\n- [ONNX Runtime](https://onnxruntime.ai/) - í¬ë¡œìŠ¤ í”Œë«í¼ ì¶”ë¡ \\n\\n---\\n\\n**ê²°ë¡ **: YOLOv11ì€ ì´ë¯¸ DrawingBOMExtractorì˜ í•µì‹¬ êµ¬ì„±ìš”ì†Œë¡œ í™œìš©ë˜ê³  ìˆì§€ë§Œ, **ì¶”ê°€ ìµœì í™”ì™€ ê³ ê¸‰ ê¸°ëŠ¥**ì„ í†µí•´ ë”ìš± í–¥ìƒëœ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ **TensorRT ê°€ì†í™”, ë©€í‹°ìŠ¤ì¼€ì¼ ê²€ì¶œ, í´ë˜ìŠ¤ë³„ ìµœì í™”**ë¥¼ í†µí•´ í•œ ë‹¨ê³„ ì§„ë³´ëœ CAD ì‹¬ë³¼ ê²€ì¶œ ì‹œìŠ¤í…œì„ êµ¬í˜„í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.";
        markdownContent['document-transformer'] = "# ğŸ“‹ Document Understanding Transformer: ë„ë©´ ì •ë³´ ì¶”ì¶œ íŠ¹í™”\\n\\n## ğŸ“‹ ê°œìš”\\n\\n**Document Understanding Transformer**ëŠ” 2025ë…„ 1ì›”ì— ë°œí‘œëœ ìµœì‹  ì—°êµ¬ë¡œ, **YOLOv11ê³¼ Donut ëª¨ë¸ì„ í•˜ì´ë¸Œë¦¬ë“œë¡œ ê²°í•©**í•˜ì—¬ 2D ì—”ì§€ë‹ˆì–´ë§ ë„ë©´ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ìë™ ì¶”ì¶œí•˜ëŠ” íŠ¹í™” ëª¨ë¸ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë‹¨ìˆœí•œ ê°ì²´ ê²€ì¶œì„ ë„›ì–´ **ì˜ë¯¸ìˆëŠ” ì •ë³´ ì¶”ì¶œ**ì— ì´ˆì ì„ ë§ì¶˜ í˜ì‹ ì  ì ‘ê·¼ë²•ì…ë‹ˆë‹¤.\\n\\n## ğŸš€ í•µì‹¬ íŠ¹ì§•\\n\\n### 1. **í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜**\\n- **Stage 1**: YOLOv11 ê¸°ë°˜ OBB(Oriented Bounding Box) ê²€ì¶œ\\n- **Stage 2**: Donut Transformer ê¸°ë°˜ ë¬¸ì„œ íŒŒì‹±\\n- **í†µí•© í”„ë ˆì„ì›Œí¬**: ê²€ì¶œê³¼ ì´í•´ì˜ ì™„ë²½í•œ ê²°í•©\\n\\n### 2. **9ê°€ì§€ í•µì‹¬ ì¹´í…Œê³ ë¦¬ ê²€ì¶œ**\\n- **GD&T**: ê¸°í•˜ê³µì°¨ (Geometric Dimensioning & Tolerancing)\\n- **General Tolerances**: ì¼ë°˜ ê³µì°¨\\n- **Measures**: ì¹˜ìˆ˜ ë° ì¸¡ì •ê°’\\n- **Materials**: ì¬ë£Œ ì •ë³´\\n- **Notes**: ì£¼ì„ ë° ì„¤ëª…\\n- **Radii**: ë°˜ì§€ë¦„ ì •ë³´\\n- **Surface Roughness**: í‘œë©´ ê±°ì¹ ê¸°\\n- **Threads**: ë‚˜ì‚¬ì‚° ì •ë³´\\n- **Title Blocks**: ì œëª©ë€\\n\\n### 3. **ë†’ì€ ì •í™•ë„**\\n- **ì •ë°€ë„**: 94.77% (GD&T ê¸°ì¤€)\\n- **ì¬í˜„ìœ¨**: 100% (ëŒ€ë¶€ë¶„ ì¹´í…Œê³ ë¦¬)\\n- **F1 ìŠ¤ì½”ì–´**: 97.3%\\n- **í™˜ê° ê°ì†Œ**: 5.23%ë¡œ ìµœì†Œí™”\\n\\n## ğŸ“Š ì„±ëŠ¥ ì§€í‘œ\\n\\n### **ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥**\\n\\`\\`\\`\\nğŸ¯ GD&T: 94.77% ì •ë°€ë„, 100% ì¬í˜„ìœ¨\\nğŸ“ Measures: 96.2% ì •ë°€ë„, 98.5% ì¬í˜„ìœ¨\\nğŸ“ Notes: 92.8% ì •ë°€ë„, 95.1% ì¬í˜„ìœ¨\\nğŸ”© Threads: 89.4% ì •ë°€ë„, 93.7% ì¬í˜„ìœ¨\\nğŸ“„ Title Blocks: 98.1% ì •ë°€ë„, 99.2% ì¬í˜„ìœ¨\\n\\`\\`\\`\\n\\n### **ì „ì²´ ì„±ëŠ¥ ì§€í‘œ**\\n| ì§€í‘œ | Single Model | Category-Specific | ê°œì„ ìœ¨ |\\n|------|--------------|------------------|--------|\\n| **ì •ë°€ë„** | **94.77%** | 91.23% | +3.54% |\\n| **ì¬í˜„ìœ¨** | **98.85%** | 96.12% | +2.73% |\\n| **F1 Score** | **97.3%** | 94.8% | +2.5% |\\n| **í™˜ê°ë¥ ** | **5.23%** | 8.91% | -3.68% |\\n\\n## ğŸ—ï¸ ì•„í‚¤í…ì²˜\\n\\n### **ì „ì²´ íŒŒì´í”„ë¼ì¸**\\n\\n\\`\\`\\`mermaid\\ngraph TB\\n    A[Engineering Drawing] --> B[YOLOv11 OBB Detection]\\n\\n    B --> C1[GD&T Regions]\\n    B --> C2[Measures Regions]\\n    B --> C3[Notes Regions]\\n    B --> C4[Materials Regions]\\n    B --> C5[Other Regions]\\n\\n    C1 --> D[Donut Transformer]\\n    C2 --> D\\n    C3 --> D\\n    C4 --> D\\n    C5 --> D\\n\\n    D --> E[Structured Information]\\n    E --> F[Enhanced BOM]\\n    E --> G[Quality Verification]\\n    E --> H[Design Compliance]\\n\\`\\`\\`\\n\\n### **í•µì‹¬ êµ¬ì„±ìš”ì†Œ**\\n\\n#### **1. OBB ê²€ì¶œ ëª¨ë“ˆ**\\n\\`\\`\\`python\\nclass OrientedBoundingBoxDetector:\\n    def __init__(self, model_path):\\n        self.yolo_model = YOLO(model_path)\\n        self.categories = [\\n            \\'gdt\\', \\'tolerances\\', \\'measures\\', \\'materials\\',\\n            \\'notes\\', \\'radii\\', \\'surface_roughness\\', \\'threads\\', \\'title_blocks\\'\\n        ]\\n\\n    def detect_information_regions(self, drawing_image):\\n        \\\"\\\"\\\"ë„ë©´ì—ì„œ ì •ë³´ ì˜ì—­ì„ OBBë¡œ ê²€ì¶œ\\\"\\\"\\\"\\n        results = self.yolo_model(drawing_image)\\n\\n        detected_regions = []\\n        for result in results:\\n            boxes = result.obb  # Oriented Bounding Boxes\\n\\n            for i, box in enumerate(boxes):\\n                region = {\\n                    \\'category\\': self.categories[int(box.cls[i])],\\n                    \\'confidence\\': float(box.conf[i]),\\n                    \\'obb_coords\\': box.xyxyxyxy[i].cpu().numpy(),  # 8ê°œ ì¢Œí‘œê°’\\n                    \\'angle\\': self.calculate_angle(box.xyxyxyxy[i]),\\n                    \\'region_image\\': self.extract_region(drawing_image, box)\\n                }\\n                detected_regions.append(region)\\n\\n        return detected_regions\\n\\n    def calculate_angle(self, obb_coords):\\n        \\\"\\\"\\\"OBBì˜ íšŒì „ ê°ë„ ê³„ì‚°\\\"\\\"\\\"\\n        # 8ê°œ ì¢Œí‘œì—ì„œ íšŒì „ ê°ë„ ì¶”ì¶œ\\n        points = obb_coords.reshape(4, 2)\\n        dx = points[1][0] - points[0][0]\\n        dy = points[1][1] - points[0][1]\\n        angle = np.arctan2(dy, dx) * 180 / np.pi\\n        return angle\\n\\n    def extract_region(self, image, obb):\\n        \\\"\\\"\\\"íšŒì „ëœ ë°”ìš´ë”© ë°•ìŠ¤ ì˜ì—­ ì¶”ì¶œ\\\"\\\"\\\"\\n        # íšŒì „ ë³€í™˜ ì ìš©í•˜ì—¬ ì •ê·œí™”ëœ ì´ë¯¸ì§€ ì¶”ì¶œ\\n        points = obb.xyxyxyxy.cpu().numpy().reshape(4, 2)\\n\\n        # ìµœì†Œ ì™¸ì ‘ ì‚¬ê°í˜• ê³„ì‚°\\n        rect = cv2.minAreaRect(points.astype(np.float32))\\n\\n        # íšŒì „ ë³€í™˜ í–‰ë ¬ ê³„ì‚°\\n        M = cv2.getRotationMatrix2D(rect[0], rect[2], 1)\\n\\n        # ì´ë¯¸ì§€ íšŒì „ ë° í¬ë¡­\\n        rotated = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))\\n        cropped = cv2.getRectSubPix(rotated,\\n                                  (int(rect[1][0]), int(rect[1][1])),\\n                                  rect[0])\\n\\n        return cropped\\n\\`\\`\\`\\n\\n#### **2. Donut Transformer íŒŒì‹±**\\n\\`\\`\\`python\\nfrom transformers import DonutProcessor, VisionEncoderDecoderModel\\n\\nclass DocumentParsingTransformer:\\n    def __init__(self, model_name=\\\"naver-clova-ix/donut-base-finetuned-docvqa\\\"):\\n        self.processor = DonutProcessor.from_pretrained(model_name)\\n        self.model = VisionEncoderDecoderModel.from_pretrained(model_name)\\n\\n        # ì—”ì§€ë‹ˆì–´ë§ ë„ë©´ íŠ¹í™” í”„ë¡¬í”„íŠ¸\\n        self.parsing_prompts = {\\n            \\'gdt\\': \\\"Extract geometric dimensioning and tolerancing information\\\",\\n            \\'measures\\': \\\"Extract dimensional measurements and values\\\",\\n            \\'materials\\': \\\"Extract material specifications and properties\\\",\\n            \\'notes\\': \\\"Extract notes and textual information\\\",\\n            \\'tolerances\\': \\\"Extract tolerance specifications\\\",\\n            \\'threads\\': \\\"Extract thread information and specifications\\\"\\n        }\\n\\n    def parse_information_regions(self, detected_regions):\\n        \\\"\\\"\\\"ê²€ì¶œëœ ì˜ì—­ë“¤ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ\\\"\\\"\\\"\\n        parsed_results = []\\n\\n        for region in detected_regions:\\n            category = region[\\'category\\']\\n            region_image = region[\\'region_image\\']\\n\\n            # ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” íŒŒì‹±\\n            parsed_info = self.parse_region_by_category(region_image, category)\\n\\n            result = {\\n                \\'category\\': category,\\n                \\'confidence\\': region[\\'confidence\\'],\\n                \\'angle\\': region[\\'angle\\'],\\n                \\'parsed_information\\': parsed_info,\\n                \\'raw_region\\': region\\n            }\\n            parsed_results.append(result)\\n\\n        return parsed_results\\n\\n    def parse_region_by_category(self, region_image, category):\\n        \\\"\\\"\\\"ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” ì •ë³´ íŒŒì‹±\\\"\\\"\\\"\\n        prompt = self.parsing_prompts.get(category, \\\"Extract information from image\\\")\\n\\n        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬\\n        pixel_values = self.processor(\\n            region_image,\\n            return_tensors=\\\"pt\\\"\\n        ).pixel_values\\n\\n        # í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©\\n        prompt_ids = self.processor.tokenizer(\\n            prompt,\\n            return_tensors=\\\"pt\\\",\\n            max_length=512,\\n            truncation=True\\n        ).input_ids\\n\\n        # Transformer ì¶”ë¡ \\n        generated_ids = self.model.generate(\\n            pixel_values,\\n            decoder_input_ids=prompt_ids,\\n            max_length=512,\\n            num_beams=5,\\n            early_stopping=True\\n        )\\n\\n        # ê²°ê³¼ ë””ì½”ë”©\\n        parsed_text = self.processor.batch_decode(\\n            generated_ids, skip_special_tokens=True\\n        )[0]\\n\\n        # êµ¬ì¡°í™”ëœ ì •ë³´ë¡œ ë³€í™˜\\n        structured_info = self.structure_parsed_text(parsed_text, category)\\n\\n        return structured_info\\n\\n    def structure_parsed_text(self, parsed_text, category):\\n        \\\"\\\"\\\"íŒŒì‹±ëœ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”ëœ ì •ë³´ë¡œ ë³€í™˜\\\"\\\"\\\"\\n        if category == \\'measures\\':\\n            return self.extract_dimensional_info(parsed_text)\\n        elif category == \\'materials\\':\\n            return self.extract_material_info(parsed_text)\\n        elif category == \\'tolerances\\':\\n            return self.extract_tolerance_info(parsed_text)\\n        elif category == \\'gdt\\':\\n            return self.extract_gdt_info(parsed_text)\\n        else:\\n            return {\\'raw_text\\': parsed_text}\\n\\n    def extract_dimensional_info(self, text):\\n        \\\"\\\"\\\"ì¹˜ìˆ˜ ì •ë³´ ì¶”ì¶œ ë° êµ¬ì¡°í™”\\\"\\\"\\\"\\n        import re\\n\\n        # ì¹˜ìˆ˜ íŒ¨í„´ ë§¤ì¹­\\n        dimension_patterns = [\\n            r\\'(\\\\d+\\\\.?\\\\d*)\\\\s*(?:mm|cm|m|inch|in|\\\")\\',  # ê¸¸ì´ ì¹˜ìˆ˜\\n            r\\'Ã˜\\\\s*(\\\\d+\\\\.?\\\\d*)\\',                      # ì§ê²½\\n            r\\'R\\\\s*(\\\\d+\\\\.?\\\\d*)\\',                      # ë°˜ì§€ë¦„\\n            r\\'(\\\\d+\\\\.?\\\\d*)\\\\s*Â°\\',                      # ê°ë„\\n        ]\\n\\n        dimensions = []\\n        for pattern in dimension_patterns:\\n            matches = re.findall(pattern, text, re.IGNORECASE)\\n            dimensions.extend([float(m) for m in matches])\\n\\n        return {\\n            \\'dimensions\\': dimensions,\\n            \\'unit\\': self.extract_unit(text),\\n            \\'type\\': \\'dimensional\\',\\n            \\'raw_text\\': text\\n        }\\n\\n    def extract_material_info(self, text):\\n        \\\"\\\"\\\"ì¬ë£Œ ì •ë³´ ì¶”ì¶œ ë° êµ¬ì¡°í™”\\\"\\\"\\\"\\n        # ì¬ë£Œëª… íŒ¨í„´ ë§¤ì¹­\\n        material_keywords = [\\n            \\'steel\\', \\'aluminum\\', \\'copper\\', \\'brass\\', \\'plastic\\',\\n            \\'stainless\\', \\'carbon\\', \\'alloy\\', \\'titanium\\'\\n        ]\\n\\n        detected_materials = []\\n        for keyword in material_keywords:\\n            if keyword.lower() in text.lower():\\n                detected_materials.append(keyword)\\n\\n        return {\\n            \\'materials\\': detected_materials,\\n            \\'specifications\\': text,\\n            \\'type\\': \\'material\\',\\n            \\'raw_text\\': text\\n        }\\n\\`\\`\\`\\n\\n## ğŸ”§ DrawingBOMExtractor í†µí•©\\n\\n### **1. BOM ìƒì„± í”„ë¡œì„¸ìŠ¤ ê³ ë„í™”**\\n\\`\\`\\`python\\nclass EnhancedBOMGenerator:\\n    def __init__(self):\\n        self.obb_detector = OrientedBoundingBoxDetector(\\'yolov11_obb.pt\\')\\n        self.document_parser = DocumentParsingTransformer()\\n        self.symbol_detector = ExistingSymbolDetector()  # ê¸°ì¡´ ì‹œìŠ¤í…œ\\n\\n    def generate_enhanced_bom(self, drawing_image):\\n        \\\"\\\"\\\"í–¥ìƒëœ BOM ìƒì„± í”„ë¡œì„¸ìŠ¤\\\"\\\"\\\"\\n\\n        # 1ë‹¨ê³„: ê¸°ì¡´ ì‹¬ë³¼ ê²€ì¶œ\\n        symbols = self.symbol_detector.detect_symbols(drawing_image)\\n\\n        # 2ë‹¨ê³„: ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ\\n        info_regions = self.obb_detector.detect_information_regions(drawing_image)\\n        parsed_info = self.document_parser.parse_information_regions(info_regions)\\n\\n        # 3ë‹¨ê³„: ì‹¬ë³¼ê³¼ ì •ë³´ ì—°ê´€ì„± ë¶„ì„\\n        enhanced_symbols = self.correlate_symbols_with_info(symbols, parsed_info)\\n\\n        # 4ë‹¨ê³„: êµ¬ì¡°í™”ëœ BOM ìƒì„±\\n        structured_bom = self.create_structured_bom(enhanced_symbols, parsed_info)\\n\\n        return structured_bom\\n\\n    def correlate_symbols_with_info(self, symbols, parsed_info):\\n        \\\"\\\"\\\"ì‹¬ë³¼ê³¼ ì¶”ì¶œëœ ì •ë³´ì˜ ì—°ê´€ì„± ë¶„ì„\\\"\\\"\\\"\\n        enhanced_symbols = []\\n\\n        for symbol in symbols:\\n            # ì‹¬ë³¼ ì£¼ë³€ì˜ ê´€ë ¨ ì •ë³´ ì°¾ê¸°\\n            nearby_info = self.find_nearby_information(symbol, parsed_info)\\n\\n            # ì‹¬ë³¼ì— ì¶”ê°€ ì •ë³´ ì—°ê²°\\n            enhanced_symbol = symbol.copy()\\n            enhanced_symbol[\\'specifications\\'] = nearby_info.get(\\'measures\\', [])\\n            enhanced_symbol[\\'materials\\'] = nearby_info.get(\\'materials\\', [])\\n            enhanced_symbol[\\'tolerances\\'] = nearby_info.get(\\'tolerances\\', [])\\n            enhanced_symbol[\\'notes\\'] = nearby_info.get(\\'notes\\', [])\\n\\n            enhanced_symbols.append(enhanced_symbol)\\n\\n        return enhanced_symbols\\n\\n    def create_structured_bom(self, enhanced_symbols, document_info):\\n        \\\"\\\"\\\"êµ¬ì¡°í™”ëœ BOM ìƒì„±\\\"\\\"\\\"\\n        # ì œëª©ë€ ì •ë³´ ì¶”ì¶œ\\n        title_block_info = self.extract_title_block_info(document_info)\\n\\n        # BOM í—¤ë” ì •ë³´\\n        bom_header = {\\n            \\'drawing_number\\': title_block_info.get(\\'drawing_number\\', \\'\\'),\\n            \\'revision\\': title_block_info.get(\\'revision\\', \\'\\'),\\n            \\'date\\': title_block_info.get(\\'date\\', \\'\\'),\\n            \\'title\\': title_block_info.get(\\'title\\', \\'\\'),\\n            \\'material_standard\\': title_block_info.get(\\'material_standard\\', \\'\\')\\n        }\\n\\n        # ë¶€í’ˆë³„ ìƒì„¸ ì •ë³´\\n        detailed_items = []\\n        for symbol in enhanced_symbols:\\n            item = {\\n                \\'item_number\\': len(detailed_items) + 1,\\n                \\'part_name\\': symbol[\\'class_name\\'],\\n                \\'quantity\\': symbol.get(\\'quantity\\', 1),\\n                \\'specifications\\': symbol.get(\\'specifications\\', []),\\n                \\'material\\': symbol.get(\\'materials\\', []),\\n                \\'tolerances\\': symbol.get(\\'tolerances\\', []),\\n                \\'notes\\': symbol.get(\\'notes\\', []),\\n                \\'location\\': symbol[\\'bbox\\'],\\n                \\'confidence\\': symbol[\\'confidence\\']\\n            }\\n            detailed_items.append(item)\\n\\n        # ìµœì¢… êµ¬ì¡°í™”ëœ BOM\\n        structured_bom = {\\n            \\'header\\': bom_header,\\n            \\'items\\': detailed_items,\\n            \\'summary\\': {\\n                \\'total_items\\': len(detailed_items),\\n                \\'drawing_info\\': document_info,\\n                \\'generation_timestamp\\': datetime.now().isoformat()\\n            }\\n        }\\n\\n        return structured_bom\\n\\`\\`\\`\\n\\n### **2. í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ**\\n\\`\\`\\`python\\nclass DocumentQualityVerifier:\\n    def __init__(self):\\n        self.verification_rules = {\\n            \\'dimensional_consistency\\': self.check_dimensional_consistency,\\n            \\'material_compatibility\\': self.check_material_compatibility,\\n            \\'tolerance_validity\\': self.check_tolerance_validity,\\n            \\'standard_compliance\\': self.check_standard_compliance\\n        }\\n\\n    def verify_bom_quality(self, structured_bom):\\n        \\\"\\\"\\\"BOM í’ˆì§ˆ ê²€ì¦\\\"\\\"\\\"\\n        verification_results = {}\\n\\n        for rule_name, rule_func in self.verification_rules.items():\\n            result = rule_func(structured_bom)\\n            verification_results[rule_name] = result\\n\\n        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\\n        quality_score = self.calculate_quality_score(verification_results)\\n\\n        return {\\n            \\'quality_score\\': quality_score,\\n            \\'detailed_results\\': verification_results,\\n            \\'recommendations\\': self.generate_recommendations(verification_results)\\n        }\\n\\n    def check_dimensional_consistency(self, bom):\\n        \\\"\\\"\\\"ì¹˜ìˆ˜ ì¼ê´€ì„± ê²€ì‚¬\\\"\\\"\\\"\\n        inconsistencies = []\\n\\n        for item in bom[\\'items\\']:\\n            specifications = item.get(\\'specifications\\', [])\\n\\n            # ì¹˜ìˆ˜ ë‹¨ìœ„ ì¼ê´€ì„± í™•ì¸\\n            units = [spec.get(\\'unit\\') for spec in specifications if \\'unit\\' in spec]\\n            if len(set(units)) > 1:\\n                inconsistencies.append({\\n                    \\'item\\': item[\\'part_name\\'],\\n                    \\'issue\\': \\'Mixed units detected\\',\\n                    \\'details\\': units\\n                })\\n\\n        return {\\n            \\'passed\\': len(inconsistencies) == 0,\\n            \\'issues\\': inconsistencies\\n        }\\n\\n    def check_material_compatibility(self, bom):\\n        \\\"\\\"\\\"ì¬ë£Œ í˜¸í™˜ì„± ê²€ì‚¬\\\"\\\"\\\"\\n        # ì¬ë£Œ í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤\\n        compatibility_matrix = {\\n            \\'steel\\': [\\'steel\\', \\'stainless\\', \\'aluminum\\'],\\n            \\'aluminum\\': [\\'aluminum\\', \\'steel\\'],\\n            \\'copper\\': [\\'copper\\', \\'brass\\'],\\n            \\'plastic\\': [\\'plastic\\']\\n        }\\n\\n        compatibility_issues = []\\n\\n        # ì—°ê²°ëœ ë¶€í’ˆë“¤ ê°„ ì¬ë£Œ í˜¸í™˜ì„± ê²€ì‚¬\\n        for i, item1 in enumerate(bom[\\'items\\']):\\n            for j, item2 in enumerate(bom[\\'items\\'][i+1:], i+1):\\n                if self.are_connected_parts(item1, item2):\\n                    materials1 = item1.get(\\'material\\', [])\\n                    materials2 = item2.get(\\'material\\', [])\\n\\n                    if not self.materials_compatible(materials1, materials2, compatibility_matrix):\\n                        compatibility_issues.append({\\n                            \\'items\\': [item1[\\'part_name\\'], item2[\\'part_name\\']],\\n                            \\'materials\\': [materials1, materials2],\\n                            \\'issue\\': \\'Incompatible materials in connected parts\\'\\n                        })\\n\\n        return {\\n            \\'passed\\': len(compatibility_issues) == 0,\\n            \\'issues\\': compatibility_issues\\n        }\\n\\`\\`\\`\\n\\n## ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼\\n\\n### **BOM ìƒì„±ì˜ ì§ˆì  í–¥ìƒ**\\n1. **ì •í™•ì„±**: 94.77% ì •ë°€ë„ë¡œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ ì¶”ì¶œ\\n2. **ì™„ì „ì„±**: ë‹¨ìˆœ ì‹¬ë³¼ ê²€ì¶œì„ ë„˜ì–´ ìƒì„¸ ìŠ¤í™ê¹Œì§€ í¬í•¨\\n3. **êµ¬ì¡°í™”**: ì²´ê³„ì ì´ê³  í‘œì¤€í™”ëœ BOM í¬ë§·\\n4. **ìë™í™”**: ìˆ˜ì‘ì—… ìµœì†Œí™”ë¡œ ì‹œê°„ê³¼ ë¹„ìš© ì ˆì•½\\n\\n### **í’ˆì§ˆ ê´€ë¦¬ ê°œì„ **\\n\\`\\`\\`python\\n# í’ˆì§ˆ ì§€í‘œ ê°œì„  ì˜ˆìƒì¹˜\\nquality_improvements = {\\n    \\'information_completeness\\': \\'+40%\\',    # ì •ë³´ ì™„ì„±ë„\\n    \\'specification_accuracy\\': \\'+35%\\',     # ìŠ¤í™ ì •í™•ë„\\n    \\'error_detection_rate\\': \\'+50%\\',       # ì˜¤ë¥˜ ê²€ì¶œë¥ \\n    \\'processing_time_reduction\\': \\'-60%\\',  # ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•\\n    \\'manual_verification_time\\': \\'-75%\\'    # ìˆ˜ì‘ì—… ê²€ì¦ ì‹œê°„\\n}\\n\\`\\`\\`\\n\\n## ğŸ¯ ì‹¤ì œ ì ìš© ì‹œë‚˜ë¦¬ì˜¤\\n\\n### **ì‹œë‚˜ë¦¬ì˜¤ 1: ì „ì²´ ë„ë©´ ë¶„ì„**\\n\\`\\`\\`python\\ndef comprehensive_drawing_analysis(drawing_path):\\n    \\\"\\\"\\\"í¬ê´„ì  ë„ë©´ ë¶„ì„ í”„ë¡œì„¸ìŠ¤\\\"\\\"\\\"\\n\\n    # ë„ë©´ ë¡œë“œ\\n    drawing = cv2.imread(drawing_path)\\n\\n    # 1. ì‹¬ë³¼ ê²€ì¶œ (ê¸°ì¡´ ì‹œìŠ¤í…œ)\\n    symbols = symbol_detector.detect(drawing)\\n\\n    # 2. ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ (ìƒˆë¡œìš´ ì‹œìŠ¤í…œ)\\n    document_info = document_transformer.extract_information(drawing)\\n\\n    # 3. í†µí•© ë¶„ì„\\n    comprehensive_analysis = {\\n        \\'symbols\\': symbols,\\n        \\'specifications\\': document_info[\\'measures\\'],\\n        \\'materials\\': document_info[\\'materials\\'],\\n        \\'tolerances\\': document_info[\\'tolerances\\'],\\n        \\'notes\\': document_info[\\'notes\\'],\\n        \\'quality_check\\': quality_verifier.verify(symbols, document_info)\\n    }\\n\\n    # 4. ìµœì¢… BOM ìƒì„±\\n    final_bom = bom_generator.generate(comprehensive_analysis)\\n\\n    return final_bom\\n\\`\\`\\`\\n\\n### **ì‹œë‚˜ë¦¬ì˜¤ 2: ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§**\\n\\`\\`\\`python\\nclass RealTimeQualityMonitor:\\n    def monitor_drawing_processing(self, drawing_stream):\\n        \\\"\\\"\\\"ì‹¤ì‹œê°„ ë„ë©´ ì²˜ë¦¬ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§\\\"\\\"\\\"\\n\\n        quality_metrics = {\\n            \\'information_extraction_rate\\': 0,\\n            \\'parsing_accuracy\\': 0,\\n            \\'error_frequency\\': 0,\\n            \\'processing_speed\\': 0\\n        }\\n\\n        for drawing in drawing_stream:\\n            start_time = time.time()\\n\\n            try:\\n                # ì •ë³´ ì¶”ì¶œ\\n                extracted_info = self.document_transformer.process(drawing)\\n\\n                # í’ˆì§ˆ ê²€ì¦\\n                quality_result = self.quality_verifier.verify(extracted_info)\\n\\n                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸\\n                processing_time = time.time() - start_time\\n                self.update_metrics(quality_metrics, quality_result, processing_time)\\n\\n                # ì‹¤ì‹œê°„ ì•Œë¦¼\\n                if quality_result[\\'quality_score\\'] < 0.8:\\n                    self.send_quality_alert(drawing, quality_result)\\n\\n            except Exception as e:\\n                self.log_error(drawing, str(e))\\n                quality_metrics[\\'error_frequency\\'] += 1\\n\\n        return quality_metrics\\n\\`\\`\\`\\n\\n## âš ï¸ êµ¬í˜„ ê³ ë ¤ì‚¬í•­\\n\\n### **ê¸°ìˆ ì  ë„ì „ê³¼ì œ**\\n1. **ëª¨ë¸ í¬ê¸°**: Donut Transformerì˜ í° ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­\\n2. **ì²˜ë¦¬ ì‹œê°„**: ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì¸í•œ ì†ë„ ì €í•˜\\n3. **ì •í™•ë„ ì˜ì¡´ì„±**: 1ë‹¨ê³„ ê²€ì¶œ ì˜¤ë¥˜ê°€ 2ë‹¨ê³„ íŒŒì‹±ì— ì˜í–¥\\n\\n### **í•´ê²° ë°©ì•ˆ**\\n\\`\\`\\`python\\nclass OptimizedDocumentTransformer:\\n    def __init__(self):\\n        # ê²½ëŸ‰í™”ëœ ëª¨ë¸ ì‚¬ìš©\\n        self.lightweight_obb = self.load_quantized_yolo()\\n        self.efficient_transformer = self.load_distilled_donut()\\n\\n        # ìºì‹± ì‹œìŠ¤í…œ\\n        self.region_cache = LRUCache(maxsize=100)\\n        self.parsing_cache = LRUCache(maxsize=200)\\n\\n    def optimized_processing(self, drawing):\\n        \\\"\\\"\\\"ìµœì í™”ëœ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\\\"\\\"\\\"\\n\\n        # 1. ì´ë¯¸ì§€ í•´ì‹œ ê¸°ë°˜ ìºì‹±\\n        image_hash = self.compute_image_hash(drawing)\\n        if image_hash in self.region_cache:\\n            regions = self.region_cache[image_hash]\\n        else:\\n            regions = self.obb_detector.detect(drawing)\\n            self.region_cache[image_hash] = regions\\n\\n        # 2. ë³‘ë ¬ íŒŒì‹±\\n        with ThreadPoolExecutor(max_workers=4) as executor:\\n            parsing_futures = []\\n\\n            for region in regions:\\n                region_hash = self.compute_region_hash(region)\\n                if region_hash not in self.parsing_cache:\\n                    future = executor.submit(\\n                        self.transformer.parse_region, region\\n                    )\\n                    parsing_futures.append((region_hash, future))\\n\\n            # ê²°ê³¼ ìˆ˜ì§‘\\n            for region_hash, future in parsing_futures:\\n                result = future.result()\\n                self.parsing_cache[region_hash] = result\\n\\n        # 3. ê²°ê³¼ ì¡°í•©\\n        final_result = self.combine_cached_results(regions)\\n\\n        return final_result\\n\\`\\`\\`\\n\\n## ğŸ”® í–¥í›„ ë°œì „ ë°©í–¥\\n\\n### **ë‹¨ê¸° ê°œë°œ (1-3ê°œì›”)**\\n1. **ê¸°ë³¸ í†µí•©**: í˜„ì¬ ì‹œìŠ¤í…œê³¼ Document Transformer ì—°ë™\\n2. **ì„±ëŠ¥ ìµœì í™”**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì²˜ë¦¬ ì†ë„ ê°œì„ \\n3. **í•œêµ­ì–´ ì§€ì›**: í•œê¸€ ë„ë©´ ì •ë³´ ì¶”ì¶œ ëŠ¥ë ¥ êµ¬ì¶•\\n\\n### **ì¤‘ì¥ê¸° ë°œì „ (3-12ê°œì›”)**\\n1. **ë‹¤êµ­ì–´ ì§€ì›**: ì˜ì–´, í•œêµ­ì–´, ì¼ë³¸ì–´ ë“± ë‹¤êµ­ì–´ ë„ë©´ ì²˜ë¦¬\\n2. **3D CAD í™•ì¥**: 3D ëª¨ë¸ì˜ 2D ë„ë©´ íˆ¬ì˜ ì •ë³´ ë¶„ì„\\n3. **AI ì–´ì‹œìŠ¤í„´íŠ¸**: ìì—°ì–´ë¡œ BOM ì •ë³´ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ\\n\\n## ğŸ“š ì¶”ê°€ ìë£Œ\\n\\n### **ë…¼ë¬¸ ë° ì—°êµ¬**\\n- [ë…¼ë¬¸ ë§í¬ (arXiv:2505.01530)](https://arxiv.org/abs/2505.01530)\\n- [Donut ëª¨ë¸ ë…¼ë¬¸](https://arxiv.org/abs/2111.15664)\\n- [YOLOv11 OBB êµ¬í˜„](https://docs.ultralytics.com/tasks/obb/)\\n\\n### **êµ¬í˜„ ì°¸ê³  ìë£Œ**\\n- [Transformers Donut](https://huggingface.co/docs/transformers/model_doc/donut)\\n- [Document AI Papers](https://paperswithcode.com/task/document-ai)\\n- [ì—”ì§€ë‹ˆì–´ë§ ë„ë©´ í‘œì¤€](https://www.iso.org/committee/54924.html)\\n\\n---\\n\\n**ê²°ë¡ **: Document Understanding TransformerëŠ” **ì‹¬ë³¼ ê²€ì¶œì„ ë„˜ì–´ ì˜ë¯¸ìˆëŠ” ì •ë³´ ì¶”ì¶œ**ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬, DrawingBOMExtractorë¥¼ **ë‹¨ìˆœí•œ ê²€ì¶œ ë„êµ¬ì—ì„œ ì§€ëŠ¥ì ì¸ ë„ë©´ ë¶„ì„ ì‹œìŠ¤í…œ**ìœ¼ë¡œ ì§„í™”ì‹œí‚¬ ìˆ˜ ìˆëŠ” í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.";
        markdownContent['recommendations'] = "# ğŸ’¡ Implementation Guide: ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ\\n\\n## ğŸ“‹ ê°œìš”\\n\\nì´ ë¬¸ì„œëŠ” ì¡°ì‚¬ëœ ìµœì‹  AI ëª¨ë¸ë“¤ì„ DrawingBOMExtractorì— ì‹¤ì œë¡œ í†µí•©í•˜ê¸° ìœ„í•œ **ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ**ì™€ **ìš°ì„ ìˆœìœ„ë³„ ë¡œë“œë§µ**ì„ ì œì‹œí•©ë‹ˆë‹¤. ê¸°ìˆ ì  ë³µì¡ì„±, êµ¬í˜„ ìš©ì´ì„±, ì˜ˆìƒ íš¨ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ êµ¬í˜„ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤.\\n\\n## ğŸ¯ ìš°ì„ ìˆœìœ„ë³„ êµ¬í˜„ ê³„íš\\n\\n### **ğŸ¥‡ 1ìˆœìœ„: DINO-X í†µí•© (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)**\\n\\n#### **êµ¬í˜„ ë‚œì´ë„**: â­â­â˜†â˜†â˜† (ì‰¬ì›€)\\n#### **ê¸°ëŒ€ íš¨ê³¼**: â­â­â­â­â­ (ë§¤ìš° ë†’ìŒ)\\n#### **êµ¬í˜„ ì‹œê°„**: 1-2ì£¼\\n\\n\\`\\`\\`python\\n# 1ë‹¨ê³„: DINO-X API í†µí•©\\nclass DinoXIntegration:\\n    def __init__(self, api_token):\\n        self.api_token = api_token\\n        self.base_url = \\\"https://api.deepdataspace.com/dino-x\\\"\\n\\n    def integrate_as_fifth_model(self):\\n        \\\"\\\"\\\"5ë²ˆì§¸ ëª¨ë¸ë¡œ DINO-X ì¶”ê°€\\\"\\\"\\\"\\n\\n        # ê¸°ì¡´ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ í™•ì¥\\n        self.model_registry.register_model({\\n            \\'id\\': \\'dino_x\\',\\n            \\'name\\': \\'DINO-X\\',\\n            \\'type\\': \\'foundation_model\\',\\n            \\'api_based\\': True,\\n            \\'supports_text_prompts\\': True,\\n            \\'weight\\': 1.4  # ë†’ì€ ê°€ì¤‘ì¹˜\\n        })\\n\\n    def text_prompt_detection(self, image, prompt):\\n        \\\"\\\"\\\"í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ê²€ì¶œ\\\"\\\"\\\"\\n        response = requests.post(f\\\"{self.base_url}/detect\\\",\\n            headers={\\\"Authorization\\\": f\\\"Bearer {self.api_token}\\\"},\\n            json={\\n                \\\"image\\\": self.encode_image(image),\\n                \\\"prompt\\\": prompt,\\n                \\\"confidence\\\": 0.25\\n            }\\n        )\\n        return self.parse_dino_x_response(response.json())\\n\\`\\`\\`\\n\\n#### **ì¦‰ì‹œ ì–»ì„ ìˆ˜ ìˆëŠ” ì´ì **:\\n- **ìƒˆë¡œìš´ ë¶€í’ˆ í´ë˜ìŠ¤**: ì¬í•™ìŠµ ì—†ì´ í…ìŠ¤íŠ¸ë¡œ ê²€ì¶œ\\n- **Zero-shot ëŠ¥ë ¥**: \\\"ì••ë ¥ ì„¼ì„œ\\\", \\\"ì˜¨ë„ ê²Œì´ì§€\\\" ë“± ìƒˆ ë¶€í’ˆ ì¦‰ì‹œ ëŒ€ì‘\\n- **5-ëª¨ë¸ Voting**: ë”ìš± ì •í™•í•œ ê²€ì¶œ ê²°ê³¼\\n\\n---\\n\\n### **ğŸ¥ˆ 2ìˆœìœ„: SAM 2 ì„¸ë¶„í™” ì¶”ê°€ (3-4ì£¼)**\\n\\n#### **êµ¬í˜„ ë‚œì´ë„**: â­â­â­â˜†â˜† (ë³´í†µ)\\n#### **ê¸°ëŒ€ íš¨ê³¼**: â­â­â­â­â˜† (ë†’ìŒ)\\n#### **êµ¬í˜„ ì‹œê°„**: 3-4ì£¼\\n\\n\\`\\`\\`python\\n# 2ë‹¨ê³„: SAM 2 ì •ë°€ ì„¸ë¶„í™” í†µí•©\\nclass SAM2Integration:\\n    def __init__(self, model_path):\\n        self.sam2_predictor = SAM2ImagePredictor(\\n            build_sam2(\\\"sam2_hiera_l.yaml\\\", model_path)\\n        )\\n\\n    def enhance_detection_with_segmentation(self, image, detections):\\n        \\\"\\\"\\\"ê²€ì¶œ ê²°ê³¼ë¥¼ SAM 2ë¡œ ì •ë°€ ì„¸ë¶„í™”\\\"\\\"\\\"\\n        self.sam2_predictor.set_image(image)\\n\\n        enhanced_detections = []\\n        for detection in detections:\\n            # YOLO ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ SAM 2 í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©\\n            bbox = detection[\\'bbox\\']\\n            input_box = np.array([bbox[\\'x1\\'], bbox[\\'y1\\'], bbox[\\'x2\\'], bbox[\\'y2\\']])\\n\\n            # ì •ë°€ ë§ˆìŠ¤í¬ ìƒì„±\\n            masks, scores, _ = self.sam2_predictor.predict(\\n                box=input_box[None, :],\\n                multimask_output=False\\n            )\\n\\n            # ì›ë³¸ ê²€ì¶œì— ì •ë°€ ë§ˆìŠ¤í¬ ì¶”ê°€\\n            enhanced_detection = detection.copy()\\n            enhanced_detection[\\'precise_mask\\'] = masks[0]\\n            enhanced_detection[\\'mask_confidence\\'] = scores[0]\\n\\n            enhanced_detections.append(enhanced_detection)\\n\\n        return enhanced_detections\\n\\`\\`\\`\\n\\n#### **ê¸°ëŒ€ íš¨ê³¼**:\\n- **ì •ë°€í•œ ê²½ê³„**: í”½ì…€ ë‹¨ìœ„ ì •í™•í•œ ì‹¬ë³¼ ë¶„í• \\n- **ê²¹ì¹˜ëŠ” ì‹¬ë³¼ ë¶„ë¦¬**: ë³µì¡í•œ ë„ë©´ì—ì„œ ê°œë³„ ì‹¬ë³¼ ëª…í™•íˆ êµ¬ë¶„\\n- **ë” ì •í™•í•œ BOM**: ì •í™•í•œ ê°œìˆ˜ì™€ ìœ„ì¹˜ ì •ë³´\\n\\n---\\n\\n### **ğŸ¥‰ 3ìˆœìœ„: RT-DETR v2 ì„±ëŠ¥ ê°•í™” (4-6ì£¼)**\\n\\n#### **êµ¬í˜„ ë‚œì´ë„**: â­â­â­â˜†â˜† (ë³´í†µ)\\n#### **ê¸°ëŒ€ íš¨ê³¼**: â­â­â­â­â˜† (ë†’ìŒ)\\n#### **êµ¬í˜„ ì‹œê°„**: 4-6ì£¼\\n\\n\\`\\`\\`python\\n# 3ë‹¨ê³„: RT-DETR v2 ì‹¤ì‹œê°„ ì„±ëŠ¥ í–¥ìƒ\\nclass RTDETRIntegration:\\n    def __init__(self):\\n        self.rt_detr = RTDETR(\\'rtdetr-x.pt\\')\\n\\n        # ë™ì  ì„±ëŠ¥ í”„ë¡œíŒŒì¼\\n        self.performance_modes = {\\n            \\'realtime\\': {\\'conf\\': 0.5, \\'speed_optimized\\': True},\\n            \\'balanced\\': {\\'conf\\': 0.35, \\'speed_optimized\\': False},\\n            \\'precision\\': {\\'conf\\': 0.25, \\'speed_optimized\\': False}\\n        }\\n\\n    def adaptive_detection(self, image, system_load, image_complexity):\\n        \\\"\\\"\\\"ì‹œìŠ¤í…œ ìƒí™©ì— ë”°ë¥¸ ì ì‘ì  ê²€ì¶œ\\\"\\\"\\\"\\n\\n        # ì„±ëŠ¥ ëª¨ë“œ ìë™ ì„ íƒ\\n        if system_load > 0.8 or image_complexity > 0.7:\\n            mode = \\'realtime\\'\\n        elif system_load < 0.3:\\n            mode = \\'precision\\'\\n        else:\\n            mode = \\'balanced\\'\\n\\n        settings = self.performance_modes[mode]\\n\\n        # RT-DETR ê²€ì¶œ\\n        results = self.rt_detr(image, conf=settings[\\'conf\\'])\\n\\n        return self.post_process_results(results, mode)\\n\\`\\`\\`\\n\\n#### **í•µì‹¬ ì¥ì **:\\n- **NMS-Free**: í›„ì²˜ë¦¬ ì—†ëŠ” ê¹”ë”í•œ ê²°ê³¼\\n- **ì‹¤ì‹œê°„ ì ì‘**: ìƒí™©ì— ë”°ë¥¸ ì„±ëŠ¥ ìë™ ì¡°ì •\\n- **íŠ¸ëœìŠ¤í¬ë¨¸ ì •í™•ë„**: CNN ëŒ€ë¹„ í–¥ìƒëœ ì •í™•ë„\\n\\n---\\n\\n### **ğŸ”¬ 4ìˆœìœ„: VectorGraphNET ì—°êµ¬ ê°œë°œ (3-6ê°œì›”)**\\n\\n#### **êµ¬í˜„ ë‚œì´ë„**: â­â­â­â­â­ (ë§¤ìš° ì–´ë ¤ì›€)\\n#### **ê¸°ëŒ€ íš¨ê³¼**: â­â­â­â­â­ (ë§¤ìš° ë†’ìŒ)\\n#### **êµ¬í˜„ ì‹œê°„**: 3-6ê°œì›”\\n\\n\\`\\`\\`python\\n# 4ë‹¨ê³„: VectorGraphNET ë²¡í„° ë¶„ì„ (ì¥ê¸° í”„ë¡œì íŠ¸)\\nclass VectorGraphNETProject:\\n    def __init__(self):\\n        self.pdf_converter = PDFToSVGConverter()\\n        self.graph_builder = CADGraphBuilder()\\n\\n    def research_and_development_plan(self):\\n        \\\"\\\"\\\"ì—°êµ¬ê°œë°œ ê³„íš\\\"\\\"\\\"\\n        phases = {\\n            \\'Phase 1 (1ê°œì›”)\\': [\\n                \\'PDFâ†’SVG ë³€í™˜ ì—”ì§„ ê°œë°œ\\',\\n                \\'ê¸°ë³¸ ê·¸ë˜í”„ êµ¬ì¡° êµ¬í˜„\\',\\n                \\'FloorplanCAD ë°ì´í„°ì…‹ ì ì‘\\'\\n            ],\\n            \\'Phase 2 (2ê°œì›”)\\': [\\n                \\'Graph Attention v2 ë„¤íŠ¸ì›Œí¬ êµ¬í˜„\\',\\n                \\'ì‚°ì—… ë„ë©´ íŠ¹í™” ì „ì²˜ë¦¬\\',\\n                \\'ê¸°ë³¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\\'\\n            ],\\n            \\'Phase 3 (3ê°œì›”)\\': [\\n                \\'ëŒ€ê·œëª¨ ê·¸ë˜í”„ ìµœì í™”\\',\\n                \\'ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥í•œ ê²½ëŸ‰í™”\\',\\n                \\'ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í†µí•©\\'\\n            ]\\n        }\\n        return phases\\n\\`\\`\\`\\n\\n#### **í˜ì‹ ì  ì ì¬ë ¥**:\\n- **ë²¡í„° ë„¤ì´í‹°ë¸Œ**: ì›ë³¸ í’ˆì§ˆ ë³´ì¡´\\n- **ê´€ê³„ ë¶„ì„**: ì‹¬ë³¼ ê°„ ì—°ê²°ì„± ì´í•´\\n- **í™•ì¥ì„±**: í•´ìƒë„ ë…ë¦½ì  ì²˜ë¦¬\\n\\n---\\n\\n## ğŸ› ï¸ ì‹¤ì œ êµ¬í˜„ ë‹¨ê³„\\n\\n### **Phase 1: ê¸°ë°˜ êµ¬ì¶• (1ì£¼ì°¨)**\\n\\n\\`\\`\\`python\\n# í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¥\\nDrawingBOMExtractor/\\nâ”œâ”€â”€ models/\\nâ”‚   â”œâ”€â”€ existing/           # ê¸°ì¡´ ëª¨ë¸ë“¤\\nâ”‚   â”‚   â”œâ”€â”€ yolov11l.pt\\nâ”‚   â”‚   â”œâ”€â”€ yolov11x.pt\\nâ”‚   â”‚   â””â”€â”€ detectron2/\\nâ”‚   â””â”€â”€ new_models/         # ìƒˆë¡œìš´ ëª¨ë¸ë“¤\\nâ”‚       â”œâ”€â”€ dino_x/\\nâ”‚       â”œâ”€â”€ sam2/\\nâ”‚       â””â”€â”€ rt_detr/\\nâ”œâ”€â”€ integrations/           # ìƒˆë¡œìš´ í†µí•© ëª¨ë“ˆ\\nâ”‚   â”œâ”€â”€ __init__.py\\nâ”‚   â”œâ”€â”€ dino_x_client.py\\nâ”‚   â”œâ”€â”€ sam2_segmenter.py\\nâ”‚   â””â”€â”€ rt_detr_detector.py\\nâ””â”€â”€ enhanced_pipeline/      # í–¥ìƒëœ íŒŒì´í”„ë¼ì¸\\n    â”œâ”€â”€ multi_modal_detector.py\\n    â”œâ”€â”€ enhanced_voting.py\\n    â””â”€â”€ quality_verifier.py\\n\\`\\`\\`\\n\\n### **Phase 2: DINO-X í†µí•© (2ì£¼ì°¨)**\\n\\n\\`\\`\\`python\\n# real_ai_app.py ìˆ˜ì •ì‚¬í•­\\nclass EnhancedModelRegistry:\\n    def __init__(self):\\n        self.models = {\\n            # ê¸°ì¡´ ëª¨ë¸ë“¤\\n            \\'yolo_v11l\\': {\\'weight\\': 1.2, \\'type\\': \\'local\\'},\\n            \\'yolo_v11x\\': {\\'weight\\': 1.3, \\'type\\': \\'local\\'},\\n            \\'yolo_v8\\': {\\'weight\\': 1.0, \\'type\\': \\'local\\'},\\n            \\'detectron2\\': {\\'weight\\': 1.1, \\'type\\': \\'local\\'},\\n\\n            # ìƒˆë¡œìš´ ëª¨ë¸\\n            \\'dino_x\\': {\\n                \\'weight\\': 1.4,\\n                \\'type\\': \\'api\\',\\n                \\'supports_text_prompts\\': True,\\n                \\'zero_shot_capable\\': True\\n            }\\n        }\\n\\ndef enhanced_detection_pipeline(image, user_prompt=None):\\n    \\\"\\\"\\\"í–¥ìƒëœ 5-ëª¨ë¸ ê²€ì¶œ íŒŒì´í”„ë¼ì¸\\\"\\\"\\\"\\n\\n    # 1. ê¸°ì¡´ 4ê°œ ëª¨ë¸ ë³‘ë ¬ ì‹¤í–‰\\n    traditional_results = run_parallel_detection(image)\\n\\n    # 2. DINO-X ì¶”ê°€ ê²€ì¶œ\\n    if user_prompt:\\n        dino_x_results = dino_x_client.detect_with_prompt(image, user_prompt)\\n    else:\\n        dino_x_results = dino_x_client.detect_general_symbols(image)\\n\\n    # 3. 5-ëª¨ë¸ Voting ì‹œìŠ¤í…œ\\n    all_results = traditional_results + dino_x_results\\n    final_results = enhanced_voting_system(all_results)\\n\\n    return final_results\\n\\`\\`\\`\\n\\n### **Phase 3: UI ê°œì„  (3ì£¼ì°¨)**\\n\\n\\`\\`\\`python\\n# Streamlit UIì— ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€\\ndef enhanced_user_interface():\\n    st.header(\\\"ğŸ¤– AI ê¸°ë°˜ CAD ì‹¬ë³¼ ê²€ì¶œ\\\")\\n\\n    # ê¸°ì¡´ ëª¨ë¸ ì„ íƒ í™•ì¥\\n    col1, col2 = st.columns(2)\\n\\n    with col1:\\n        st.subheader(\\\"ğŸ·ï¸ ê¸°ì¡´ ëª¨ë¸\\\")\\n        traditional_models = st.multiselect(\\n            \\\"ê²€ì¶œ ëª¨ë¸ ì„ íƒ\\\",\\n            [\\'YOLOv11L\\', \\'YOLOv11X\\', \\'YOLOv8\\', \\'Detectron2\\'],\\n            default=[\\'YOLOv11L\\', \\'YOLOv11X\\']\\n        )\\n\\n    with col2:\\n        st.subheader(\\\"ğŸš€ ì°¨ì„¸ëŒ€ ëª¨ë¸\\\")\\n        advanced_models = st.multiselect(\\n            \\\"ê³ ê¸‰ ëª¨ë¸ ì„ íƒ\\\",\\n            [\\'DINO-X\\', \\'RT-DETR v2\\'],\\n            default=[\\'DINO-X\\']\\n        )\\n\\n    # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê¸°ëŠ¥\\n    if \\'DINO-X\\' in advanced_models:\\n        st.subheader(\\\"ğŸ’¬ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸\\\")\\n        user_prompt = st.text_input(\\n            \\\"ì°¾ê³  ì‹¶ì€ ë¶€í’ˆì„ ì„¤ëª…í•˜ì„¸ìš”\\\",\\n            placeholder=\\\"ì˜ˆ: ì••ë ¥ ì„¼ì„œ, ì˜¨ë„ ê³„ì¸¡ê¸°, ë¹„ìƒì •ì§€ ìŠ¤ìœ„ì¹˜\\\"\\n        )\\n\\n    # SAM 2 ì •ë°€ ë¶„í•  ì˜µì…˜\\n    enable_precise_segmentation = st.checkbox(\\n        \\\"ğŸ¯ ì •ë°€ ë¶„í•  í™œì„±í™” (SAM 2)\\\",\\n        help=\\\"í”½ì…€ ë‹¨ìœ„ ì •í™•í•œ ì‹¬ë³¼ ê²½ê³„ ì¶”ì¶œ\\\"\\n    )\\n\\`\\`\\`\\n\\n### **Phase 4: ì„±ëŠ¥ ìµœì í™” (4ì£¼ì°¨)**\\n\\n\\`\\`\\`python\\n# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”\\nclass PerformanceOptimizer:\\n    def __init__(self):\\n        self.metrics = {\\n            \\'model_inference_times\\': {},\\n            \\'memory_usage\\': {},\\n            \\'accuracy_scores\\': {},\\n            \\'user_satisfaction\\': {}\\n        }\\n\\n    def optimize_model_selection(self, image_properties, system_resources):\\n        \\\"\\\"\\\"ì´ë¯¸ì§€ì™€ ì‹œìŠ¤í…œ ìƒí™©ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì¡°í•© ì„ íƒ\\\"\\\"\\\"\\n\\n        # ì´ë¯¸ì§€ ë³µì¡ë„ ë¶„ì„\\n        complexity = analyze_image_complexity(image_properties)\\n\\n        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸\\n        available_memory = get_available_memory()\\n        cpu_usage = get_cpu_usage()\\n\\n        # ìµœì  ì „ëµ ê²°ì •\\n        if complexity > 0.8 and available_memory > 8e9:\\n            # ë³µì¡í•œ ì´ë¯¸ì§€ + ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ = ëª¨ë“  ëª¨ë¸ ì‚¬ìš©\\n            strategy = \\'maximum_accuracy\\'\\n            selected_models = [\\'yolo_v11x\\', \\'dino_x\\', \\'sam2\\']\\n\\n        elif cpu_usage > 0.7:\\n            # ë†’ì€ CPU ì‚¬ìš©ë¥  = ë¹ ë¥¸ ëª¨ë¸ë§Œ\\n            strategy = \\'speed_optimized\\'\\n            selected_models = [\\'yolo_v11l\\', \\'rt_detr\\']\\n\\n        else:\\n            # ê· í˜•ì¡íŒ ì ‘ê·¼\\n            strategy = \\'balanced\\'\\n            selected_models = [\\'yolo_v11l\\', \\'yolo_v11x\\', \\'dino_x\\']\\n\\n        return {\\n            \\'strategy\\': strategy,\\n            \\'models\\': selected_models,\\n            \\'expected_performance\\': self.estimate_performance(selected_models)\\n        }\\n\\`\\`\\`\\n\\n## ğŸ“Š êµ¬í˜„ ë¡œë“œë§µ íƒ€ì„ë¼ì¸\\n\\n### **ğŸš€ ë‹¨ê¸° ëª©í‘œ (1-2ê°œì›”)**\\n\\n| ì£¼ì°¨ | ì£¼ìš” ì‘ì—… | ë‹´ë‹¹ ì˜ì—­ | ì™„ë£Œ ê¸°ì¤€ |\\n|------|-----------|-----------|-----------|\\n| **1ì£¼** | í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¥ | Infrastructure | ìƒˆ ëª¨ë“ˆ ë””ë ‰í† ë¦¬ ìƒì„± |\\n| **2ì£¼** | DINO-X API í†µí•© | AI Integration | í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê²€ì¶œ ì‘ë™ |\\n| **3ì£¼** | 5-ëª¨ë¸ Voting êµ¬í˜„ | Core Logic | í–¥ìƒëœ ì •í™•ë„ í™•ì¸ |\\n| **4ì£¼** | UI/UX ê°œì„  | Frontend | ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ í†µê³¼ |\\n| **5-6ì£¼** | SAM 2 ì„¸ë¶„í™” ì¶”ê°€ | AI Enhancement | ì •ë°€ ë§ˆìŠ¤í¬ ìƒì„± í™•ì¸ |\\n| **7-8ì£¼** | ì„±ëŠ¥ ìµœì í™” | Performance | ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ ë‹¬ì„± |\\n\\n### **ğŸ¯ ì¤‘ê¸° ëª©í‘œ (3-6ê°œì›”)**\\n\\n| ì›” | ì£¼ìš” ëª©í‘œ | í•µì‹¬ ê¸°ëŠ¥ |\\n|----|-----------|-----------|\\n| **3ê°œì›”** | RT-DETR v2 í†µí•© | ì‹¤ì‹œê°„ íŠ¸ëœìŠ¤í¬ë¨¸ ê²€ì¶œ |\\n| **4ê°œì›”** | Document Transformer | ë„ë©´ ì •ë³´ ì¶”ì¶œ |\\n| **5ê°œì›”** | í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ | ìë™ ì˜¤ë¥˜ ê²€ì¶œ |\\n| **6ê°œì›”** | í†µí•© í…ŒìŠ¤íŠ¸ | ì „ì²´ ì‹œìŠ¤í…œ ì•ˆì •í™” |\\n\\n### **ğŸ”® ì¥ê¸° ë¹„ì „ (6-12ê°œì›”)**\\n\\n| ë¶„ê¸° | ì—°êµ¬ ê°œë°œ | í˜ì‹  ê¸°ëŠ¥ |\\n|------|-----------|-----------|\\n| **Q3** | VectorGraphNET ê¸°ì´ˆ ì—°êµ¬ | PDF ë²¡í„° ë¶„ì„ |\\n| **Q4** | GAT-CADNet ê´€ê³„ ë¶„ì„ | ì‹¬ë³¼ ê°„ ì—°ê²°ì„± |\\n| **Q1** | 3D CAD í™•ì¥ ì—°êµ¬ | 3D ëª¨ë¸ ì§€ì› |\\n| **Q2** | AI ì–´ì‹œìŠ¤í„´íŠ¸ ê°œë°œ | ìì—°ì–´ ì§ˆì˜ì‘ë‹µ |\\n\\n## ğŸ’° ë¹„ìš© ë¶„ì„ ë° ROI\\n\\n### **ê°œë°œ ë¹„ìš© ì¶”ì •**\\n\\n\\`\\`\\`python\\nimplementation_costs = {\\n    \\'immediate_integration\\': {\\n        \\'dino_x_api\\': {\\n            \\'setup_cost\\': 0,           # ë¬´ë£Œ í‹°ì–´ í™œìš©\\n            \\'monthly_api_cost\\': 100,   # API ì‚¬ìš©ë£Œ (USD)\\n            \\'development_time\\': \\'2ì£¼\\'\\n        },\\n        \\'sam2_integration\\': {\\n            \\'compute_cost\\': 0,         # ë¡œì»¬ ì¶”ë¡ \\n            \\'model_storage\\': \\'2GB\\',    # ì¶”ê°€ ì €ì¥ê³µê°„\\n            \\'development_time\\': \\'3ì£¼\\'\\n        }\\n    },\\n    \\'research_projects\\': {\\n        \\'vectorgraphnet\\': {\\n            \\'research_time\\': \\'3ê°œì›”\\',\\n            \\'compute_resources\\': \\'ê³ ì„±ëŠ¥ GPU í•„ìš”\\',\\n            \\'dataset_cost\\': 0          # ì˜¤í”ˆì†ŒìŠ¤ ë°ì´í„°ì…‹\\n        }\\n    }\\n}\\n\\`\\`\\`\\n\\n### **ì˜ˆìƒ ROI (Return on Investment)**\\n\\n\\`\\`\\`python\\nroi_analysis = {\\n    \\'cost_savings\\': {\\n        \\'manual_bom_creation_time\\': \\'-60%\\',    # ìˆ˜ì‘ì—… ì‹œê°„ 60% ë‹¨ì¶•\\n        \\'error_correction_cost\\': \\'-40%\\',       # ì˜¤ë¥˜ ìˆ˜ì • ë¹„ìš© 40% ê°ì†Œ\\n        \\'quality_assurance_time\\': \\'-50%\\'       # QA ì‹œê°„ 50% ì ˆì•½\\n    },\\n    \\'productivity_gains\\': {\\n        \\'processing_throughput\\': \\'+150%\\',      # ì²˜ë¦¬ëŸ‰ 2.5ë°° ì¦ê°€\\n        \\'accuracy_improvement\\': \\'+25%\\',        # ì •í™•ë„ 25% í–¥ìƒ\\n        \\'new_capabilities\\': \\'Immeasurable\\'     # ìƒˆë¡œìš´ ê¸°ëŠ¥ì˜ ë¬´í˜• ê°€ì¹˜\\n    },\\n    \\'competitive_advantages\\': {\\n        \\'time_to_market\\': \\'ê¸°ì¡´ ëŒ€ë¹„ 3ë°° ë¹ ë¥¸ BOM ìƒì„±\\',\\n        \\'quality_differentiation\\': \\'ì—…ê³„ ìµœê³  ìˆ˜ì¤€ ì •í™•ë„\\',\\n        \\'scalability\\': \\'ë¬´ì œí•œ ë„ë©´ ì²˜ë¦¬ ëŠ¥ë ¥\\'\\n    }\\n}\\n\\`\\`\\`\\n\\n## ğŸš¨ ìœ„í—˜ ê´€ë¦¬ ë° ëŒ€ì‘ ì „ëµ\\n\\n### **ê¸°ìˆ ì  ìœ„í—˜**\\n\\n| ìœ„í—˜ ìš”ì†Œ | í™•ë¥  | ì˜í–¥ë„ | ëŒ€ì‘ ì „ëµ |\\n|-----------|------|--------|-----------|\\n| **API ì˜ì¡´ì„±** | ì¤‘ | ê³  | ë¡œì»¬ ëª¨ë¸ ë°±ì—… ì¤€ë¹„ |\\n| **ì„±ëŠ¥ ì €í•˜** | ì € | ì¤‘ | ì ì§„ì  ìµœì í™” ì ìš© |\\n| **ëª¨ë¸ í˜¸í™˜ì„±** | ì¤‘ | ì¤‘ | ì² ì €í•œ ì‚¬ì „ í…ŒìŠ¤íŠ¸ |\\n\\n### **ëŒ€ì‘ ë°©ì•ˆ**\\n\\n\\`\\`\\`python\\nclass RiskMitigationStrategy:\\n    def __init__(self):\\n        self.backup_plans = {\\n            \\'api_failure\\': self.fallback_to_local_models,\\n            \\'performance_issues\\': self.activate_lightweight_mode,\\n            \\'accuracy_degradation\\': self.revert_to_previous_version\\n        }\\n\\n    def fallback_to_local_models(self):\\n        \\\"\\\"\\\"API ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ëª¨ë¸ë¡œ í´ë°±\\\"\\\"\\\"\\n        return \\\"ê¸°ì¡´ 4-ëª¨ë¸ ì‹œìŠ¤í…œìœ¼ë¡œ ìë™ ì „í™˜\\\"\\n\\n    def activate_lightweight_mode(self):\\n        \\\"\\\"\\\"ì„±ëŠ¥ ë¬¸ì œ ì‹œ ê²½ëŸ‰ ëª¨ë“œ í™œì„±í™”\\\"\\\"\\\"\\n        return \\\"RT-DETR ë¹ ë¥¸ ëª¨ë“œë¡œ ì „í™˜, ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ\\\"\\n\\n    def revert_to_previous_version(self):\\n        \\\"\\\"\\\"ì •í™•ë„ ì €í•˜ ì‹œ ì´ì „ ë²„ì „ìœ¼ë¡œ ë³µì›\\\"\\\"\\\"\\n        return \\\"Git ê¸°ë°˜ ìë™ ë¡¤ë°± ì‹œìŠ¤í…œ í™œì„±í™”\\\"\\n\\`\\`\\`\\n\\n## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° ë§ˆì¼ìŠ¤í†¤\\n\\n### **âœ… 1ë‹¨ê³„ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸**\\n\\n- [ ] DINO-X API í‚¤ ë°œê¸‰ ë° ì—°ë™ í…ŒìŠ¤íŠ¸\\n- [ ] ê¸°ì¡´ 4-ëª¨ë¸ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„± í™•ì¸\\n- [ ] 5-ëª¨ë¸ Voting ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„\\n- [ ] í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ UI ê°œë°œ\\n- [ ] ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ì •í™•ë„, ì†ë„)\\n- [ ] ì‚¬ìš©ì ê°€ì´ë“œ ë¬¸ì„œ ì‘ì„±\\n\\n### **ğŸ¯ ì¤‘ìš” ë§ˆì¼ìŠ¤í†¤**\\n\\n1. **MVP (Minimum Viable Product)**: DINO-X í†µí•© ì™„ë£Œ\\n2. **Enhanced Version**: SAM 2 ì¶”ê°€ë¡œ ì •ë°€ë„ í–¥ìƒ\\n3. **Professional Version**: RT-DETRë¡œ ì‹¤ì‹œê°„ ì„±ëŠ¥ ë‹¬ì„±\\n4. **Research Version**: VectorGraphNETìœ¼ë¡œ ì°¨ì„¸ëŒ€ ê¸°ìˆ  ì ìš©\\n\\n## ğŸ“ ì§€ì› ë° í˜‘ì—…\\n\\n### **ê°œë°œ ì§€ì› ì±„ë„**\\n- **ê¸°ìˆ  ë¬¸ì˜**: ê° ëª¨ë¸ë³„ ê³µì‹ GitHub Issues\\n- **API ì§€ì›**: DINO-X ê³µì‹ ë¬¸ì„œ ë° ì»¤ë®¤ë‹ˆí‹°\\n- **ì„±ëŠ¥ ìµœì í™”**: PyTorch, CUDA ìµœì í™” ì»¤ë®¤ë‹ˆí‹°\\n\\n### **í˜‘ì—… íŒŒíŠ¸ë„ˆ**\\n- **IDEA Research**: DINO-X ê´€ë ¨ ê¸°ìˆ  ì§€ì›\\n- **Meta AI**: SAM 2 í™œìš© ê°€ì´ë“œ\\n- **Ultralytics**: RT-DETR ìµœì í™” ìë¬¸\\n\\n---\\n\\n**ê²°ë¡ **: ì´ êµ¬í˜„ ê°€ì´ë“œëŠ” **ì‹¤ìš©ì„±ê³¼ í˜ì‹ ì„±ì˜ ê· í˜•**ì„ ê³ ë ¤í•˜ì—¬ ë‹¨ê³„ë³„ë¡œ ì‹œìŠ¤í…œì„ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆëŠ” í˜„ì‹¤ì ì¸ ë¡œë“œë§µì„ ì œì‹œí•©ë‹ˆë‹¤. **DINO-Xë¶€í„° ì‹œì‘**í•˜ì—¬ ì ì§„ì ìœ¼ë¡œ ê³ ê¸‰ ê¸°ëŠ¥ì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨, ì•ˆì •ì„±ì„ ìœ ì§€í•˜ë©´ì„œë„ ìµœì‹  AI ê¸°ìˆ ì˜ í˜œíƒì„ ìµœëŒ€í•œ í™œìš©í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.";
        markdownContent['comparison'] = "# âš–ï¸ Model Comparison: ì¢…í•©ì  ì„±ëŠ¥ ë¹„êµ ë¶„ì„\\n\\n## ğŸ“‹ ê°œìš”\\n\\nì´ ë¬¸ì„œëŠ” ì¡°ì‚¬ëœ **8ê°œ AI ëª¨ë¸**ì˜ **ì„±ëŠ¥, êµ¬í˜„ ë³µì¡ì„±, ì‹¤ìš©ì„±**ì„ ì¢…í•©ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•˜ì—¬ DrawingBOMExtractorì— ìµœì í•œ ëª¨ë¸ ì¡°í•©ì„ ì œì‹œí•©ë‹ˆë‹¤. ê° ëª¨ë¸ì˜ ê°•ì ê³¼ ì•½ì ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬ ì‹¤ì§ˆì ì¸ ì˜ì‚¬ê²°ì •ì„ ì§€ì›í•©ë‹ˆë‹¤.\\n\\n## ğŸ† ëª¨ë¸ë³„ ì¢…í•© ì ìˆ˜\\n\\n### **ì ìˆ˜ ì²´ê³„**\\n- **ì„±ëŠ¥ (30%)**: ì •í™•ë„, ì†ë„, ì•ˆì •ì„±\\n- **êµ¬í˜„ì„± (25%)**: ê°œë°œ ë‚œì´ë„, í†µí•© ìš©ì´ì„±\\n- **ì‹¤ìš©ì„± (25%)**: ìœ ì§€ë³´ìˆ˜, í™•ì¥ì„±, ë¹„ìš©\\n- **í˜ì‹ ì„± (20%)**: ê¸°ìˆ ì  ìš°ìˆ˜ì„±, ë¯¸ë˜ ì ì¬ë ¥\\n\\n---\\n\\n## ğŸ“Š ìƒì„¸ ë¹„êµ ë¶„ì„\\n\\n### **ğŸ¥‡ 1ìœ„: DINO-X (93/100ì )**\\n\\n#### **ğŸ”¥ í•µì‹¬ ê°•ì **\\n\\`\\`\\`\\nâœ… Zero-Shot ëŠ¥ë ¥: ì¬í•™ìŠµ ì—†ì´ ìƒˆ ë¶€í’ˆ ê²€ì¶œ\\nâœ… í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: ìì—°ì–´ë¡œ ì›í•˜ëŠ” ë¶€í’ˆ ê²€ìƒ‰\\nâœ… ë†’ì€ ì •í™•ë„: COCO 56.0 AP ë‹¬ì„±\\nâœ… ì¦‰ì‹œ ì ìš© ê°€ëŠ¥: API ê¸°ë°˜ìœ¼ë¡œ ë¹ ë¥¸ í†µí•©\\n\\`\\`\\`\\n\\n#### **âš ï¸ ì œí•œì‚¬í•­**\\n\\`\\`\\`\\nâŒ API ì˜ì¡´ì„±: ì¸í„°ë„· ì—°ê²° í•„ìˆ˜\\nâŒ ë¹„ìš© ë°œìƒ: ì‚¬ìš©ëŸ‰ì— ë”°ë¥¸ API ìš”ê¸ˆ\\nâŒ ì‘ë‹µ ì§€ì—°: ë„¤íŠ¸ì›Œí¬ ì§€ì—° ê°€ëŠ¥ì„±\\n\\`\\`\\`\\n\\n#### **ì ìˆ˜ ìƒì„¸**\\n| í‰ê°€ í•­ëª© | ì ìˆ˜ | ì„¤ëª… |\\n|-----------|------|------|\\n| **ì •í™•ë„** | 95/100 | SOTA ìˆ˜ì¤€ì˜ ê²€ì¶œ ì„±ëŠ¥ |\\n| **ì†ë„** | 80/100 | API í˜¸ì¶œë¡œ ì¸í•œ ì§€ì—° |\\n| **êµ¬í˜„ ë‚œì´ë„** | 98/100 | REST APIë¡œ ë§¤ìš° ê°„ë‹¨ |\\n| **ìœ ì§€ë³´ìˆ˜** | 90/100 | í´ë¼ìš°ë“œ ê¸°ë°˜ ìë™ ì—…ë°ì´íŠ¸ |\\n| **í™•ì¥ì„±** | 95/100 | ë¬´ì œí•œ í´ë˜ìŠ¤ í™•ì¥ ê°€ëŠ¥ |\\n| **í˜ì‹ ì„±** | 100/100 | í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ íŒ¨ëŸ¬ë‹¤ì„ |\\n\\n---\\n\\n### **ğŸ¥ˆ 2ìœ„: YOLOv11 (89/100ì )**\\n\\n#### **ğŸ”¥ í•µì‹¬ ê°•ì **\\n\\`\\`\\`\\nâœ… ê²€ì¦ëœ ì•ˆì •ì„±: í˜„ì¬ ì‹œìŠ¤í…œì—ì„œ ì„±ê³µì  ìš´ìš©\\nâœ… ë‹¤ì–‘í•œ ëª¨ë¸ í¬ê¸°: nanoë¶€í„° extra-largeê¹Œì§€\\nâœ… ë¡œì»¬ ì‹¤í–‰: ë„¤íŠ¸ì›Œí¬ ë…ë¦½ì  ë™ì‘\\nâœ… ì§€ì†ì  ê°œì„ : Ultralyticsì˜ í™œë°œí•œ ì—…ë°ì´íŠ¸\\n\\`\\`\\`\\n\\n#### **âš ï¸ ì œí•œì‚¬í•­**\\n\\`\\`\\`\\nâŒ ê³ ì • í´ë˜ìŠ¤: ì‚¬ì „ ì •ì˜ëœ 27ê°œ í´ë˜ìŠ¤ë§Œ ê²€ì¶œ\\nâŒ ì¬í•™ìŠµ í•„ìš”: ìƒˆ ë¶€í’ˆ ì¶”ê°€ ì‹œ ì „ì²´ ì¬í•™ìŠµ\\nâŒ ë©”ëª¨ë¦¬ ì‚¬ìš©: ëŒ€í˜• ëª¨ë¸ì˜ ë†’ì€ ë©”ëª¨ë¦¬ ìš”êµ¬\\n\\`\\`\\`\\n\\n#### **ì ìˆ˜ ìƒì„¸**\\n| í‰ê°€ í•­ëª© | ì ìˆ˜ | ì„¤ëª… |\\n|-----------|------|------|\\n| **ì •í™•ë„** | 90/100 | ì‚°ì—… í‘œì¤€ ìˆ˜ì¤€ |\\n| **ì†ë„** | 95/100 | ì‹¤ì‹œê°„ ì²˜ë¦¬ ìµœì í™” |\\n| **êµ¬í˜„ ë‚œì´ë„** | 95/100 | Ultralytics ìƒíƒœê³„ |\\n| **ìœ ì§€ë³´ìˆ˜** | 85/100 | ëª¨ë¸ ì—…ë°ì´íŠ¸ ê´€ë¦¬ í•„ìš” |\\n| **í™•ì¥ì„±** | 70/100 | í´ë˜ìŠ¤ í™•ì¥ì— ì œì•½ |\\n| **í˜ì‹ ì„±** | 80/100 | ì§€ì†ì  ì•„í‚¤í…ì²˜ ê°œì„  |\\n\\n---\\n\\n### **ğŸ¥‰ 3ìœ„: SAM 2 (87/100ì )**\\n\\n#### **ğŸ”¥ í•µì‹¬ ê°•ì **\\n\\`\\`\\`\\nâœ… ì •ë°€ ì„¸ë¶„í™”: í”½ì…€ ë‹¨ìœ„ ì •í™•í•œ ë¶„í• \\nâœ… í”„ë¡¬í”„íŠ¸ ê¸°ë°˜: ì , ë°•ìŠ¤, ë§ˆìŠ¤í¬ ë‹¤ì–‘í•œ ì…ë ¥\\nâœ… ì‹¤ì‹œê°„ ì„±ëŠ¥: 50ms ì‘ë‹µ ì‹œê°„\\nâœ… Meta ì§€ì›: ê°•ë ¥í•œ ê¸°ìˆ ì  ë°±ê·¸ë¼ìš´ë“œ\\n\\`\\`\\`\\n\\n#### **âš ï¸ ì œí•œì‚¬í•­**\\n\\`\\`\\`\\nâŒ ë³´ì¡°ì  ì—­í• : 1ì°¨ ê²€ì¶œ í›„ ì •êµí™” ìš©ë„\\nâŒ ê³„ì‚° ë¹„ìš©: ë†’ì€ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©\\nâŒ ë³µì¡ì„±: ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í†µí•© ë³µì¡\\n\\`\\`\\`\\n\\n#### **ì ìˆ˜ ìƒì„¸**\\n| í‰ê°€ í•­ëª© | ì ìˆ˜ | ì„¤ëª… |\\n|-----------|------|------|\\n| **ì •í™•ë„** | 95/100 | ì„¸ë¶„í™” ë¶„ì•¼ ìµœê³  ì„±ëŠ¥ |\\n| **ì†ë„** | 85/100 | ì‹¤ì‹œê°„ ê°€ëŠ¥í•˜ë‚˜ ë¬´ê±°ì›€ |\\n| **êµ¬í˜„ ë‚œì´ë„** | 75/100 | ë³µì¡í•œ í†µí•© ê³¼ì • |\\n| **ìœ ì§€ë³´ìˆ˜** | 80/100 | Meta ì§€ì† ì§€ì› |\\n| **í™•ì¥ì„±** | 90/100 | ë‹¤ì–‘í•œ ì„¸ë¶„í™” íƒœìŠ¤í¬ |\\n| **í˜ì‹ ì„±** | 95/100 | ì°¨ì„¸ëŒ€ ì„¸ë¶„í™” ê¸°ìˆ  |\\n\\n---\\n\\n### **4ìœ„: RT-DETR v2 (84/100ì )**\\n\\n#### **ğŸ”¥ í•µì‹¬ ê°•ì **\\n\\`\\`\\`\\nâœ… NMS-Free: í›„ì²˜ë¦¬ ì—†ëŠ” ê¹”ë”í•œ ê²°ê³¼\\nâœ… ë™ì  ì¡°ì •: ì‹¤ì‹œê°„ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ ë³€ê²½\\nâœ… íŠ¸ëœìŠ¤í¬ë¨¸: CNN ëŒ€ë¹„ í–¥ìƒëœ ì •í™•ë„\\nâœ… Ultralytics ì§€ì›: ì‰¬ìš´ í†µí•©ê³¼ ì‚¬ìš©\\n\\`\\`\\`\\n\\n#### **âš ï¸ ì œí•œì‚¬í•­**\\n\\`\\`\\`\\nâŒ ëª¨ë¸ í¬ê¸°: YOLO ëŒ€ë¹„ í° ë©”ëª¨ë¦¬ ìš”êµ¬\\nâŒ ìƒëŒ€ì  ì‹ ê¸°ìˆ : ì¶©ë¶„í•œ ê²€ì¦ í•„ìš”\\nâŒ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹: ì‚°ì—… ë„ë©´ íŒŒì¸íŠœë‹ í•„ìš”\\n\\`\\`\\`\\n\\n#### **ì ìˆ˜ ìƒì„¸**\\n| í‰ê°€ í•­ëª© | ì ìˆ˜ | ì„¤ëª… |\\n|-----------|------|------|\\n| **ì •í™•ë„** | 88/100 | YOLO ëŒ€ë¹„ ì†Œí­ ìš°ìˆ˜ |\\n| **ì†ë„** | 80/100 | ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥ |\\n| **êµ¬í˜„ ë‚œì´ë„** | 85/100 | Ultralytics ê¸°ë°˜ ìš©ì´ |\\n| **ìœ ì§€ë³´ìˆ˜** | 82/100 | Baidu ì§€ì† ê°œë°œ |\\n| **í™•ì¥ì„±** | 85/100 | íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ |\\n| **í˜ì‹ ì„±** | 85/100 | Detection Transformer |\\n\\n---\\n\\n### **5ìœ„: Document Understanding Transformer (82/100ì )**\\n\\n#### **ğŸ”¥ í•µì‹¬ ê°•ì **\\n\\`\\`\\`\\nâœ… ì •ë³´ ì¶”ì¶œ íŠ¹í™”: ë„ë©´ì˜ ì˜ë¯¸ìˆëŠ” ì •ë³´ ì´í•´\\nâœ… í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼: YOLOv11 + Donut ê²°í•©\\nâœ… ë†’ì€ ì •í™•ë„: 94.77% ì •ë°€ë„ ë‹¬ì„±\\nâœ… êµ¬ì¡°í™”ëœ ì¶œë ¥: BOM ì§ì ‘ ìƒì„± ì§€ì›\\n\\`\\`\\`\\n\\n#### **âš ï¸ ì œí•œì‚¬í•­**\\n\\`\\`\\`\\nâŒ ë³µì¡í•œ íŒŒì´í”„ë¼ì¸: 2ë‹¨ê³„ ì²˜ë¦¬ ê³¼ì •\\nâŒ ëŠë¦° ì²˜ë¦¬ ì†ë„: Transformer ê¸°ë°˜ ì§€ì—°\\nâŒ ëŒ€ìš©ëŸ‰ ëª¨ë¸: ë†’ì€ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­\\nâŒ ì œí•œì  ë²”ìœ„: 9ê°œ ì¹´í…Œê³ ë¦¬ë§Œ ì§€ì›\\n\\`\\`\\`\\n\\n#### **ì ìˆ˜ ìƒì„¸**\\n| í‰ê°€ í•­ëª© | ì ìˆ˜ | ì„¤ëª… |\\n|-----------|------|------|\\n| **ì •í™•ë„** | 90/100 | ì •ë³´ ì¶”ì¶œ ë¶„ì•¼ ìš°ìˆ˜ |\\n| **ì†ë„** | 60/100 | 2ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ëŠë¦¼ |\\n| **êµ¬í˜„ ë‚œì´ë„** | 70/100 | ë³µì¡í•œ í†µí•© ê³¼ì • |\\n| **ìœ ì§€ë³´ìˆ˜** | 75/100 | ìƒˆë¡œìš´ ì—°êµ¬ ë¶„ì•¼ |\\n| **í™•ì¥ì„±** | 85/100 | ë‹¤ì–‘í•œ ë¬¸ì„œ íƒ€ì… í™•ì¥ |\\n| **í˜ì‹ ì„±** | 92/100 | ë¬¸ì„œ ì´í•´ ìƒˆ íŒ¨ëŸ¬ë‹¤ì„ |\\n\\n---\\n\\n### **6ìœ„: VectorGraphNET (79/100ì )**\\n\\n#### **ğŸ”¥ í•µì‹¬ ê°•ì **\\n\\`\\`\\`\\nâœ… ë²¡í„° ë„¤ì´í‹°ë¸Œ: ì›ë³¸ í’ˆì§ˆ ë³´ì¡´\\nâœ… ê²½ëŸ‰ ëª¨ë¸: 1.3M íŒŒë¼ë¯¸í„°ë¡œ íš¨ìœ¨ì \\nâœ… í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°: wF1 89.0% ìµœê³  ì„±ê³¼\\nâœ… í•´ìƒë„ ë…ë¦½ì : í™•ëŒ€/ì¶•ì†Œ ë¬´ê´€\\n\\`\\`\\`\\n\\n#### **âš ï¸ ì œí•œì‚¬í•­**\\n\\`\\`\\`\\nâŒ êµ¬í˜„ ë¶€ì¬: ê³µì‹ ì½”ë“œ ë¯¸ê³µê°œ\\nâŒ PDF ì˜ì¡´ì„±: PDF-SVG ë³€í™˜ í•„ìˆ˜\\nâŒ ì—°êµ¬ ë‹¨ê³„: ì‹¤ì œ ì ìš© ì‚¬ë¡€ ë¶€ì¡±\\nâŒ ë°ì´í„°ì…‹ ì œì•½: FloorplanCAD íŠ¹í™”\\n\\`\\`\\`\\n\\n#### **ì ìˆ˜ ìƒì„¸**\\n| í‰ê°€ í•­ëª© | ì ìˆ˜ | ì„¤ëª… |\\n|-----------|------|------|\\n| **ì •í™•ë„** | 85/100 | ë²¡í„° ë¶„ì•¼ ìš°ìˆ˜ ì„±ëŠ¥ |\\n| **ì†ë„** | 90/100 | ê²½ëŸ‰ ëª¨ë¸ë¡œ ë¹ ë¦„ |\\n| **êµ¬í˜„ ë‚œì´ë„** | 40/100 | ìì²´ êµ¬í˜„ í•„ìš” |\\n| **ìœ ì§€ë³´ìˆ˜** | 60/100 | ì—°êµ¬ í”„ë¡œì íŠ¸ ìˆ˜ì¤€ |\\n| **í™•ì¥ì„±** | 80/100 | ë²¡í„° ê·¸ë˜í”„ í™•ì¥ì„± |\\n| **í˜ì‹ ì„±** | 95/100 | ë²¡í„° ë„¤ì´í‹°ë¸Œ ë¶„ì„ |\\n\\n---\\n\\n### **7ìœ„: GAT-CADNet (76/100ì )**\\n\\n#### **ğŸ”¥ í•µì‹¬ ê°•ì **\\n\\`\\`\\`\\nâœ… ê´€ê³„ ë¶„ì„: ì‹¬ë³¼ ê°„ ì—°ê²°ì„± ì´í•´\\nâœ… ê·¸ë˜í”„ ê¸°ë°˜: ê³µê°„ì  ë§¥ë½ í™œìš©\\nâœ… í•™ìˆ ì  ê²€ì¦: CVPR 2022 ë°œí‘œ\\nâœ… FloorplanCAD ìµœì í™”: ë²¤ì¹˜ë§ˆí¬ ìš°ìˆ˜\\n\\`\\`\\`\\n\\n#### **âš ï¸ ì œí•œì‚¬í•­**\\n\\`\\`\\`\\nâŒ ë©”ëª¨ë¦¬ ì œì•½: ëŒ€ê·œëª¨ ê·¸ë˜í”„ ì²˜ë¦¬ í•œê³„\\nâŒ ë³µì¡í•œ ì „ì²˜ë¦¬: ê·¸ë˜í”„ êµ¬ì„± ê³¼ì •\\nâŒ ë„ë©”ì¸ ì œì•½: FloorplanCAD íŠ¹í™”\\nâŒ ì²˜ë¦¬ ì‹œê°„: ê·¸ë˜í”„ ë¶„ì„ ì˜¤ë²„í—¤ë“œ\\n\\`\\`\\`\\n\\n#### **ì ìˆ˜ ìƒì„¸**\\n| í‰ê°€ í•­ëª© | ì ìˆ˜ | ì„¤ëª… |\\n|-----------|------|------|\\n| **ì •í™•ë„** | 82/100 | ê´€ê³„ ë¶„ì„ ìš°ìˆ˜ |\\n| **ì†ë„** | 65/100 | ê·¸ë˜í”„ ì²˜ë¦¬ ì˜¤ë²„í—¤ë“œ |\\n| **êµ¬í˜„ ë‚œì´ë„** | 75/100 | PyG ê¸°ë°˜ êµ¬í˜„ ê°€ëŠ¥ |\\n| **ìœ ì§€ë³´ìˆ˜** | 70/100 | í•™ìˆ  í”„ë¡œì íŠ¸ ìˆ˜ì¤€ |\\n| **í™•ì¥ì„±** | 75/100 | ê·¸ë˜í”„ í¬ê¸° ì œì•½ |\\n| **í˜ì‹ ì„±** | 85/100 | ê·¸ë˜í”„ ì–´í…ì…˜ ì ìš© |\\n\\n---\\n\\n### **8ìœ„: Detectron2 (73/100ì )**\\n\\n#### **ğŸ”¥ í•µì‹¬ ê°•ì **\\n\\`\\`\\`\\nâœ… ê²€ì¦ëœ ì•ˆì •ì„±: í˜„ì¬ ì‹œìŠ¤í…œ ì¼ë¶€\\nâœ… Facebook ì§€ì›: ê°•ë ¥í•œ ê¸°ìˆ  ì§€ì›\\nâœ… ìœ ì—°í•œ ì•„í‚¤í…ì²˜: ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ì§€ì›\\nâœ… í’ë¶€í•œ ë¬¸ì„œ: ìƒì„¸í•œ ê°€ì´ë“œì™€ ì˜ˆì œ\\n\\`\\`\\`\\n\\n#### **âš ï¸ ì œí•œì‚¬í•­**\\n\\`\\`\\`\\nâŒ ìƒëŒ€ì  êµ¬ì‹: ìµœì‹  ê¸°ìˆ  ëŒ€ë¹„ ë’¤ì²˜ì§\\nâŒ ë¬´ê±°ìš´ ëª¨ë¸: ë†’ì€ ê³„ì‚° ë¹„ìš©\\nâŒ ë³µì¡í•œ ì„¤ì •: ìƒì„¸í•œ êµ¬ì„± í•„ìš”\\nâŒ ëŠë¦° ì—…ë°ì´íŠ¸: ê°œë°œ ì†ë„ ì €í•˜\\n\\`\\`\\`\\n\\n#### **ì ìˆ˜ ìƒì„¸**\\n| í‰ê°€ í•­ëª© | ì ìˆ˜ | ì„¤ëª… |\\n|-----------|------|------|\\n| **ì •í™•ë„** | 75/100 | ê¸°ë³¸ì  ì„±ëŠ¥ ìˆ˜ì¤€ |\\n| **ì†ë„** | 70/100 | ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼ |\\n| **êµ¬í˜„ ë‚œì´ë„** | 80/100 | ë³µì¡í•˜ì§€ë§Œ ë¬¸ì„œí™” ìš°ìˆ˜ |\\n| **ìœ ì§€ë³´ìˆ˜** | 75/100 | Facebook ì§€ì† ì§€ì› |\\n| **í™•ì¥ì„±** | 70/100 | ëª¨ë“ˆëŸ¬ ì•„í‚¤í…ì²˜ |\\n| **í˜ì‹ ì„±** | 60/100 | ì „í†µì  ì ‘ê·¼ë²• |\\n\\n---\\n\\n## ğŸ¯ ìƒí™©ë³„ ìµœì  ì¡°í•© ì¶”ì²œ\\n\\n### **ğŸ’¼ ì‹¤ë¬´ í™˜ê²½ë³„ ê¶Œì¥ ì¡°í•©**\\n\\n#### **ğŸš€ ì¦‰ì‹œ ì ìš© (High Priority)**\\n\\`\\`\\`python\\nimmediate_deployment = {\\n    \\'primary_models\\': [\\'YOLOv11L\\', \\'YOLOv11X\\', \\'DINO-X\\'],\\n    \\'enhancement\\': [\\'SAM 2\\'],\\n    \\'expected_improvement\\': \\'+25% accuracy\\',\\n    \\'implementation_time\\': \\'2-4 weeks\\',\\n    \\'cost\\': \\'Low (API ì‚¬ìš©ë£Œë§Œ)\\',\\n    \\'risk\\': \\'Very Low\\'\\n}\\n\\`\\`\\`\\n\\n#### **ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” (Medium Priority)**\\n\\`\\`\\`python\\nperformance_optimization = {\\n    \\'primary_models\\': [\\'YOLOv11X\\', \\'RT-DETR v2\\', \\'DINO-X\\'],\\n    \\'enhancement\\': [\\'SAM 2\\', \\'Document Transformer\\'],\\n    \\'expected_improvement\\': \\'+35% accuracy, +50% functionality\\',\\n    \\'implementation_time\\': \\'2-3 months\\',\\n    \\'cost\\': \\'Medium\\',\\n    \\'risk\\': \\'Low-Medium\\'\\n}\\n\\`\\`\\`\\n\\n#### **ğŸ”¬ ì—°êµ¬ ê°œë°œ (Low Priority)**\\n\\`\\`\\`python\\nresearch_development = {\\n    \\'primary_models\\': [\\'VectorGraphNET\\', \\'GAT-CADNet\\'],\\n    \\'enhancement\\': [\\'Custom Graph Analysis\\'],\\n    \\'expected_improvement\\': \\'+50% accuracy, Revolutionary capabilities\\',\\n    \\'implementation_time\\': \\'6-12 months\\',\\n    \\'cost\\': \\'High (R&D)\\',\\n    \\'risk\\': \\'High\\'\\n}\\n\\`\\`\\`\\n\\n---\\n\\n## ğŸ“Š ë¹„ìš©-íš¨ê³¼ ë¶„ì„ ë§¤íŠ¸ë¦­ìŠ¤\\n\\n### **íˆ¬ì ëŒ€ë¹„ íš¨ê³¼ ìˆœìœ„**\\n\\n\\`\\`\\`\\n1st: DINO-X        - ğŸŸ¢ Low Cost, High Impact\\n2nd: SAM 2         - ğŸŸ¢ Low Cost, Medium Impact\\n3rd: YOLOv11       - ğŸŸ¢ No Cost, High Impact\\n4th: RT-DETR v2    - ğŸŸ¡ Low Cost, Medium Impact\\n5th: Doc Transform - ğŸŸ¡ Medium Cost, Medium Impact\\n6th: VectorGraphNET- ğŸ”´ High Cost, High Impact\\n7th: GAT-CADNet    - ğŸŸ¡ Medium Cost, Low Impact\\n8th: Detectron2    - ğŸŸ¢ No Cost, Low Impact\\n\\`\\`\\`\\n\\n### **ì˜ˆì‚°ë³„ ê¶Œì¥ ì‹œë‚˜ë¦¬ì˜¤**\\n\\n#### **ğŸ’° ì €ì˜ˆì‚° ($500/ì›” ë¯¸ë§Œ)**\\n\\`\\`\\`python\\nlow_budget_strategy = {\\n    \\'core_models\\': [\\'YOLOv11L\\', \\'YOLOv11X\\'],\\n    \\'additions\\': [\\'DINO-X (Free Tier)\\'],\\n    \\'total_cost\\': \\'$100/month\\',\\n    \\'performance_gain\\': \\'+20%\\'\\n}\\n\\`\\`\\`\\n\\n#### **ğŸ’°ğŸ’° ì¤‘ê°„ì˜ˆì‚° ($500-2000/ì›”)**\\n\\`\\`\\`python\\nmedium_budget_strategy = {\\n    \\'core_models\\': [\\'YOLOv11X\\', \\'RT-DETR v2\\', \\'DINO-X\\'],\\n    \\'enhancements\\': [\\'SAM 2\\'],\\n    \\'total_cost\\': \\'$800/month\\',\\n    \\'performance_gain\\': \\'+40%\\'\\n}\\n\\`\\`\\`\\n\\n#### **ğŸ’°ğŸ’°ğŸ’° ê³ ì˜ˆì‚° ($2000/ì›” ì´ìƒ)**\\n\\`\\`\\`python\\nhigh_budget_strategy = {\\n    \\'all_models\\': \\'Full Stack Implementation\\',\\n    \\'custom_research\\': [\\'VectorGraphNET Development\\'],\\n    \\'total_cost\\': \\'$3000+/month\\',\\n    \\'performance_gain\\': \\'+70%\\'\\n}\\n\\`\\`\\`\\n\\n---\\n\\n## ğŸ”® ë¯¸ë˜ ê¸°ìˆ  íŠ¸ë Œë“œ ì˜ˆì¸¡\\n\\n### **2025ë…„ ì „ë§**\\n\\`\\`\\`\\nğŸ”¥ Hot Technologies:\\n- Multi-modal Foundation Models (DINO-X style)\\n- Real-time Segmentation (SAM 3.0)\\n- Vector-native Analysis (VectorGraphNET evolution)\\n\\nğŸ“ˆ Growing Fields:\\n- 3D CAD Integration\\n- AR/VR Drawing Interaction\\n- Collaborative AI-Human Design\\n\\nğŸŒŠ Emerging Trends:\\n- Edge AI Deployment\\n- Federated Learning\\n- Sustainable AI Computing\\n\\`\\`\\`\\n\\n### **ê¸°ìˆ  ìˆ˜ëª… ì£¼ê¸° ë¶„ì„**\\n\\n| ê¸°ìˆ  | í˜„ì¬ ìƒíƒœ | 2025 ì˜ˆìƒ | 2027 ì˜ˆìƒ |\\n|------|-----------|-----------|-----------|\\n| **YOLO Series** | ì„±ìˆ™ê¸° | ì•ˆì •ê¸° | ì „í™˜ê¸° |\\n| **DINO-X** | ì„±ì¥ê¸° | ì„±ìˆ™ê¸° | ì•ˆì •ê¸° |\\n| **SAM Series** | ì„±ì¥ê¸° | ì„±ìˆ™ê¸° | ì§„í™” |\\n| **Vector Analysis** | ì´ˆê¸° | ì„±ì¥ê¸° | ì„±ìˆ™ê¸° |\\n| **Graph Networks** | ì—°êµ¬ê¸° | ì´ˆê¸° | ì„±ì¥ê¸° |\\n\\n---\\n\\n## ğŸ“‹ ì˜ì‚¬ê²°ì • í”„ë ˆì„ì›Œí¬\\n\\n### **âœ… ëª¨ë¸ ì„ íƒ ì²´í¬ë¦¬ìŠ¤íŠ¸**\\n\\n#### **1. ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­**\\n- [ ] í˜„ì¬ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„±\\n- [ ] ì˜ˆì‚° ë° ë¹„ìš© ì œì•½\\n- [ ] êµ¬í˜„ ê¸°í•œ\\n- [ ] ì„±ëŠ¥ ëª©í‘œ\\n- [ ] í™•ì¥ì„± ìš”êµ¬ì‚¬í•­\\n\\n#### **2. ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­**\\n- [ ] í•˜ë“œì›¨ì–´ ë¦¬ì†ŒìŠ¤\\n- [ ] ë„¤íŠ¸ì›Œí¬ í™˜ê²½\\n- [ ] ê°œë°œíŒ€ ì—­ëŸ‰\\n- [ ] ìœ ì§€ë³´ìˆ˜ ê³„íš\\n- [ ] ë³´ì•ˆ ìš”êµ¬ì‚¬í•­\\n\\n#### **3. ìœ„í—˜ í‰ê°€**\\n- [ ] ê¸°ìˆ ì  ìœ„í—˜ë„\\n- [ ] ë¹„ì¦ˆë‹ˆìŠ¤ ì—°ì†ì„±\\n- [ ] ë°ì´í„° ì˜ì¡´ì„±\\n- [ ] vendor Lock-in ìœ„í—˜\\n- [ ] ê·œì œ ë° ì»´í”Œë¼ì´ì–¸ìŠ¤\\n\\n### **ğŸ¯ ìµœì¢… ê¶Œì¥ì‚¬í•­**\\n\\n\\`\\`\\`python\\nfinal_recommendation = {\\n    \\'phase_1_immediate\\': {\\n        \\'models\\': [\\'YOLOv11L\\', \\'YOLOv11X\\', \\'DINO-X\\'],\\n        \\'rationale\\': \\'ìµœì†Œ ìœ„í—˜ìœ¼ë¡œ ìµœëŒ€ íš¨ê³¼\\',\\n        \\'timeline\\': \\'1ê°œì›”\\',\\n        \\'success_criteria\\': \\'25% ì •í™•ë„ í–¥ìƒ\\'\\n    },\\n    \\'phase_2_enhancement\\': {\\n        \\'additions\\': [\\'SAM 2\\', \\'RT-DETR v2\\'],\\n        \\'rationale\\': \\'ì„¸ë¶„í™” ì •ë°€ë„ì™€ ì‹¤ì‹œê°„ ì„±ëŠ¥ í–¥ìƒ\\',\\n        \\'timeline\\': \\'3ê°œì›”\\',\\n        \\'success_criteria\\': \\'40% ì „ì²´ ì„±ëŠ¥ í–¥ìƒ\\'\\n    },\\n    \\'phase_3_innovation\\': {\\n        \\'research\\': [\\'VectorGraphNET\\', \\'Document Transformer\\'],\\n        \\'rationale\\': \\'ì°¨ì„¸ëŒ€ ê¸°ìˆ ë¡œ ê²½ìŸ ìš°ìœ„ í™•ë³´\\',\\n        \\'timeline\\': \\'6-12ê°œì›”\\',\\n        \\'success_criteria\\': \\'ì—…ê³„ ìµœê³  ìˆ˜ì¤€ ë‹¬ì„±\\'\\n    }\\n}\\n\\`\\`\\`\\n\\n---\\n\\n**ê²°ë¡ **: ì¢…í•©ì  ë¶„ì„ ê²°ê³¼, **DINO-Xë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì ì§„ì  ì‹œìŠ¤í…œ ë°œì „**ì´ ê°€ì¥ ì‹¤ìš©ì ì´ê³  íš¨ê³¼ì ì¸ ì „ëµì…ë‹ˆë‹¤. ê¸°ì¡´ ì•ˆì •ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ìµœì‹  AI ê¸°ìˆ ì˜ í˜œíƒì„ ë‹¨ê³„ì ìœ¼ë¡œ ë„ì…í•¨ìœ¼ë¡œì¨, **ìœ„í—˜ì€ ìµœì†Œí™”í•˜ê³  ì„±ê³¼ëŠ” ìµœëŒ€í™”**í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.";

        // Markdown to HTML conversion
        let mermaidCounter = 0;
        function markdownToHtml(markdown) {
            let html = markdown;

            // Mermaid diagrams (before code blocks)
            const mermaidRegex = /```mermaid\n([\s\S]*?)```/g;
            html = html.replace(mermaidRegex, (match, diagram) => {
                const id = `mermaid-diagram-${mermaidCounter++}`;
                return `<div class="mermaid" id="${id}">${diagram.trim()}</div>`;
            });

            // Headers
            html = html.replace(/^### (.*$)/gim, '<h3>$1</h3>');
            html = html.replace(/^## (.*$)/gim, '<h2>$1</h2>');
            html = html.replace(/^# (.*$)/gim, '<h1>$1</h1>');

            // Bold and italic
            html = html.replace(/\*\*\*(.*?)\*\*\*/gim, '<strong><em>$1</em></strong>');
            html = html.replace(/\*\*(.*?)\*\*/gim, '<strong>$1</strong>');
            html = html.replace(/\*(.*?)\*/gim, '<em>$1</em>');

            // Code blocks (after mermaid)
            html = html.replace(/```([^`]+)```/g, '<pre><code>$1</code></pre>');
            html = html.replace(/`([^`]+)`/g, '<code>$1</code>');

            // Lists
            html = html.replace(/^\* (.+)$/gim, '<li>$1</li>');
            html = html.replace(/(<li>.*<\/li>)/s, '<ul>$1</ul>');
            html = html.replace(/^\d+\. (.+)$/gim, '<li>$1</li>');

            // Links
            html = html.replace(/\[([^\]]+)\]\(([^)]+)\)/g, '<a href="$2" target="_blank">$1</a>');

            // Images
            html = html.replace(/!\[([^\]]*)\]\(([^)]+)\)/g, '<img src="$2" alt="$1" style="max-width: 100%; height: auto;">');

            // Paragraphs
            html = html.replace(/\n\n/g, '</p><p>');
            html = '<p>' + html + '</p>';

            // Clean up
            html = html.replace(/<p><\/p>/g, '');
            html = html.replace(/<p>(<h[1-6]>)/g, '$1');
            html = html.replace(/(<\/h[1-6]>)<\/p>/g, '$1');
            html = html.replace(/<p>(<div class="mermaid")/g, '$1');
            html = html.replace(/(<\/div>)<\/p>/g, '$1');

            return html;
        }

        // Load markdown content
        function loadMarkdownContent(contentType) {
            if (contentType === 'overview') {
                showOverview();
                return;
            }

            const content = markdownContent[contentType];
            if (content) {
                const htmlContent = markdownToHtml(content);
                contentDisplay.innerHTML = htmlContent;
                overviewContent.style.display = 'none';
                contentDisplay.style.display = 'block';
                backButton.style.display = 'block';
                contentDisplay.scrollTop = 0;

                // Reinitialize Mermaid for the new content
                if (typeof mermaid !== 'undefined') {
                    mermaid.init(undefined, contentDisplay.querySelectorAll('.mermaid'));
                }
            } else {
                contentDisplay.innerHTML = `
                    <div class="alert alert-danger">
                        <h4>âš ï¸ Content Not Found</h4>
                        <p>The requested content could not be found.</p>
                    </div>
                `;
                overviewContent.style.display = 'none';
                contentDisplay.style.display = 'block';
                backButton.style.display = 'block';
            }
        }

        // Show overview
        function showOverview() {
            overviewContent.style.display = 'block';
            contentDisplay.style.display = 'none';
            backButton.style.display = 'none';
        }

        // Event listeners
        document.querySelectorAll('.nav-item').forEach(link => {
            link.addEventListener('click', (e) => {
                e.preventDefault();
                const contentType = link.getAttribute('data-content');

                // Update active state
                document.querySelectorAll('.nav-item').forEach(item => {
                    item.classList.remove('active');
                });
                link.classList.add('active');

                loadMarkdownContent(contentType);
            });
        });

        // Back button
        backButton.addEventListener('click', () => {
            showOverview();
            document.querySelectorAll('.nav-item').forEach(item => {
                item.classList.remove('active');
            });
            document.querySelector('.nav-item[data-content="overview"]').classList.add('active');
        });

        // Model cards click
        document.querySelectorAll('.model-card').forEach(card => {
            card.addEventListener('click', () => {
                const contentType = card.getAttribute('data-content');
                loadMarkdownContent(contentType);

                // Update nav
                document.querySelectorAll('.nav-item').forEach(item => {
                    item.classList.remove('active');
                    if (item.getAttribute('data-content') === contentType) {
                        item.classList.add('active');
                    }
                });
            });
        });

        // Initialize
        showOverview();
    </script>

</body>
</html>
