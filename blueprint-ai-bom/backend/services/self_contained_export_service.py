"""Self-contained Export Service - Docker ì´ë¯¸ì§€ í¬í•¨ Export

Phase 2F: Self-contained Export íŒ¨í‚¤ì§€ ìƒì„±
- Docker ì´ë¯¸ì§€ Export
- docker-compose.yml ë™ì  ìƒì„± (í¬íŠ¸ ì˜¤í”„ì…‹ ì ìš©)
- Import ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
"""

import base64
import json
import mimetypes
import os
import subprocess
import tempfile
import uuid
import zipfile
import logging
import shutil
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List

import yaml

from schemas.export import (
    SelfContainedExportRequest,
    SelfContainedExportResponse,
    SelfContainedPreview,
    DockerImageInfo,
)

logger = logging.getLogger(__name__)


class SelfContainedExportService:
    """Self-contained Export ì„œë¹„ìŠ¤ (Docker ì´ë¯¸ì§€ í¬í•¨)"""

    # ë…¸ë“œ íƒ€ì… â†’ Docker ì„œë¹„ìŠ¤ ë§¤í•‘
    NODE_TO_SERVICE_MAP = {
        "yolo": "yolo-api",
        "edocr2": "edocr2-v2-api",
        "paddleocr": "paddleocr-api",
        "skinmodel": "skinmodel-api",
        "vl": "vl-api",
        "edgnet": "edgnet-api",
        "knowledge": "knowledge-api",
        "tesseract": "tesseract-api",
        "trocr": "trocr-api",
        "esrgan": "esrgan-api",
        "ocr-ensemble": "ocr-ensemble-api",
        "surya-ocr": "surya-ocr-api",
        "doctr": "doctr-api",
        "easyocr": "easyocr-api",
        "line-detector": "line-detector-api",
        "table-detector": "table-detector-api",
        "pid-analyzer": "pid-analyzer-api",
        "design-checker": "design-checker-api",
        "pid-composer": "pid-composer-api",
        "blueprint-ai-bom": "blueprint-ai-bom-backend",
    }

    # ì„œë¹„ìŠ¤ â†’ ì›ë³¸ í¬íŠ¸ ë§¤í•‘
    SERVICE_PORT_MAP = {
        "gateway-api": 8000,
        "edocr2-v2-api": 5002,
        "edgnet-api": 5012,
        "skinmodel-api": 5003,
        "vl-api": 5004,
        "yolo-api": 5005,
        "paddleocr-api": 5006,
        "knowledge-api": 5007,
        "tesseract-api": 5008,
        "trocr-api": 5009,
        "esrgan-api": 5010,
        "ocr-ensemble-api": 5011,
        "surya-ocr-api": 5013,
        "doctr-api": 5014,
        "easyocr-api": 5015,
        "line-detector-api": 5016,
        "pid-analyzer-api": 5018,
        "design-checker-api": 5019,
        "blueprint-ai-bom-backend": 5020,
        "blueprint-ai-bom-frontend": 3000,  # BOM í”„ë¡ íŠ¸ì—”ë“œ
        "pid-composer-api": 5021,
        "table-detector-api": 5022,
        "web-ui": 5173,  # BlueprintFlow í¸ì§‘ê¸° (ì˜µì…˜)
    }

    # ë°±ì—”ë“œ â†’ í”„ë¡ íŠ¸ì—”ë“œ ìë™ í¬í•¨ ë§¤í•‘
    # ë°±ì—”ë“œê°€ í¬í•¨ë˜ë©´ í•´ë‹¹ í”„ë¡ íŠ¸ì—”ë“œë„ ìë™ìœ¼ë¡œ í¬í•¨
    BACKEND_TO_FRONTEND_MAP = {
        "blueprint-ai-bom-backend": "blueprint-ai-bom-frontend",
    }

    # í”„ë¡ íŠ¸ì—”ë“œ ì„œë¹„ìŠ¤ ëª©ë¡ (docker-compose ìƒì„± ì‹œ íŠ¹ë³„ ì²˜ë¦¬)
    FRONTEND_SERVICES = {"blueprint-ai-bom-frontend", "web-ui"}

    # ì˜µì…˜ ì„œë¹„ìŠ¤ (ê¸°ë³¸ì ìœ¼ë¡œ í¬í•¨ë˜ì§€ ì•ŠìŒ, ìš”ì²­ ì‹œì—ë§Œ í¬í•¨)
    OPTIONAL_SERVICES = {
        "web-ui": {
            "description": "BlueprintFlow í¸ì§‘ê¸° (ì›Œí¬í”Œë¡œìš° í¸ì§‘ í•„ìš” ì‹œ)",
            "port": 5173,
            "depends_on": ["gateway-api"],
        }
    }

    # ì„¸ì…˜ features â†’ Docker ì„œë¹„ìŠ¤ ë§¤í•‘
    # Blueprint AI BOM ì„¸ì…˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” feature ì´ë¦„ì„ ì„œë¹„ìŠ¤ë¡œ ë³€í™˜
    # ì£¼ì˜: dimension_ocrëŠ” YOLO í›„ì²˜ë¦¬ë¡œ ë³„ë„ OCR ì„œë¹„ìŠ¤ ë¶ˆí•„ìš”
    FEATURE_TO_SERVICE_MAP = {
        "symbol_detection": ["yolo-api"],
        # dimension_ocr: YOLO ê²€ì¶œ ê²°ê³¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë³„ë„ ì„œë¹„ìŠ¤ ë¶ˆí•„ìš”)
        "text_ocr": ["paddleocr-api", "tesseract-api"],
        "general_ocr": ["paddleocr-api"],
        "table_extraction": ["table-detector-api"],
        "pid_analysis": ["pid-analyzer-api", "line-detector-api"],
        "design_check": ["design-checker-api"],
        "tolerance_analysis": ["skinmodel-api"],
        "knowledge_graph": ["knowledge-api"],
        "vl_classification": ["vl-api"],
        "edge_detection": ["edgnet-api"],
        "image_enhancement": ["esrgan-api"],
    }

    def __init__(self, export_dir: Path, upload_dir: Path):
        self.export_dir = export_dir
        self.upload_dir = upload_dir
        self.export_dir.mkdir(parents=True, exist_ok=True)

    def get_mapped_port(self, service: str, port_offset: int) -> int:
        """í¬íŠ¸ ì˜¤í”„ì…‹ì´ ì ìš©ëœ í¬íŠ¸ ë°˜í™˜"""
        original_port = self.SERVICE_PORT_MAP.get(service, 5000)
        return original_port + port_offset

    def detect_required_services(
        self,
        workflow_definition: Dict[str, Any],
        include_web_ui: bool = False,
        session_features: Optional[List[str]] = None
    ) -> List[str]:
        """ì›Œí¬í”Œë¡œìš°ì—ì„œ í•„ìš”í•œ Docker ì„œë¹„ìŠ¤ ì¶”ì¶œ

        Args:
            workflow_definition: ì›Œí¬í”Œë¡œìš° ì •ì˜
            include_web_ui: web-ui (BlueprintFlow í¸ì§‘ê¸°) í¬í•¨ ì—¬ë¶€
            session_features: ì„¸ì…˜ì˜ features ë°°ì—´ (Blueprint AI BOM ìš©)

        ë°±ì—”ë“œ ì„œë¹„ìŠ¤ê°€ í¬í•¨ë˜ë©´ í•´ë‹¹ í”„ë¡ íŠ¸ì—”ë“œë„ ìë™ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤.
        """
        services = {"gateway-api"}  # GatewayëŠ” í•­ìƒ í•„ìš”

        # 1. ì›Œí¬í”Œë¡œìš° ë…¸ë“œì—ì„œ ì„œë¹„ìŠ¤ ì¶”ì¶œ
        nodes = workflow_definition.get("nodes", [])
        for node in nodes:
            node_type = node.get("type", "").lower().replace("_", "-")

            if node_type in self.NODE_TO_SERVICE_MAP:
                services.add(self.NODE_TO_SERVICE_MAP[node_type])
            elif node_type == "edocr":
                services.add("edocr2-v2-api")
            elif node_type in ("paddle", "paddle-ocr"):
                services.add("paddleocr-api")
            elif node_type in ("bom", "ai-bom"):
                services.add("blueprint-ai-bom-backend")

        # 2. ì„¸ì…˜ featuresì—ì„œ ì„œë¹„ìŠ¤ ì¶”ì¶œ (Blueprint AI BOM ì„¸ì…˜ìš©)
        if session_features:
            # Blueprint AI BOMì—ì„œ Exportí•˜ëŠ” ê²½ìš° í•­ìƒ BOM ë°±ì—”ë“œ í¬í•¨
            services.add("blueprint-ai-bom-backend")

            for feature in session_features:
                feature_key = feature.lower().replace("-", "_")
                if feature_key in self.FEATURE_TO_SERVICE_MAP:
                    # ì²« ë²ˆì§¸ ì„œë¹„ìŠ¤ë§Œ ì¶”ê°€ (ê¸°ë³¸ ì„œë¹„ìŠ¤)
                    services.add(self.FEATURE_TO_SERVICE_MAP[feature_key][0])

        # ë°±ì—”ë“œê°€ í¬í•¨ë˜ë©´ í”„ë¡ íŠ¸ì—”ë“œë„ ìë™ í¬í•¨
        frontends_to_add = set()
        for service in services:
            if service in self.BACKEND_TO_FRONTEND_MAP:
                frontends_to_add.add(self.BACKEND_TO_FRONTEND_MAP[service])
        services.update(frontends_to_add)

        # ì˜µì…˜: web-ui (BlueprintFlow í¸ì§‘ê¸°) í¬í•¨
        if include_web_ui:
            services.add("web-ui")

        return sorted(list(services))

    def get_workflow_node_types(
        self,
        workflow_definition: Dict[str, Any]
    ) -> List[str]:
        """ì›Œí¬í”Œë¡œìš°ì—ì„œ ì‚¬ìš©ëœ ë…¸ë“œ íƒ€ì… ì¶”ì¶œ"""
        nodes = workflow_definition.get("nodes", [])
        node_types = set()
        for node in nodes:
            node_type = node.get("type", "")
            if node_type:
                node_types.add(node_type)
        return sorted(list(node_types))

    def get_docker_image_size(self, service_name: str, source_prefix: str = "") -> float:
        """Docker ì´ë¯¸ì§€ í¬ê¸° ì¡°íšŒ (MB)

        Args:
            service_name: ì„œë¹„ìŠ¤ ì´ë¦„ (ì˜ˆ: yolo-api)
            source_prefix: ì†ŒìŠ¤ ì´ë¯¸ì§€ ì ‘ë‘ì‚¬ (ì˜ˆ: poc_, poc-)
        """
        # ì—¬ëŸ¬ ì´ë¯¸ì§€ ì´ë¦„ í˜•ì‹ ì‹œë„ (prefix ìˆëŠ” ê²ƒ, ì—†ëŠ” ê²ƒ)
        image_names_to_try = [
            f"{source_prefix}{service_name}:latest",
            f"{service_name}:latest",
        ]

        for image_name in image_names_to_try:
            try:
                result = subprocess.run(
                    ["docker", "image", "inspect", image_name,
                     "--format", "{{.Size}}"],
                    capture_output=True, text=True, timeout=10
                )
                if result.returncode == 0:
                    size_bytes = int(result.stdout.strip())
                    return round(size_bytes / (1024 * 1024), 2)
            except Exception as e:
                logger.debug(f"Image not found: {image_name}")
                continue

        logger.warning(f"Failed to get image size for {service_name}")
        return 0.0

    def export_docker_images(
        self,
        services: List[str],
        output_dir: Path,
        compress: bool,
        port_offset: int,
        source_prefix: str = ""
    ) -> Dict[str, DockerImageInfo]:
        """Docker ì´ë¯¸ì§€ë¥¼ tar íŒŒì¼ë¡œ ì €ì¥

        Args:
            services: ì„œë¹„ìŠ¤ ëª©ë¡
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            compress: gzip ì••ì¶• ì—¬ë¶€
            port_offset: í¬íŠ¸ ì˜¤í”„ì…‹
            source_prefix: ì†ŒìŠ¤ ì´ë¯¸ì§€ ì ‘ë‘ì‚¬ (ì˜ˆ: poc_, poc-)
        """
        results = {}
        output_dir.mkdir(parents=True, exist_ok=True)

        for service in services:
            # ì—¬ëŸ¬ ì´ë¯¸ì§€ ì´ë¦„ í˜•ì‹ ì‹œë„
            image_names_to_try = [
                f"{source_prefix}{service}:latest",
                f"{service}:latest",
            ]

            found_image = None
            for img_name in image_names_to_try:
                # ì´ë¯¸ì§€ ì¡´ì¬ í™•ì¸
                check_result = subprocess.run(
                    ["docker", "image", "inspect", img_name],
                    capture_output=True, text=True
                )
                if check_result.returncode == 0:
                    found_image = img_name
                    break

            if not found_image:
                logger.warning(f"Docker image not found for {service}, skipping...")
                continue

            # ì¶œë ¥ íŒŒì¼ì€ í•­ìƒ í‘œì¤€ ì´ë¦„ ì‚¬ìš© (prefix ì—†ì´)
            target_image_name = f"{service}:latest"
            file_ext = ".tar.gz" if compress else ".tar"
            output_file = output_dir / f"{service}{file_ext}"

            try:
                # ì†ŒìŠ¤ ì´ë¯¸ì§€ë¥¼ í‘œì¤€ ì´ë¦„ìœ¼ë¡œ íƒœê·¸ (import ì‹œ ì¼ê´€ì„± ìœ„í•´)
                if found_image != target_image_name:
                    subprocess.run(
                        ["docker", "tag", found_image, target_image_name],
                        check=True, timeout=30
                    )
                    logger.info(f"[Export] Tagged {found_image} as {target_image_name}")

                # ì´ë¯¸ì§€ ì €ì¥
                if compress:
                    cmd = f"docker save {target_image_name} | gzip > {output_file}"
                    subprocess.run(cmd, shell=True, check=True, timeout=600)
                else:
                    subprocess.run(
                        ["docker", "save", target_image_name, "-o", str(output_file)],
                        check=True, timeout=600
                    )

                size_mb = round(output_file.stat().st_size / (1024 * 1024), 2)
                original_port = self.SERVICE_PORT_MAP.get(service, 5000)
                mapped_port = original_port + port_offset

                results[service] = DockerImageInfo(
                    service_name=service,
                    image_name=target_image_name,
                    file_name=output_file.name,
                    size_mb=size_mb,
                    original_port=original_port,
                    mapped_port=mapped_port
                )
                logger.info(f"[Export] Docker image saved: {service} ({size_mb} MB)")

            except subprocess.CalledProcessError as e:
                logger.error(f"Failed to export Docker image {service}: {e}")
            except subprocess.TimeoutExpired:
                logger.error(f"Timeout exporting Docker image {service}")

        return results

    def generate_docker_compose(
        self,
        services: List[str],
        output_path: Path,
        port_offset: int,
        container_prefix: str
    ) -> str:
        """docker-compose.yml ìƒì„± (í¬íŠ¸ ì˜¤í”„ì…‹ ë° ì»¨í…Œì´ë„ˆ ì ‘ë‘ì‚¬ ì ìš©)"""
        compose_content = {
            "version": "3.8",
            "services": {},
            "networks": {
                "imported_network": {
                    "name": f"{container_prefix}_network",
                    "driver": "bridge"
                }
            }
        }

        for service in services:
            original_port = self.SERVICE_PORT_MAP.get(service, 5000)
            mapped_port = original_port + port_offset
            container_name = f"{container_prefix}-{service}"

            # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë‚´ë¶€ URLë„ ì˜¤í”„ì…‹ ì ìš©
            env_vars = ["PYTHONUNBUFFERED=1"]

            if service == "gateway-api":
                env_vars.extend([
                    f"GATEWAY_PORT={original_port}",
                    "GATEWAY_WORKERS=1",
                ])
                # Gatewayê°€ ë‹¤ë¥¸ ì„œë¹„ìŠ¤ í˜¸ì¶œ ì‹œ ì˜¤í”„ì…‹ ì ìš©ëœ URL ì‚¬ìš©
                for svc in services:
                    if svc != "gateway-api":
                        svc_port = self.SERVICE_PORT_MAP.get(svc, 5000)
                        svc_name = f"{container_prefix}-{svc}"
                        env_key = svc.upper().replace("-", "_") + "_URL"
                        env_vars.append(f"{env_key}=http://{svc_name}:{svc_port}")

            elif service == "yolo-api":
                env_vars.append(f"YOLO_API_PORT={original_port}")
            elif service == "edocr2-v2-api":
                env_vars.append(f"EDOCR2_PORT={original_port}")
            elif service == "paddleocr-api":
                env_vars.extend([f"PADDLEOCR_PORT={original_port}", "USE_GPU=false"])
            elif service == "blueprint-ai-bom-backend":
                yolo_container = f"{container_prefix}-yolo-api"
                env_vars.extend([
                    f"BOM_PORT={original_port}",
                    f"YOLO_API_URL=http://{yolo_container}:5005"
                ])
            elif service == "blueprint-ai-bom-frontend":
                # í”„ë¡ íŠ¸ì—”ë“œëŠ” íŠ¹ë³„ ì²˜ë¦¬ (ì•„ë˜ì—ì„œ ë³„ë„ ìƒì„±)
                pass
            else:
                # ê¸°ë³¸ í¬íŠ¸ í™˜ê²½ë³€ìˆ˜
                port_env_key = service.upper().replace("-", "_") + "_PORT"
                env_vars.append(f"{port_env_key}={original_port}")

            # í”„ë¡ íŠ¸ì—”ë“œ ì„œë¹„ìŠ¤ëŠ” ë³„ë„ ì²˜ë¦¬
            if service in self.FRONTEND_SERVICES:
                backend_service = None
                # í•´ë‹¹ í”„ë¡ íŠ¸ì—”ë“œì˜ ë°±ì—”ë“œ ì°¾ê¸°
                for backend, frontend in self.BACKEND_TO_FRONTEND_MAP.items():
                    if frontend == service:
                        backend_service = backend
                        break

                backend_container = f"{container_prefix}-{backend_service}" if backend_service else None

                compose_content["services"][service] = {
                    "image": f"{service}:latest",
                    "container_name": container_name,
                    "ports": [f"{mapped_port}:80"],  # nginxëŠ” 80 í¬íŠ¸
                    "environment": [
                        # nginxê°€ ë°±ì—”ë“œë¡œ í”„ë¡ì‹œí•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
                        f"BACKEND_URL=http://{backend_container}:5020" if backend_container else "",
                    ],
                    "depends_on": [backend_service] if backend_service and backend_service in services else [],
                    "networks": ["imported_network"],
                    "restart": "unless-stopped"
                }
                # ë¹ˆ í™˜ê²½ë³€ìˆ˜ ì œê±°
                compose_content["services"][service]["environment"] = [
                    e for e in compose_content["services"][service]["environment"] if e
                ]
                continue  # ë‹¤ìŒ ì„œë¹„ìŠ¤ë¡œ

            compose_content["services"][service] = {
                "image": f"{service}:latest",
                "container_name": container_name,
                "ports": [f"{mapped_port}:{original_port}"],
                "environment": env_vars,
                "networks": ["imported_network"],
                "restart": "unless-stopped"
            }

        with open(output_path, "w") as f:
            yaml.dump(compose_content, f, default_flow_style=False, sort_keys=False)

        return str(output_path)

    def generate_import_scripts(
        self,
        output_dir: Path,
        services: List[str],
        port_offset: int,
        container_prefix: str
    ) -> None:
        """Import ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        output_dir.mkdir(parents=True, exist_ok=True)

        # Nginx ì„¤ì • íŒŒì¼ ìƒì„± (Frontendìš©)
        if "blueprint-ai-bom-frontend" in services:
            self._generate_nginx_config(output_dir, container_prefix)

        # BOM Backend í¬íŠ¸ ê³„ì‚°
        bom_backend_port = self.SERVICE_PORT_MAP.get("blueprint-ai-bom-backend", 5020) + port_offset

        # Linux/macOS import.sh
        sh_script = f'''#!/bin/bash
set -e

echo "=========================================="
echo "  Blueprint AI BOM - Self-contained Import"
echo "  (Port Offset: +{port_offset})"
echo "=========================================="
echo ""

SCRIPT_DIR="$(cd "$(dirname "${{BASH_SOURCE[0]}}")" && pwd)"
cd "$SCRIPT_DIR/.."

# [1/5] Docker ì´ë¯¸ì§€ ë¡œë“œ
echo "[1/5] Loading Docker images..."
for img in docker/images/*.tar.gz; do
    if [ -f "$img" ]; then
        echo "  Loading: $(basename "$img")"
        gunzip -c "$img" | docker load
    fi
done

for img in docker/images/*.tar; do
    if [ -f "$img" ]; then
        echo "  Loading: $(basename "$img")"
        docker load -i "$img"
    fi
done

# [2/5] Docker ë„¤íŠ¸ì›Œí¬ ìƒì„±
echo ""
echo "[2/5] Creating Docker network..."
docker network create {container_prefix}_network 2>/dev/null || echo "  Network already exists"

# [3/5] ì„œë¹„ìŠ¤ ì‹œì‘
echo ""
echo "[3/5] Starting services..."
cd docker
docker-compose up -d

# [4/5] Frontend Nginx ì„¤ì • ì—…ë°ì´íŠ¸
FRONTEND_CONTAINER="{container_prefix}-blueprint-ai-bom-frontend"
if docker ps --format "{{{{.Names}}}}" | grep -q "$FRONTEND_CONTAINER"; then
    echo ""
    echo "[4/5] Updating frontend nginx configuration..."
    docker cp ../scripts/nginx.conf "$FRONTEND_CONTAINER":/etc/nginx/conf.d/default.conf
    docker exec "$FRONTEND_CONTAINER" nginx -s reload 2>/dev/null || true
    echo "  Nginx configuration updated"
fi

# [5/5] ì„¸ì…˜ ë°ì´í„° ìë™ ë³µì›
cd "$SCRIPT_DIR/.."
if [ -f "session_import.json" ]; then
    echo ""
    echo "[5/5] Restoring session data..."

    # ë°±ì—”ë“œê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
    for i in {{1..30}}; do
        if curl -s "http://localhost:{bom_backend_port}/health" | grep -q "healthy"; then
            break
        fi
        echo "  Waiting for backend to be ready... ($i/30)"
        sleep 1
    done

    # ì„¸ì…˜ Import
    IMPORT_RESULT=$(curl -s -X POST "http://localhost:{bom_backend_port}/sessions/import" \\
        -F "file=@session_import.json" 2>/dev/null || echo "failed")

    if echo "$IMPORT_RESULT" | grep -q "session_id"; then
        SESSION_ID=$(echo "$IMPORT_RESULT" | grep -o '"session_id":"[^"]*"' | cut -d'"' -f4)
        echo "  âœ… Session restored: $SESSION_ID"
    else
        echo "  âš ï¸  Session restore failed (you can manually import session_import.json)"
    fi
else
    echo ""
    echo "[5/5] No session data to restore (session_import.json not found)"
fi

echo ""
echo "=========================================="
echo "  Import Complete!"
echo "=========================================="
echo ""
echo "ì»¨í…Œì´ë„ˆ ì ‘ë‘ì‚¬: {container_prefix}-"
echo "í¬íŠ¸ ì˜¤í”„ì…‹: +{port_offset}"
echo ""
'''
        # í”„ë¡ íŠ¸ì—”ë“œ URL ë¨¼ì € í‘œì‹œ
        frontend_services = [s for s in services if s in self.FRONTEND_SERVICES]
        backend_services = [s for s in services if s not in self.FRONTEND_SERVICES]

        if frontend_services:
            sh_script += 'echo "=========================================="\n'
            sh_script += 'echo "  UI ì ‘ì† URL:"\n'
            sh_script += 'echo "=========================================="\n'
            for service in frontend_services:
                original_port = self.SERVICE_PORT_MAP.get(service, 3000)
                mapped_port = original_port + port_offset
                sh_script += f'echo "  â˜… http://localhost:{mapped_port}"\n'
            sh_script += 'echo ""\n'

        sh_script += 'echo "API endpoints:"\n'
        for service in backend_services:
            original_port = self.SERVICE_PORT_MAP.get(service, 5000)
            mapped_port = original_port + port_offset
            sh_script += f'echo "  - {container_prefix}-{service}: http://localhost:{mapped_port}"\n'

        sh_script += f'''
echo ""
echo "ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸:"
echo "  cd docker && docker-compose ps"
echo ""
echo "ì„œë¹„ìŠ¤ ì¤‘ì§€:"
echo "  cd docker && docker-compose down"
'''

        sh_path = output_dir / "import.sh"
        with open(sh_path, "w", newline="\n") as f:
            f.write(sh_script)
        os.chmod(sh_path, 0o755)

        # Windows import.ps1
        ps_script = f'''# Blueprint AI BOM - Self-contained Import (Windows)
# Port Offset: +{port_offset}

$ErrorActionPreference = "Stop"

Write-Host "==========================================" -ForegroundColor Cyan
Write-Host "  Blueprint AI BOM - Self-contained Import" -ForegroundColor Cyan
Write-Host "  (Port Offset: +{port_offset})" -ForegroundColor Cyan
Write-Host "==========================================" -ForegroundColor Cyan
Write-Host ""

$ScriptDir = Split-Path -Parent $MyInvocation.MyCommand.Path
Set-Location "$ScriptDir\\.."
$RootDir = Get-Location

# [1/5] Docker ì´ë¯¸ì§€ ë¡œë“œ
Write-Host "[1/5] Loading Docker images..." -ForegroundColor Yellow

$gzFiles = Get-ChildItem -Path "docker\\images\\*.tar.gz" -ErrorAction SilentlyContinue
foreach ($file in $gzFiles) {{
    Write-Host "  Loading: $($file.Name)"
    & gzip -d -c $file.FullName | docker load
}}

$tarFiles = Get-ChildItem -Path "docker\\images\\*.tar" -ErrorAction SilentlyContinue
foreach ($file in $tarFiles) {{
    Write-Host "  Loading: $($file.Name)"
    docker load -i $file.FullName
}}

# [2/5] Docker ë„¤íŠ¸ì›Œí¬ ìƒì„±
Write-Host ""
Write-Host "[2/5] Creating Docker network..." -ForegroundColor Yellow
docker network create {container_prefix}_network 2>$null
if ($LASTEXITCODE -ne 0) {{ Write-Host "  Network already exists" }}

# [3/5] ì„œë¹„ìŠ¤ ì‹œì‘
Write-Host ""
Write-Host "[3/5] Starting services..." -ForegroundColor Yellow
Set-Location docker
docker-compose up -d

# [4/5] Frontend Nginx ì„¤ì • ì—…ë°ì´íŠ¸
$FrontendContainer = "{container_prefix}-blueprint-ai-bom-frontend"
$RunningContainers = docker ps --format "{{{{.Names}}}}"
if ($RunningContainers -match $FrontendContainer) {{
    Write-Host ""
    Write-Host "[4/5] Updating frontend nginx configuration..." -ForegroundColor Yellow
    docker cp "..\\scripts\\nginx.conf" "${{FrontendContainer}}:/etc/nginx/conf.d/default.conf"
    docker exec $FrontendContainer nginx -s reload 2>$null
    Write-Host "  Nginx configuration updated"
}}

# [5/5] ì„¸ì…˜ ë°ì´í„° ìë™ ë³µì›
Set-Location $RootDir
if (Test-Path "session_import.json") {{
    Write-Host ""
    Write-Host "[5/5] Restoring session data..." -ForegroundColor Yellow

    # ë°±ì—”ë“œê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
    for ($i = 1; $i -le 30; $i++) {{
        try {{
            $health = Invoke-RestMethod -Uri "http://localhost:{bom_backend_port}/health" -Method Get -ErrorAction SilentlyContinue
            if ($health.status -eq "healthy") {{ break }}
        }} catch {{}}
        Write-Host "  Waiting for backend to be ready... ($i/30)"
        Start-Sleep -Seconds 1
    }}

    # ì„¸ì…˜ Import
    try {{
        $importResult = Invoke-RestMethod -Uri "http://localhost:{bom_backend_port}/sessions/import" `
            -Method Post `
            -Form @{{ file = Get-Item "session_import.json" }} `
            -ErrorAction SilentlyContinue

        if ($importResult.session_id) {{
            Write-Host "  Session restored: $($importResult.session_id)" -ForegroundColor Green
        }}
    }} catch {{
        Write-Host "  Session restore failed (you can manually import session_import.json)" -ForegroundColor Yellow
    }}
}} else {{
    Write-Host ""
    Write-Host "[5/5] No session data to restore (session_import.json not found)" -ForegroundColor Yellow
}}

Write-Host ""
Write-Host "==========================================" -ForegroundColor Green
Write-Host "  Import Complete!" -ForegroundColor Green
Write-Host "==========================================" -ForegroundColor Green
Write-Host ""
Write-Host "Container prefix: {container_prefix}-"
Write-Host "Port offset: +{port_offset}"
Write-Host ""
'''
        # í”„ë¡ íŠ¸ì—”ë“œ URL ë¨¼ì € í‘œì‹œ
        if frontend_services:
            ps_script += 'Write-Host "==========================================" -ForegroundColor Magenta\n'
            ps_script += 'Write-Host "  UI Access URL:" -ForegroundColor Magenta\n'
            ps_script += 'Write-Host "==========================================" -ForegroundColor Magenta\n'
            for service in frontend_services:
                original_port = self.SERVICE_PORT_MAP.get(service, 3000)
                mapped_port = original_port + port_offset
                ps_script += f'Write-Host "  * http://localhost:{mapped_port}" -ForegroundColor Yellow\n'
            ps_script += 'Write-Host ""\n'

        ps_script += 'Write-Host "API endpoints:"\n'
        for service in backend_services:
            original_port = self.SERVICE_PORT_MAP.get(service, 5000)
            mapped_port = original_port + port_offset
            ps_script += f'Write-Host "  - {container_prefix}-{service}: http://localhost:{mapped_port}"\n'

        ps_script += '''
Write-Host ""
Write-Host "Check status: cd docker; docker-compose ps"
Write-Host "Stop services: cd docker; docker-compose down"
'''

        ps_path = output_dir / "import.ps1"
        with open(ps_path, "w", newline="\r\n") as f:
            f.write(ps_script)

        logger.info(f"[Export] Import scripts generated in {output_dir}")

    def _generate_nginx_config(
        self,
        output_dir: Path,
        container_prefix: str
    ) -> None:
        """Frontendìš© Nginx ì„¤ì • íŒŒì¼ ìƒì„±"""
        backend_container = f"{container_prefix}-blueprint-ai-bom-backend"

        nginx_config = f'''server {{
    listen 80;
    server_name localhost;
    root /usr/share/nginx/html;
    index index.html;

    gzip on;
    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;

    location / {{
        try_files $uri $uri/ /index.html;
    }}

    location /api {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }}

    location /sessions {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location /detection {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location /bom {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location /health {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location /export {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location /customer {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location /analysis {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location /verification {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location /feedback {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location /projects {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location /config {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location /openapi.json {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location /docs {{
        proxy_pass http://{backend_container}:5020;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }}

    location ~* \\.(js|css|png|jpg|jpeg|gif|ico|svg)$ {{
        expires 1y;
        add_header Cache-Control "public, immutable";
    }}
}}
'''
        nginx_path = output_dir / "nginx.conf"
        with open(nginx_path, "w") as f:
            f.write(nginx_config)

        logger.info(f"[Export] Nginx config generated: {nginx_path}")

    def _encode_image_file(self, img_path: Path, filename: str) -> Optional[Dict[str, Any]]:
        """ì´ë¯¸ì§€ íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©"""
        try:
            with open(img_path, "rb") as f:
                image_bytes = f.read()

            mime_type, _ = mimetypes.guess_type(str(img_path))
            if not mime_type:
                mime_type = "image/png"

            result = {
                "filename": filename,
                "image_base64": base64.b64encode(image_bytes).decode("utf-8"),
                "mime_type": mime_type,
                "file_size": len(image_bytes)
            }
            logger.info(f"[Export] Image encoded: {img_path.name} ({len(image_bytes)} bytes)")
            return result
        except Exception as e:
            logger.warning(f"[Export] Failed to encode image {img_path}: {e}")
            return None

    def generate_importable_session(
        self,
        session: Dict[str, Any],
        output_path: Path,
        upload_dir: Path
    ) -> bool:
        """Import ì—”ë“œí¬ì¸íŠ¸ì™€ í˜¸í™˜ë˜ëŠ” ì„¸ì…˜ JSON ìƒì„±

        Args:
            session: ì„¸ì…˜ ë°ì´í„°
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            upload_dir: ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ (ì´ë¯¸ì§€ íŒŒì¼ ìœ„ì¹˜)

        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        session_id = session.get("session_id", "")
        filename = session.get("filename", "")

        # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸° ë° base64 ì¸ì½”ë”©
        image_data = None
        session_dir = upload_dir / session_id
        logger.info(f"[Export] Looking for session image in: {session_dir}, filename: {filename}")

        if session_dir.exists():
            # 1. ì„¸ì…˜ì˜ filenameìœ¼ë¡œ ë¨¼ì € ì°¾ê¸°
            if filename:
                img_path = session_dir / filename
                if img_path.exists():
                    image_data = self._encode_image_file(img_path, filename)

            # 2. filenameìœ¼ë¡œ ëª» ì°¾ìœ¼ë©´ ì´ë¯¸ì§€ í™•ì¥ìë¡œ ì°¾ê¸°
            if not image_data:
                for ext in [".jpg", ".jpeg", ".png", ".bmp", ".tiff", ".gif"]:
                    for pattern in [f"original{ext}", f"*{ext}"]:
                        if pattern.startswith("*"):
                            # glob íŒ¨í„´ìœ¼ë¡œ ì°¾ê¸°
                            matches = list(session_dir.glob(pattern))
                            if matches:
                                img_path = matches[0]
                                image_data = self._encode_image_file(img_path, filename or img_path.name)
                                break
                        else:
                            img_path = session_dir / pattern
                            if img_path.exists():
                                image_data = self._encode_image_file(img_path, filename or img_path.name)
                                break
                    if image_data:
                        break
        else:
            logger.warning(f"[Export] Session directory not found: {session_dir}")

        if not image_data:
            logger.warning(f"[Export] No image found for session {session_id}, session_import.json will not have image data")

        # Import ì—”ë“œí¬ì¸íŠ¸ í˜¸í™˜ í˜•ì‹ ìƒì„±
        importable_data = {
            "export_version": "1.0",
            "session_metadata": {
                "session_id": session_id,
                "filename": session.get("filename", ""),
                "status": session.get("status", "uploaded"),
                "drawing_type": session.get("drawing_type", "auto"),
                "image_width": session.get("image_width"),
                "image_height": session.get("image_height"),
                "features": session.get("features", []),
                "created_at": session.get("created_at"),
                "template_id": session.get("template_id"),
                "template_name": session.get("template_name"),
            },
            "image_data": image_data,
            "detections": session.get("detections", []),
            "verification_status": {
                d.get("id"): d.get("verification_status", "pending")
                for d in session.get("detections", [])
            },
            "bom_data": session.get("bom_data"),
            "ocr_texts": session.get("ocr_texts", []),
        }

        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(importable_data, f, indent=2, ensure_ascii=False, default=str)
            logger.info(f"[Export] Importable session JSON created: {output_path}")
            return True
        except Exception as e:
            logger.error(f"[Export] Failed to create importable session: {e}")
            return False

    def get_preview(
        self,
        session: Dict[str, Any],
        template: Optional[Dict[str, Any]] = None,
        port_offset: int = 10000,
        include_web_ui: bool = False,
        source_prefix: str = "poc_",
    ) -> SelfContainedPreview:
        """Self-contained Export ë¯¸ë¦¬ë³´ê¸°

        Args:
            session: ì„¸ì…˜ ë°ì´í„°
            template: í…œí”Œë¦¿ ë°ì´í„° (ì˜µì…˜)
            port_offset: í¬íŠ¸ ì˜¤í”„ì…‹
            include_web_ui: web-ui (BlueprintFlow í¸ì§‘ê¸°) í¬í•¨ ì—¬ë¶€
            source_prefix: ì†ŒìŠ¤ ì´ë¯¸ì§€ ì ‘ë‘ì‚¬ (ì˜ˆ: poc_, poc-)
        """
        session_id = session.get("session_id", "")

        workflow_def = (
            template.get("workflow_definition", {})
            if template
            else session.get("workflow_definition", {})
        )
        session_features = session.get("features", [])
        required_services = self.detect_required_services(
            workflow_def,
            include_web_ui=include_web_ui,
            session_features=session_features
        )
        node_types = self.get_workflow_node_types(workflow_def)

        # í¬ê¸° ë° í¬íŠ¸ ë§¤í•‘ ì¡°íšŒ
        estimated_sizes = {}
        port_mapping = {}
        total_size = 0.0

        for service in required_services:
            size = self.get_docker_image_size(service, source_prefix=source_prefix)
            estimated_sizes[service] = size
            total_size += size

            original_port = self.SERVICE_PORT_MAP.get(service, 5000)
            port_mapping[service] = {
                "original": original_port,
                "mapped": original_port + port_offset
            }

        can_export = len(required_services) > 0
        reason = None if can_export else "ì›Œí¬í”Œë¡œìš°ì— ë¶„ì„ ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤."

        return SelfContainedPreview(
            session_id=session_id,
            can_export=can_export,
            reason=reason,
            required_services=required_services,
            estimated_sizes_mb=estimated_sizes,
            total_estimated_size_mb=round(total_size, 2),
            port_mapping=port_mapping,
            workflow_node_types=node_types,
        )

    def create_package(
        self,
        session: Dict[str, Any],
        request: SelfContainedExportRequest,
        prepare_session_data_func,
        template: Optional[Dict[str, Any]] = None,
        project: Optional[Dict[str, Any]] = None,
    ) -> SelfContainedExportResponse:
        """Self-contained Export íŒ¨í‚¤ì§€ ìƒì„±"""
        session_id = session.get("session_id", "")
        export_id = str(uuid.uuid4())[:8]
        port_offset = request.port_offset
        container_prefix = request.container_prefix
        include_web_ui = getattr(request, 'include_web_ui', False)
        source_prefix = getattr(request, 'source_image_prefix', 'poc_')

        workflow_def = (
            template.get("workflow_definition", {})
            if template
            else session.get("workflow_definition", {})
        )
        session_features = session.get("features", [])
        required_services = self.detect_required_services(
            workflow_def,
            include_web_ui=include_web_ui,
            session_features=session_features
        )

        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)

            images = session.get("images", [])
            detections = session.get("detections", [])

            # í¬íŠ¸ ë§¤í•‘ ì •ë³´ ìƒì„±
            port_mapping = {}
            for service in required_services:
                original_port = self.SERVICE_PORT_MAP.get(service, 5000)
                port_mapping[service] = {
                    "original": original_port,
                    "mapped": original_port + port_offset
                }

            manifest_data = {
                "export_version": "3.0",
                "export_timestamp": datetime.now().isoformat(),
                "export_type": "self-contained",
                "session_id": session_id,
                "session_filename": session.get("filename", ""),
                "drawing_type": session.get("drawing_type"),
                "image_count": len(images),
                "detection_count": len(detections),
                "included_services": required_services,
                "port_offset": port_offset,
                "container_prefix": container_prefix,
                "port_mapping": port_mapping,
                "exported_by": request.exported_by,
                "notes": request.notes,
            }

            with open(temp_path / "manifest.json", "w") as f:
                json.dump(manifest_data, f, indent=2, default=str)

            session_data = prepare_session_data_func(session, include_rejected=False)
            with open(temp_path / "session.json", "w") as f:
                json.dump(session_data, f, indent=2, default=str)

            # Import ì—”ë“œí¬ì¸íŠ¸ í˜¸í™˜ ì„¸ì…˜ íŒŒì¼ ìƒì„± (ìë™ ë³µì›ìš©)
            self.generate_importable_session(
                session=session,
                output_path=temp_path / "session_import.json",
                upload_dir=self.upload_dir
            )

            docker_images_info = {}
            docker_total_size = 0.0

            if request.include_images:
                images_dir = temp_path / "images"
                images_dir.mkdir()
                for img in images:
                    file_path = Path(img.get("file_path", ""))
                    if file_path.exists():
                        image_id = img.get("image_id", "")
                        dest_path = images_dir / f"{image_id}_{file_path.name}"
                        shutil.copy2(file_path, dest_path)

            if request.include_docker:
                docker_dir = temp_path / "docker"
                docker_dir.mkdir()
                images_out_dir = docker_dir / "images"
                images_out_dir.mkdir()

                docker_images_info = self.export_docker_images(
                    required_services, images_out_dir,
                    compress=request.compress_images,
                    port_offset=port_offset,
                    source_prefix=source_prefix
                )

                for info in docker_images_info.values():
                    docker_total_size += info.size_mb

                self.generate_docker_compose(
                    required_services,
                    docker_dir / "docker-compose.yml",
                    port_offset=port_offset,
                    container_prefix=container_prefix
                )

                scripts_dir = temp_path / "scripts"
                self.generate_import_scripts(
                    scripts_dir, required_services,
                    port_offset=port_offset,
                    container_prefix=container_prefix
                )

            readme = self._generate_readme(
                session_id, required_services, docker_images_info,
                request.include_docker, port_offset, container_prefix
            )
            with open(temp_path / "README.md", "w") as f:
                f.write(readme)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{session.get('filename', 'session')}_{export_id}_{timestamp}_self_contained.zip"
            export_path = self.export_dir / filename

            with zipfile.ZipFile(export_path, "w", zipfile.ZIP_DEFLATED) as zf:
                for file_path in temp_path.rglob("*"):
                    if file_path.is_file():
                        arcname = file_path.relative_to(temp_path)
                        zf.write(file_path, arcname)

        file_size = export_path.stat().st_size
        logger.info(
            f"[Export] Self-contained package: {filename} "
            f"({file_size / (1024*1024):.1f} MB, offset=+{port_offset})"
        )

        # í”„ë¡ íŠ¸ì—”ë“œ URL í¬í•¨
        frontend_url_info = ""
        ui_urls = []
        if "blueprint-ai-bom-frontend" in required_services:
            frontend_port = 3000 + port_offset
            ui_urls.append(f"   â˜… Blueprint AI BOM: http://localhost:{frontend_port}")
        if "web-ui" in required_services:
            webui_port = 5173 + port_offset
            ui_urls.append(f"   â˜… BlueprintFlow í¸ì§‘ê¸°: http://localhost:{webui_port}")

        if ui_urls:
            frontend_url_info = f"""
5. UI ì ‘ì†:
{chr(10).join(ui_urls)}
"""

        import_instructions = f"""
1. íŒ¨í‚¤ì§€ ì••ì¶• í•´ì œ:
   unzip {filename}

2. Import ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰:
   # Linux/macOS
   chmod +x scripts/import.sh && ./scripts/import.sh

   # Windows (PowerShell)
   .\\scripts\\import.ps1

3. ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸:
   cd docker && docker-compose ps

4. í¬íŠ¸ ì •ë³´:
   - í¬íŠ¸ ì˜¤í”„ì…‹: +{port_offset}
   - ì»¨í…Œì´ë„ˆ ì ‘ë‘ì‚¬: {container_prefix}-
   - ì˜ˆ: yolo-api (5005) â†’ {container_prefix}-yolo-api:{5005 + port_offset}
{frontend_url_info}"""

        return SelfContainedExportResponse(
            success=True,
            session_id=session_id,
            export_id=export_id,
            filename=filename,
            file_path=str(export_path),
            file_size_bytes=file_size,
            created_at=datetime.now().isoformat(),
            included_services=required_services,
            docker_images=list(docker_images_info.values()),
            docker_images_size_mb=round(docker_total_size, 2),
            port_offset=port_offset,
            import_instructions=import_instructions,
        )

    def _generate_readme(
        self,
        session_id: str,
        services: List[str],
        docker_images: Dict[str, DockerImageInfo],
        include_docker: bool,
        port_offset: int,
        container_prefix: str
    ) -> str:
        """README.md ìƒì„±"""
        # í”„ë¡ íŠ¸ì—”ë“œì™€ ë°±ì—”ë“œ ë¶„ë¦¬
        frontend_services = [s for s in services if s in self.FRONTEND_SERVICES]
        backend_services = [s for s in services if s not in self.FRONTEND_SERVICES]

        # í”„ë¡ íŠ¸ì—”ë“œ URL ê³„ì‚°
        frontend_url = ""
        if "blueprint-ai-bom-frontend" in services:
            frontend_port = 3000 + port_offset
            frontend_url = f"http://localhost:{frontend_port}"

        readme = f"""# Blueprint AI BOM - Self-contained Export Package

## ì„¸ì…˜ ì •ë³´
- **Session ID**: {session_id}
- **Export Date**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **Export Type**: Self-contained (Docker ì´ë¯¸ì§€ í¬í•¨)
- **Port Offset**: +{port_offset}
- **Container Prefix**: {container_prefix}-

"""
        # í”„ë¡ íŠ¸ì—”ë“œê°€ ìˆìœ¼ë©´ Quick Start ì„¹ì…˜ ì¶”ê°€
        if frontend_url:
            readme += f"""## ğŸš€ Quick Start

Import í›„ ë¸Œë¼ìš°ì €ì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥:

**UI ì ‘ì† URL**: **{frontend_url}**

"""

        readme += """## í¬í•¨ëœ ì„œë¹„ìŠ¤

"""
        # í”„ë¡ íŠ¸ì—”ë“œ ì„¹ì…˜
        if frontend_services:
            readme += """### ğŸ–¥ï¸ í”„ë¡ íŠ¸ì—”ë“œ (UI)

| ì„œë¹„ìŠ¤ | Import í¬íŠ¸ | ì ‘ì† URL |
|--------|-------------|----------|
"""
            for service in frontend_services:
                original_port = self.SERVICE_PORT_MAP.get(service, 0)
                mapped_port = original_port + port_offset
                size_info = ""
                if service in docker_images:
                    size_info = f" ({docker_images[service].size_mb} MB)"
                readme += f"| {service}{size_info} | **{mapped_port}** | http://localhost:{mapped_port} |\n"

            readme += "\n"

        # ë°±ì—”ë“œ ì„¹ì…˜
        if backend_services:
            readme += """### âš™ï¸ ë°±ì—”ë“œ (API)

| ì„œë¹„ìŠ¤ | ì›ë³¸ í¬íŠ¸ | Import í¬íŠ¸ | ì»¨í…Œì´ë„ˆ ì´ë¦„ |
|--------|----------|-------------|--------------|
"""
            for service in backend_services:
                original_port = self.SERVICE_PORT_MAP.get(service, 0)
                mapped_port = original_port + port_offset
                container_name = f"{container_prefix}-{service}"
                size_info = ""
                if service in docker_images:
                    size_info = f" ({docker_images[service].size_mb} MB)"
                readme += f"| {service} | {original_port} | **{mapped_port}** | {container_name}{size_info} |\n"

        if include_docker:
            readme += f"""
## Import ë°©ë²•

### Linux/macOS
```bash
unzip <íŒ¨í‚¤ì§€ëª…>.zip -d blueprint-export
cd blueprint-export
chmod +x scripts/import.sh
./scripts/import.sh
```

### Windows (PowerShell)
```powershell
Expand-Archive <íŒ¨í‚¤ì§€ëª…>.zip -DestinationPath blueprint-export
cd blueprint-export
.\\scripts\\import.ps1
```

## ì„œë¹„ìŠ¤ í™•ì¸

```bash
# ì»¨í…Œì´ë„ˆ ìƒíƒœ
docker ps --filter "name={container_prefix}"

# ë¡œê·¸ í™•ì¸
docker logs {container_prefix}-yolo-api

# API í…ŒìŠ¤íŠ¸
curl http://localhost:{5005 + port_offset}/health
```

## ìš”êµ¬ì‚¬í•­
- Docker 20.10+
- docker-compose 2.0+

## ì„œë¹„ìŠ¤ ì¤‘ì§€
```bash
cd docker && docker-compose down
docker network rm {container_prefix}_network
```
"""
        return readme


# Singleton
_self_contained_export_service: Optional[SelfContainedExportService] = None


def get_self_contained_export_service(
    export_dir: Path, upload_dir: Path
) -> SelfContainedExportService:
    global _self_contained_export_service
    if _self_contained_export_service is None:
        _self_contained_export_service = SelfContainedExportService(export_dir, upload_dir)
    return _self_contained_export_service
